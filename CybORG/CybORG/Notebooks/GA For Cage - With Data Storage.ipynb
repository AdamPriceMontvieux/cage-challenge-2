{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Union[arg, ...]: each arg must be a type. Got <module 'CybORG.CybORG' from '/app/CybORG/CybORG/__init__.py'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mCybORG\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCybORG\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mAgents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mWrappers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mFixedFlatWrapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FixedFlatWrapper\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mCybORG\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCybORG\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mAgents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mWrappers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mOpenAIGymWrapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIGymWrapper\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mCybORG\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCybORG\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mAgents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mWrappers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChallengeWrapper\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mppo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PPOTrainer\n",
      "File \u001b[0;32m/app/CybORG/CybORG/Agents/Wrappers/__init__.py:9\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mEnumActionWrapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EnumActionWrapper\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mChallengeWrapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChallengeWrapper\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mReduceActionSpaceWrapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReduceActionSpaceWrapper\n",
      "File \u001b[0;32m/app/CybORG/CybORG/Agents/Wrappers/ReduceActionSpaceWrapper.py:9\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mCybORG\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCybORG\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mAgents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mWrappers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mBaseWrapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseWrapper\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mCybORG\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCybORG\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mShared\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Results\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mReduceActionSpaceWrapper\u001b[39;00m(BaseWrapper):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env: Union[BaseWrapper, CybORG]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, agent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(env, agent)\n",
      "File \u001b[0;32m/app/CybORG/CybORG/Agents/Wrappers/ReduceActionSpaceWrapper.py:10\u001b[0m, in \u001b[0;36mReduceActionSpaceWrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mReduceActionSpaceWrapper\u001b[39;00m(BaseWrapper):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env: \u001b[43mUnion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mBaseWrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCybORG\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, agent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(env, agent)\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_signature \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/usr/lib/python3.8/typing.py:261\u001b[0m, in \u001b[0;36m_tp_cache.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# All real errors (not unhashable args) are raised below.\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/typing.py:358\u001b[0m, in \u001b[0;36m_SpecialForm.__getitem__\u001b[0;34m(self, parameters)\u001b[0m\n\u001b[1;32m    356\u001b[0m     parameters \u001b[38;5;241m=\u001b[39m (parameters,)\n\u001b[1;32m    357\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnion[arg, ...]: each arg must be a type.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 358\u001b[0m parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_type_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m parameters \u001b[38;5;241m=\u001b[39m _remove_dups_flatten(parameters)\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(parameters) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/lib/python3.8/typing.py:358\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    356\u001b[0m     parameters \u001b[38;5;241m=\u001b[39m (parameters,)\n\u001b[1;32m    357\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnion[arg, ...]: each arg must be a type.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 358\u001b[0m parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[43m_type_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters)\n\u001b[1;32m    359\u001b[0m parameters \u001b[38;5;241m=\u001b[39m _remove_dups_flatten(parameters)\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(parameters) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/lib/python3.8/typing.py:149\u001b[0m, in \u001b[0;36m_type_check\u001b[0;34m(arg, msg, is_argument)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callable(arg):\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m!r:\u001b[39;00m\u001b[38;5;124m.100\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "\u001b[0;31mTypeError\u001b[0m: Union[arg, ...]: each arg must be a type. Got <module 'CybORG.CybORG' from '/app/CybORG/CybORG/__init__.py'>."
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import time\n",
    "from statistics import mean, stdev\n",
    "from CybORG.CybORG.CybORG import CybORG\n",
    "from CybORG.CybORG.Agents import B_lineAgent, SleepAgent, GreenAgent\n",
    "from CybORG.CybORG.Agents.SimpleAgents.BaseAgent import BaseAgent\n",
    "from CybORG.CybORG.Agents.SimpleAgents.BlueReactAgent import BlueReactRemoveAgent\n",
    "from CybORG.CybORG.Agents.SimpleAgents.Meander import RedMeanderAgent\n",
    "from CybORG.CybORG.Agents.Wrappers.FixedFlatWrapper import FixedFlatWrapper\n",
    "from CybORG.CybORG.Agents.Wrappers.OpenAIGymWrapper import OpenAIGymWrapper\n",
    "from CybORG.CybORG.Agents.Wrappers import ChallengeWrapper\n",
    "import os\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.tune.registry import register_env\n",
    "from CybORG.Agents.Wrappers.rllib_wrapper import RLlibWrapper\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy\n",
    "from collections import deque\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPS = 50\n",
    "agent_name = 'Blue'\n",
    "\n",
    "def wrap(env):\n",
    "    return RLlibWrapper(agent_name=\"Blue\", env=env)\n",
    "\n",
    "\n",
    "def evaluate(steps):\n",
    "    path = str(inspect.getfile(CybORG))\n",
    "    path = path[:-10] + '/Shared/Scenarios/Scenario1b.yaml'\n",
    "\n",
    "    #print(f'using CybORG v{cyborg_version}, {scenario}\\n')\n",
    "    for num_steps in steps:\n",
    "        for red_agent in [B_lineAgent, RedMeanderAgent, SleepAgent]:\n",
    "\n",
    "            cyborg = CybORG(path, 'sim', agents={'Red': red_agent})\n",
    "            wrapped_cyborg = wrap(cyborg)\n",
    "\n",
    "            observation = wrapped_cyborg.reset()\n",
    "            # observation = cyborg.reset().observation\n",
    "\n",
    "            action_space = wrapped_cyborg.get_action_space(agent_name)\n",
    "            # action_space = cyborg.get_action_space(agent_name)\n",
    "            total_reward = []\n",
    "            actions = []\n",
    "            for i in range(MAX_EPS):\n",
    "                r = []\n",
    "                a = []\n",
    "                # cyborg.env.env.tracker.render()\n",
    "                for j in range(num_steps):\n",
    "                    action = trainer.compute_single_action(observation)\n",
    "                    #action = agent.get_action(observation, action_space)\n",
    "                    observation, rew, done, info = wrapped_cyborg.step(action)\n",
    "                    # result = cyborg.step(agent_name, action)\n",
    "                    r.append(rew)\n",
    "                    # r.append(result.reward)\n",
    "                    a.append((str(cyborg.get_last_action('Blue')), str(cyborg.get_last_action('Red'))))\n",
    "                total_reward.append(sum(r))\n",
    "                actions.append(a)\n",
    "                # observation = cyborg.reset().observation\n",
    "                observation = wrapped_cyborg.reset()\n",
    "            print(f'Average reward for red agent {red_agent.__name__} and steps {num_steps} is: {mean(total_reward):.1f} with a standard deviation of {stdev(total_reward):.1f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tree  # pip install dm_tree\n",
    "from typing import Optional\n",
    "\n",
    "import ray\n",
    "import ray.experimental.tf_utils\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.filter import get_filter\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "from ray.rllib.utils.spaces.space_utils import get_base_struct_from_space, unbatch\n",
    "from ray.rllib.execution.replay_ops import Replay, StoreToReplayBuffer\n",
    "from decimal import Decimal\n",
    "tf1, tf, tfv = try_import_tf()\n",
    "\n",
    "def rollout(\n",
    "    policy: Policy,\n",
    "    env: gym.Env,\n",
    "    novelty_archive,\n",
    "    timestep_limit: Optional[int] = None,\n",
    "    add_noise: bool = False,\n",
    "    offset: float = 0.0,\n",
    "    novelty_type: str = \"action\"\n",
    "):\n",
    "    \"\"\"Do a rollout.\n",
    "    If add_noise is True, the rollout will take noisy actions with\n",
    "    noise drawn from that stream. Otherwise, no action noise will be added.\n",
    "    Args:\n",
    "        policy: RLlib Policy from which to draw actions.\n",
    "        env: Environment from which to draw rewards, done, and\n",
    "            next state.\n",
    "        timestep_limit: Steps after which to end the rollout.\n",
    "            If None, use `env.spec.max_episode_steps` or 999999.\n",
    "        add_noise: Indicates whether exploratory action noise should be\n",
    "            added.\n",
    "        offset: Value to subtract from the reward (e.g. survival bonus\n",
    "            from humanoid).\n",
    "    \"\"\"\n",
    "    max_timestep_limit = 999999\n",
    "    env_timestep_limit = (\n",
    "        env.spec.max_episode_steps\n",
    "        if (hasattr(env, \"spec\") and hasattr(env.spec, \"max_episode_steps\"))\n",
    "        else max_timestep_limit\n",
    "    )\n",
    "    timestep_limit = (\n",
    "        env_timestep_limit\n",
    "        if timestep_limit is None\n",
    "        else min(timestep_limit, env_timestep_limit)\n",
    "    )\n",
    "    t = 0\n",
    "    cur_obs = env.reset()\n",
    "    novel = []; returns = []\n",
    "    batch = SampleBatchBuilder() \n",
    "    for _ in range(timestep_limit or max_timestep_limit):\n",
    "        action, dist, _ = policy.compute_actions([cur_obs], add_noise=add_noise, update=True)\n",
    "        new_obs, r, done, _ = env.step(action[0])\n",
    "        if novelty_type == 'action':\n",
    "            action_vector = np.zeros(54)\n",
    "            action_vector[action] = 1\n",
    "            novel.append(action_vector); \n",
    "        else:\n",
    "            novel.append(cur_obs); \n",
    "        returns.append(r)          \n",
    "        batch.add_values(\n",
    "                obs=cur_obs,\n",
    "                actions=action[0],\n",
    "                rewards=r,\n",
    "                dones=done,\n",
    "                new_obs=new_obs)      \n",
    "        cur_obs = new_obs\n",
    "       # print(new_obs)\n",
    "        if offset != 0.0: r -= np.abs(offset)\n",
    "        t += 1\n",
    "        if done:\n",
    "            sample = batch.build_and_reset()\n",
    "            returns = np.array(returns)\n",
    "            sample[Postprocessing.ADVANTAGES] = scipy.signal.lfilter([1], [1, float(-0.9)], returns[::-1], axis=0)[::-1]\n",
    "            break\n",
    "        \n",
    "    \n",
    "    returns = np.array(returns, dtype=np.float64)\n",
    "    novel = np.mean(np.array(novel), axis=0)\n",
    "    return returns, t, novel, sample\n",
    "\n",
    "def make_session(single_threaded):\n",
    "    if not single_threaded:\n",
    "        return tf1.Session()\n",
    "    return tf1.Session(\n",
    "        config=tf1.ConfigProto(\n",
    "            inter_op_parallelism_threads=1, intra_op_parallelism_threads=1\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "class GATFPolicy(Policy):\n",
    "    def __init__(self, obs_space, action_space, config):\n",
    "        super().__init__(obs_space, action_space, config)\n",
    "        self.action_space_struct = get_base_struct_from_space(action_space)\n",
    "        self.action_noise_std = self.config[\"action_noise_std\"]\n",
    "        self.preprocessor = ModelCatalog.get_preprocessor_for_space(obs_space)\n",
    "        self.observation_filter = get_filter(\n",
    "            self.config[\"observation_filter\"], self.preprocessor.shape\n",
    "        )\n",
    "        self.single_threaded = self.config.get(\"single_threaded\", False)\n",
    "        self.config[\"framework\"] = \"tfe\"\n",
    "        if self.config[\"framework\"] == \"tf2\":\n",
    "            self.sess = make_session(single_threaded=self.single_threaded)\n",
    "\n",
    "            # Set graph-level seed.\n",
    "            if config.get(\"seed\") is not None:\n",
    "                with self.sess.as_default():\n",
    "                    tf1.set_random_seed(config[\"seed\"])\n",
    "\n",
    "            self.inputs = tf1.placeholder(\n",
    "                tf.float32, [None] + list(self.preprocessor.shape)\n",
    "            )\n",
    "        else:\n",
    "            if not tf1.executing_eagerly():\n",
    "                tf1.enable_eager_execution()\n",
    "            self.sess = self.inputs = None\n",
    "            if config.get(\"seed\") is not None:\n",
    "                # Tf2.x.\n",
    "                if config.get(\"framework\") == \"tf2\":\n",
    "                    tf.random.set_seed(config[\"seed\"])\n",
    "                # Tf-eager.\n",
    "                elif tf1 and config.get(\"framework\") == \"tfe\":\n",
    "                    tf1.set_random_seed(config[\"seed\"])\n",
    "\n",
    "        # Policy network.\n",
    "        self.dist_class, dist_dim = ModelCatalog.get_action_dist(\n",
    "            self.action_space, self.config[\"model\"], dist_type=\"deterministic\"\n",
    "        )\n",
    "\n",
    "        self.model = ModelCatalog.get_model_v2(\n",
    "            obs_space=self.preprocessor.observation_space,\n",
    "            action_space=action_space,\n",
    "            num_outputs=dist_dim,\n",
    "            model_config=self.config[\"model\"],\n",
    "        )\n",
    "\n",
    "        self.sampler = None\n",
    "        if self.sess:\n",
    "            dist_inputs, _ = self.model({SampleBatch.CUR_OBS: self.inputs})\n",
    "            dist = self.dist_class(dist_inputs, self.model)\n",
    "            self.sampler = dist.sample()\n",
    "            self.variables = ray.experimental.tf_utils.TensorFlowVariables(\n",
    "                dist_inputs, self.sess\n",
    "            )\n",
    "            self.sess.run(tf1.global_variables_initializer())\n",
    "        else:\n",
    "            self.variables = ray.experimental.tf_utils.TensorFlowVariables(\n",
    "                [], None, self.model.variables()\n",
    "            )\n",
    "\n",
    "        self.num_params = sum(\n",
    "            np.prod(variable.shape.as_list())\n",
    "            for _, variable in self.variables.variables.items()\n",
    "        )\n",
    "\n",
    "    @override(Policy)\n",
    "    def compute_actions(self, observation, add_noise=False, update=True, **kwargs):\n",
    "        # Squeeze batch dimension (we always calculate actions for only a\n",
    "        # single obs).\n",
    "        observation = observation[0]\n",
    "        observation = self.preprocessor.transform(observation)\n",
    "        observation = self.observation_filter(observation[None], update=update)\n",
    "        # `actions` is a list of (component) batches.\n",
    "        # Eager mode.\n",
    "        dist_inputs, _ = self.model({SampleBatch.CUR_OBS: observation})\n",
    "        dist = self.dist_class(dist_inputs, self.model)\n",
    "        actions = dist.sample()\n",
    "        actions = tree.map_structure(lambda a: a.numpy(), actions)\n",
    "        return actions, dist_inputs.numpy()[0], {}\n",
    "\n",
    "    def compute_single_action(\n",
    "        self, observation, add_noise=False, update=True, **kwargs\n",
    "    ):\n",
    "        action, state_outs, extra_fetches = self.compute_actions(\n",
    "            [observation], add_noise=add_noise, update=update, **kwargs\n",
    "        )\n",
    "        return action[0], state_outs, extra_fetches\n",
    "\n",
    "    def _add_noise(self, single_action, single_action_space):\n",
    "        if isinstance(\n",
    "            single_action_space, gym.spaces.Box\n",
    "        ) and single_action_space.dtype.name.startswith(\"float\"):\n",
    "            single_action += (\n",
    "                np.random.randn(*single_action.shape) * self.action_noise_std\n",
    "            )\n",
    "        return single_action\n",
    "\n",
    "    def get_state(self):\n",
    "        return {\"state\": self.get_flat_weights()}\n",
    "\n",
    "    def set_state(self, state):\n",
    "        return self.set_flat_weights(state[\"state\"])\n",
    "\n",
    "    def set_flat_weights(self, x):\n",
    "        self.variables.set_flat(x)\n",
    "\n",
    "    def get_flat_weights(self):\n",
    "        return self.variables.get_flat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import namedtuple\n",
    "import logging\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import Trainer\n",
    "from CybORG.CybORG.Agents.ES.RLLibFiles.trainer_config import TrainerConfig\n",
    "from CybORG.CybORG.Agents.ES import optimizers, utils\n",
    "from ray.rllib.agents.dqn.dqn_tf_policy import DQNTFPolicy\n",
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "from ray.rllib.env.env_context import EnvContext\n",
    "from ray.rllib.policy.sample_batch import DEFAULT_POLICY_ID\n",
    "from ray.rllib.utils import FilterManager\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.deprecation import Deprecated\n",
    "from ray.rllib.utils.metrics import (\n",
    "    NUM_AGENT_STEPS_SAMPLED,\n",
    "    NUM_AGENT_STEPS_TRAINED,\n",
    "    NUM_ENV_STEPS_SAMPLED,\n",
    "    NUM_ENV_STEPS_TRAINED,\n",
    ")\n",
    "from ray.rllib.utils.torch_utils import set_torch_seed\n",
    "from ray.rllib.utils.typing import TrainerConfigDict\n",
    "from ray.rllib.evaluation.sample_batch_builder import SampleBatchBuilder\n",
    "from ray.rllib.evaluation.postprocessing import Postprocessing\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "Result = namedtuple(\n",
    "    \"Result\",\n",
    "    [\n",
    "        \"noise_indices\",\n",
    "        \"eval_returns\",\n",
    "        \"observations\",\n",
    "        \"novelties\",\n",
    "        \"experience\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "class GAConfig(TrainerConfig):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(trainer_class=GATrainer)\n",
    "\n",
    "        # ES specific settings:\n",
    "        self.action_noise_std = 0.0\n",
    "        self.l2_coeff = 0.005\n",
    "        self.noise_stdev = 0.02\n",
    "        self.noise_decay = 0.995\n",
    "        self.episodes_per_batch = 100\n",
    "        self.reward_coefficient = 0.5\n",
    "        self.store_novelty_probs = 0.03\n",
    "        self.stepsize = 0.01\n",
    "        self.noise_size = 250000000\n",
    "        self.num_workers = 30\n",
    "        self.individuals_per_worker = 1\n",
    "        self.observation_filter = \"MeanStdFilter\"\n",
    "        self.framework = \"tf\"\n",
    "        self.evaluation_config[\"num_envs_per_worker\"] = 1\n",
    "        self.evaluation_config[\"observation_filter\"] = \"NoFilter\"\n",
    "        self.experience_sample_rate = 0.1\n",
    "        self.experience_store_dir = \"tmp\"\n",
    "        self.tourney_size = 12\n",
    "        self.novelty_max_size = 150\n",
    "        self.novelty_k = 25\n",
    "        self.novelty_type = 'action'\n",
    "\n",
    "\n",
    "    @override(TrainerConfig)\n",
    "    def training(\n",
    "        self,\n",
    "        *,\n",
    "        action_noise_std: Optional[float] = None,\n",
    "        noise_stdev: Optional[int] = None,\n",
    "        noise_decay: Optional[float] = None,\n",
    "        episodes_per_batch: Optional[int] = None,\n",
    "        reward_coefficient: Optional[float] = None,\n",
    "        stepsize: Optional[float] = None,\n",
    "        noise_size: Optional[int] = None,\n",
    "        tourney_size: Optional[int] = None,\n",
    "        individuals_per_worker: Optional[int] = None,\n",
    "        store_novelty_probs: Optional[float] = None,\n",
    "        experience_sample_rate: Optional[float] = None,\n",
    "        experience_store_dir: Optional[str] = None,\n",
    "        novelty_max_size: Optional[int] = None,\n",
    "        novelty_k: Optional[int] = None,\n",
    "        novelty_type: Optional[str] = None,\n",
    "        **kwargs,\n",
    "    ) -> \"GAConfig\":\n",
    "        \"\"\"Sets the training related configuration.\n",
    "        Args:\n",
    "            action_noise_std: Std. deviation to be used when adding (standard normal)\n",
    "                noise to computed actions. Action noise is only added, if\n",
    "                `compute_actions` is called with the `add_noise` arg set to True.\n",
    "            l2_coeff: Coefficient to multiply current weights with inside the globalg\n",
    "                optimizer update term.\n",
    "            noise_stdev: Std. deviation of parameter noise.\n",
    "            episodes_per_batch: Minimum number of episodes to pack into the train batch.\n",
    "            store_novelty_probs: Probability of \n",
    "            stepsize: SGD step-size used for the Adam optimizer.\n",
    "            noise_size: Number of rows in the noise table (shared across workers).\n",
    "                Each row contains a gaussian noise value for each model parameter.\n",
    "        Returns:\n",
    "            This updated TrainerConfig object.\n",
    "        \"\"\"\n",
    "        # Pass kwargs onto super's `training()` method.\n",
    "        super().training(**kwargs)\n",
    "\n",
    "        if action_noise_std is not None:\n",
    "            self.action_noise_std = action_noise_std\n",
    "        if noise_stdev is not None:\n",
    "            self.noise_stdev = noise_stdev\n",
    "        if noise_decay is not None:\n",
    "            self.noise_decay = noise_decay\n",
    "        if episodes_per_batch is not None:\n",
    "            self.episodes_per_batch = episodes_per_batch\n",
    "        if reward_coefficient is not None:\n",
    "            self.reward_coefficient = reward_coefficient\n",
    "        if individuals_per_worker is not None:\n",
    "            self.individuals_per_worker = individuals_per_worker\n",
    "        if stepsize is not None:\n",
    "            self.stepsize = stepsize\n",
    "        if noise_size is not None:\n",
    "            self.noise_size = noise_size\n",
    "        if store_novelty_probs is not None:\n",
    "            self.store_novelty_probs = store_novelty_probs\n",
    "        if experience_sample_rate is not None:\n",
    "            self.experience_sample_rate = experience_sample_rate\n",
    "        if experience_store_dir is not None:\n",
    "            self.experience_store_dir = experience_store_dir\n",
    "        if tourney_size is not None:\n",
    "            self.tourney_size = tourney_size\n",
    "        if novelty_max_size is not None:\n",
    "            self.novelty_max_size = novelty_max_size\n",
    "        if novelty_k is not None:\n",
    "            self.novelty_k = novelty_k\n",
    "        if novelty_type is not None:\n",
    "            self.novelty_type = novelty_type\n",
    "        return self\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def create_shared_noise(count):\n",
    "    \"\"\"Create a large array of noise to be shared by all workers.\"\"\"\n",
    "    seed = 123\n",
    "    noise = np.random.RandomState(seed).randn(count).astype(np.float32)\n",
    "    return noise\n",
    "\n",
    "\n",
    "class SharedNoiseTable:\n",
    "    def __init__(self, noise):\n",
    "        self.noise = noise\n",
    "        assert self.noise.dtype == np.float32\n",
    "\n",
    "    def get(self, i, dim):\n",
    "        return self.noise[i : i + dim]\n",
    "\n",
    "    def sample_index(self, dim):\n",
    "        return np.random.randint(0, len(self.noise) - dim + 1)\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class Worker:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        policy_params,\n",
    "        env_creator,\n",
    "        noise,\n",
    "        worker_index,\n",
    "        min_task_runtime=0.2,\n",
    "    ):\n",
    "\n",
    "        # Set Python random, numpy, env, and torch/tf seeds.\n",
    "        seed = config.get(\"seed\")\n",
    "        if seed is not None:\n",
    "            # Python random module.\n",
    "            random.seed(seed)\n",
    "            # Numpy.\n",
    "            np.random.seed(seed)\n",
    "            # Torch.\n",
    "            if config.get(\"framework\") == \"torch\":\n",
    "                set_torch_seed(seed)\n",
    "\n",
    "        self.min_task_runtime = min_task_runtime\n",
    "        self.config = config\n",
    "        self.config.update(policy_params)\n",
    "        self.config[\"single_threaded\"] = False\n",
    "        self.noise = SharedNoiseTable(noise)\n",
    "\n",
    "        env_context = EnvContext(config[\"env_config\"] or {}, worker_index)\n",
    "        self.env = env_creator(env_context)\n",
    "        # Seed the env, if gym.Env.\n",
    "        if not hasattr(self.env, \"seed\"):\n",
    "            logger.info(\"Env doesn't support env.seed(): {}\".format(self.env))\n",
    "        else:\n",
    "            self.env.seed(seed)\n",
    "\n",
    "        from ray.rllib import models\n",
    "\n",
    "        _policy_class = get_policy_class(config)\n",
    "        self.policy = _policy_class(\n",
    "            self.env.observation_space, self.env.action_space, config\n",
    "        )\n",
    "\n",
    "    def rollout(self, timestep_limit, novelty_archive, add_noise=True):\n",
    "        rollout_reward, rollout_fragment_length, obs, batch = rollout(\n",
    "            self.policy, self.env, novelty_archive, timestep_limit=timestep_limit, add_noise=add_noise, novelty_type=self.config[\"novelty_type\"]\n",
    "        )\n",
    "        return rollout_reward, obs, batch\n",
    "    \n",
    "    #optimise this? \n",
    "    def calculate_model_weights(self, noise_indexes):\n",
    "        weights = self.config[\"noise_stdev\"] * self.noise.get(noise_indexes[0], self.policy.num_params)\n",
    "        for i in range(1, len(noise_indexes)):\n",
    "            weights += (self.config[\"noise_stdev\"] * self.config[\"noise_decay\"]**i) * self.noise.get(noise_indexes[i], self.policy.num_params)    \n",
    "        return weights \n",
    "    \n",
    "    def euclidean_distance(self, x, y):\n",
    "        n, m = len(x), len(y)\n",
    "        if n > m:\n",
    "            a = np.linalg.norm(y - x[:m])\n",
    "            b = np.linalg.norm(y[-1] - x[m:])\n",
    "        else:\n",
    "            a = np.linalg.norm(x - y[:n])\n",
    "            b = np.linalg.norm(x[-1] - y[n:])\n",
    "        return np.sqrt(a**2 + b**2)\n",
    "\n",
    "    def compute_novelty_vs_archive(self, archive, novelty_vector, k):\n",
    "        if len(archive) < k:\n",
    "            return 0\n",
    "        distances = []\n",
    "        nov = novelty_vector.astype(np.float64)\n",
    "        for point in archive:\n",
    "            distances.append(self.euclidean_distance(point.astype(np.float64), nov))\n",
    "\n",
    "        # Pick k nearest neighbors\n",
    "        distances = np.array(distances)\n",
    "        top_k = np.sort(distances)[:k]\n",
    "        return top_k.mean()\n",
    "\n",
    "    \n",
    "    def do_rollouts(self, noise_indices, novelty_archive, timestep_limit=None):\n",
    "        # Set the network weights.\n",
    "        pop = []\n",
    "        rewards = []\n",
    "        observations = []\n",
    "        novelties = []\n",
    "        batch_builder = SampleBatchBuilder()\n",
    "        writer = JsonWriterEdit(os.path.join(ray._private.utils.get_user_temp_dir(), self.config[\"experience_store_dir\"])) \n",
    "        for ni in noise_indices:\n",
    "            weights = self.calculate_model_weights(ni)\n",
    "\n",
    "            # Do a regular run with parameter perturbations.\n",
    "            noise_index = self.noise.sample_index(self.policy.num_params)\n",
    "\n",
    "            perturbation = self.config[\"noise_stdev\"] * self.noise.get(\n",
    "                noise_index, self.policy.num_params\n",
    "            )\n",
    "\n",
    "            self.policy.set_flat_weights(weights + perturbation)\n",
    "            reward = 0; obs = []\n",
    "            for i in range(self.config[\"episodes_per_batch\"]):\n",
    "                r, o, batch = self.rollout(timestep_limit, novelty_archive)\n",
    "                if np.random.rand() <= self.config[\"experience_sample_rate\"]:  \n",
    "                    writer.write(batch)\n",
    "                reward += np.sum(r)\n",
    "                obs.append(o)\n",
    "            rewards.append(reward/self.config[\"episodes_per_batch\"])\n",
    "            obvs = np.mean(obs, axis=0)\n",
    "            observations.append(obvs)\n",
    "            novelties.append(self.compute_novelty_vs_archive(novelty_archive, obvs, self.config[\"novelty_k\"]))\n",
    "            ni.append(noise_index)\n",
    "            pop.append(ni)\n",
    "            \n",
    "        return Result(\n",
    "            noise_indices=pop,\n",
    "            eval_returns=rewards,\n",
    "            observations=observations,  \n",
    "            novelties=novelties,\n",
    "            experience=batch_builder.build_and_reset(),\n",
    "        )\n",
    "\n",
    "\n",
    "def get_policy_class(config):\n",
    "    return GATFPolicy\n",
    "\n",
    "class GATrainer(Trainer):\n",
    "    \"\"\"Large-scale implementation of Evolution Strategies in Ray.\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    @override(Trainer)\n",
    "    def get_default_config(cls) -> TrainerConfigDict:\n",
    "        return GAConfig().to_dict()\n",
    "\n",
    "    @override(Trainer)\n",
    "    def validate_config(self, config: TrainerConfigDict) -> None:\n",
    "        # Call super's validation method.\n",
    "        super().validate_config(config)\n",
    "\n",
    "        if config[\"num_gpus\"] > 1:\n",
    "            raise ValueError(\"`num_gpus` > 1 not yet supported for ES!\")\n",
    "        if config[\"num_workers\"] <= 0:\n",
    "            raise ValueError(\"`num_workers` must be > 0 for ES!\")\n",
    "        if config[\"evaluation_config\"][\"num_envs_per_worker\"] != 1:\n",
    "            raise ValueError(\n",
    "                \"`evaluation_config.num_envs_per_worker` must always be 1 for \"\n",
    "                \"ES! To parallelize evaluation, increase \"\n",
    "                \"`evaluation_num_workers` to > 1.\"\n",
    "            )\n",
    "\n",
    "    @override(Trainer)\n",
    "    def setup(self, config):\n",
    "        # Setup our config: Merge the user-supplied config (which could\n",
    "        # be a partial config dict with the class' default).\n",
    "        if isinstance(config, dict):\n",
    "            self.config = self.merge_trainer_configs(\n",
    "                self.get_default_config(), config, self._allow_unknown_configs\n",
    "            )\n",
    "        else:\n",
    "            self.config = config.to_dict()\n",
    "\n",
    "        # Call super's validation method.\n",
    "        self.validate_config(self.config)\n",
    "\n",
    "        # Generate `self.env_creator` callable to create an env instance.\n",
    "        self.env_creator = self._get_env_creator_from_env_id(self._env_id)\n",
    "        # Generate the local env.\n",
    "        env_context = EnvContext(self.config[\"env_config\"] or {}, worker_index=0)\n",
    "        env = self.env_creator(env_context)\n",
    "\n",
    "        self.callbacks = self.config[\"callbacks\"]()\n",
    "\n",
    "        self._policy_class = get_policy_class(self.config)\n",
    "        self.policy = self._policy_class(\n",
    "            obs_space=env.observation_space,\n",
    "            action_space=env.action_space,\n",
    "            config=self.config,\n",
    "        )\n",
    "\n",
    "        # Create the shared noise table.\n",
    "        logger.info(\"Creating shared noise table.\")\n",
    "        noise_id = create_shared_noise.remote(self.config[\"noise_size\"])\n",
    "        self.noise = SharedNoiseTable(ray.get(noise_id))\n",
    "\n",
    "        # Create the actors.\n",
    "        logger.info(\"Creating actors.\")\n",
    "        self.workers = [\n",
    "            Worker.remote(self.config, {}, self.env_creator, noise_id, idx + 1)\n",
    "            for idx in range(self.config[\"num_workers\"])\n",
    "        ]\n",
    "        \n",
    "        self.population = [[i] for i in range(int(self.config[\"num_workers\"] * self.config[\"individuals_per_worker\"]))]\n",
    "        self.novelty_archive = deque([], maxlen=self.config[\"novelty_max_size\"])\n",
    "        self.novelty_history = []\n",
    "        self.episodes_so_far = 0\n",
    "        self.reward_list = []\n",
    "        self.tstart = time.time()\n",
    "        self.elite = [0]\n",
    "\n",
    "    @override(Trainer)\n",
    "    def get_policy(self, policy=DEFAULT_POLICY_ID):\n",
    "        if policy != DEFAULT_POLICY_ID:\n",
    "            raise ValueError(\n",
    "                \"ES has no policy '{}'! Use {} \"\n",
    "                \"instead.\".format(policy, DEFAULT_POLICY_ID)\n",
    "            )\n",
    "        return self.policy\n",
    "\n",
    "    @override(Trainer)\n",
    "    def step_attempt(self):\n",
    "        config = self.config\n",
    "\n",
    "        theta = self.policy.get_flat_weights()\n",
    "        assert theta.dtype == np.float32\n",
    "        assert len(theta.shape) == 1\n",
    "\n",
    "        # Put the current policy weights in the object store.\n",
    "        theta_id = ray.put(theta)\n",
    "        # Use the actors to do rollouts. Note that we pass in the ID of the\n",
    "        # policy weights as these are shared.\n",
    "        results = self._collect_results(self.population, self.novelty_archive)\n",
    "        \n",
    "        # Update our sample steps counters.\n",
    "\n",
    "        # Loop over the results.\n",
    "        self.episodes_so_far += 1\n",
    "        # Assemble the results.\n",
    "        returns = []\n",
    "        novelty = []\n",
    "        individuals = []\n",
    "        for i, result in enumerate(results):\n",
    "            returns.extend(result.eval_returns)\n",
    "            novelty.extend(result.novelties)\n",
    "            individuals.extend(result.noise_indices)\n",
    "            if np.random.rand() <= self.config[\"store_novelty_probs\"]:\n",
    "                #print(result.observations)\n",
    "                self.novelty_archive.extend(result.observations)\n",
    "                self.novelty_history.extend(result.observations)\n",
    "            \n",
    "        #Learn GA\n",
    "        novelty = np.array(novelty); returns = np.array(returns)\n",
    "        values = []\n",
    "        for i in range(len(individuals)): \n",
    "            n = (novelty[i] / np.max(novelty)) * (1-self.config[\"reward_coefficient\"]) if np.max(novelty) > 0 else 0\n",
    "            r = ((returns[i]+np.abs(np.min(returns))) / (np.max(returns)+np.abs(np.min(returns)))) * self.config[\"reward_coefficient\"]\n",
    "            values.append(n + r)\n",
    "        values = np.array(values)\n",
    "        \n",
    "        \n",
    "        population = [self.elite]\n",
    "        self.elite = individuals[np.argmax(returns)]\n",
    "        indexes = np.arange(len(individuals))\n",
    "        for i in range(len(individuals)-1):\n",
    "            np.random.shuffle(indexes)\n",
    "            winner = np.max(values[indexes[0:self.config[\"tourney_size\"]]])\n",
    "            index = np.where(values == winner)[0][0]\n",
    "            population.append(individuals[index])\n",
    "        \n",
    "        self.population = population\n",
    "        \n",
    "\n",
    "        info = {\n",
    "            \"episodes_so_far\": self.episodes_so_far,\n",
    "        }\n",
    "\n",
    "        result = dict(\n",
    "            episode_reward_mean=mean(returns),\n",
    "            episode_reward_max=max(returns),\n",
    "            episode_novelty_mean=mean(novelty),\n",
    "            #episode_len_mean=eval_lengths.mean(),\n",
    "            #timesteps_this_iter=noisy_lengths.sum(),\n",
    "            info=info,\n",
    "        )\n",
    "\n",
    "        return result\n",
    "\n",
    "    @override(Trainer)\n",
    "    def compute_single_action(self, observation, *args, **kwargs):\n",
    "        action, _, _ = self.policy.compute_actions([observation], update=False)\n",
    "        if kwargs.get(\"full_fetch\"):\n",
    "            return action[0], [], {}\n",
    "        return action[0]\n",
    "\n",
    "    @Deprecated(new=\"compute_single_action\", error=False)\n",
    "    def compute_action(self, observation, *args, **kwargs):\n",
    "        return self.compute_single_action(observation, *args, **kwargs)\n",
    "\n",
    "    @override(Trainer)\n",
    "    def _sync_weights_to_workers(self, *, worker_set=None, workers=None):\n",
    "        # Broadcast the new policy weights to all evaluation workers.\n",
    "        assert worker_set is not None\n",
    "        logger.info(\"Synchronizing weights to evaluation workers.\")\n",
    "        weights = ray.put(self.policy.get_flat_weights())\n",
    "        worker_set.foreach_policy(lambda p, pid: p.set_flat_weights(ray.get(weights)))\n",
    "\n",
    "    @override(Trainer)\n",
    "    def cleanup(self):\n",
    "        # workaround for https://github.com/ray-project/ray/issues/1516\n",
    "        for w in self.workers:\n",
    "            w.__ray_terminate__.remote()\n",
    "\n",
    "    def _collect_results(self, population, novelty_archive):\n",
    "        num_timesteps = 0\n",
    "        results = []\n",
    "\n",
    "        #logger.info(\n",
    "        #    \"Collected {} episodes {} timesteps so far this iter\".format(\n",
    "        #        num_episodes, num_timesteps\n",
    "        #    )\n",
    "        #)\n",
    "        ind = self.config['individuals_per_worker']\n",
    "        rollout_ids = [\n",
    "            worker.do_rollouts.remote(population[int(i*ind):int((i+1)*ind)], novelty_archive) for i, worker in enumerate(self.workers)\n",
    "        ]\n",
    "        # Get the results of the rollouts.\n",
    "        for result in ray.get(rollout_ids):\n",
    "            results.append(result)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return {\n",
    "            \"weights\": self.policy.get_flat_weights(),\n",
    "            \"filter\": self.policy.observation_filter,\n",
    "            \"episodes_so_far\": self.episodes_so_far,\n",
    "        }\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.episodes_so_far = state[\"episodes_so_far\"]\n",
    "        self.policy.set_flat_weights(state[\"weights\"])\n",
    "        self.policy.observation_filter = state[\"filter\"]\n",
    "        FilterManager.synchronize(\n",
    "            {DEFAULT_POLICY_ID: self.policy.observation_filter}, self.workers\n",
    "        )\n",
    "\n",
    "\n",
    "# Deprecated: Use ray.rllib.algorithms.es.ESConfig instead!\n",
    "class _deprecated_default_config(dict):\n",
    "    def __init__(self):\n",
    "        super().__init__(GAConfig().to_dict())\n",
    "\n",
    "    @Deprecated(\n",
    "        old=\"ray.rllib.algorithms.es.es.DEFAULT_CONFIG\",\n",
    "        new=\"ray.rllib.algorithms.es.es.ESConfig(...)\",\n",
    "        error=False,\n",
    "    )\n",
    "    def __getitem__(self, item):\n",
    "        return super().__getitem__(item)\n",
    "\n",
    "\n",
    "DEFAULT_CONFIG = _deprecated_default_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.offline.json_writer import JsonWriter\n",
    "from ray.rllib.offline.dataset_writer import DatasetWriter\n",
    "from ray.rllib.offline.io_context import IOContext\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "def env_creator(env_config: dict):\n",
    "    path = str(inspect.getfile(CybORG))\n",
    "    path = path[:-10] + '/Shared/Scenarios/Scenario1b.yaml'\n",
    "    agents = {\"Red\": B_lineAgent, \"Green\": GreenAgent}\n",
    "    cyborg = CybORG(scenario_file=path, environment='sim', agents=agents)\n",
    "    env = RLlibWrapper(env=cyborg, agent_name=\"Blue\", max_steps=100)\n",
    "    return env\n",
    "\n",
    "def print_results(results_dict):\n",
    "    train_iter = results_dict[\"training_iteration\"]\n",
    "    r_mean = results_dict[\"episode_reward_mean\"]\n",
    "    r_max = results_dict[\"episode_reward_max\"]\n",
    "    r_min = results_dict[\"episode_reward_min\"]\n",
    "    print(f\"{train_iter:4d} \\tr_mean: {r_mean:.1f} \\tr_max: {r_max:.1f} \\tr_min: {r_min: .1f}\")\n",
    "    \n",
    "register_env(name=\"CybORG\", env_creator=env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "from shutil import make_archive\n",
    "model = {\"fcnet_hiddens\": [512,512],\n",
    "         \"fcnet_activation\": \"relu\",}\n",
    "\n",
    "names = [\"7_data_s\", \"8_data_s\", \"9_data_s\"]\n",
    "#names = [\"50_store\",\"150_store\",\"250_store\",\"350_store\"]\n",
    "means_all = []\n",
    "max_all = []\n",
    "novel_all = []\n",
    "archives = []\n",
    "for i, co in enumerate([.7,.8,.9]):\n",
    "    if os.path.isdir(os.path.join(ray._private.utils.get_user_temp_dir(), names[i])):\n",
    "        shutil.rmtree(os.path.join(ray._private.utils.get_user_temp_dir(), names[i]))\n",
    "\n",
    "    config = GAConfig().training(\n",
    "                             episodes_per_batch=10, \n",
    "                             reward_coefficient=co,\n",
    "                             model=model, \n",
    "                             noise_stdev=0.045,\n",
    "                             noise_decay=0.995,\n",
    "                             store_novelty_probs=0.1,\n",
    "                             individuals_per_worker=4,\n",
    "                             novelty_max_size=10000, #Go as high as you dare\n",
    "                             experience_sample_rate=0,\n",
    "                             tourney_size=12,\n",
    "                             novelty_k=30,\n",
    "                             novelty_type='state', #'action' or 'state'\n",
    "                             experience_store_dir=names[i])\\\n",
    "                             .resources(num_gpus=1).rollouts(num_rollout_workers=30).debugging(log_level='ERROR')\n",
    "    trainer = config.build(env=\"CybORG\")\n",
    "    print(co)\n",
    "    s = \"{:3d} reward mean: {:6.2f}, reward max: {:6.2f}, novelty mean: {:6.2f}\"\n",
    "    means = []; maxs = []; nov = []\n",
    "    for j in range(int(200)):\n",
    "        result = trainer.train()\n",
    "        means.append(result[\"episode_reward_mean\"])\n",
    "        maxs.append(result[\"episode_reward_max\"])\n",
    "        nov.append(result[\"episode_novelty_mean\"])\n",
    "        print(s.format(j,result[\"episode_reward_mean\"], result[\"episode_reward_max\"], result[\"episode_novelty_mean\"]))\n",
    "\n",
    "    #Collected data needs to be cleaned \n",
    "    result = subprocess.run(['ls', os.path.join(ray._private.utils.get_user_temp_dir(), names[i])], stdout=subprocess.PIPE)\n",
    "    removed = 0\n",
    "    for j, name in enumerate(str(result.stdout)[2:].split('\\\\n')[0:-1]):\n",
    "        f = open(os.path.join(ray._private.utils.get_user_temp_dir(), names[i], name))\n",
    "        try:\n",
    "            json.load(f)\n",
    "        except ValueError as err:\n",
    "            os.remove(os.path.join(ray._private.utils.get_user_temp_dir(), names[i], name)) \n",
    "            removed += 1\n",
    "    print('Removed ' + str(removed) + ' files, of ' + str(j) + 'files')\n",
    "    make_archive(names[i], 'zip', os.path.join(ray._private.utils.get_user_temp_dir(), names[i]))\n",
    "    \n",
    "    \n",
    "    means_all.append(means); np.save(names[i]+'_means.npy', np.array(means))\n",
    "    max_all.append(maxs); np.save(names[i]+'_maxs.npy', np.array(maxs))\n",
    "    novel_all.append(nov); np.save(names[i]+'_nov.npy', np.array(nov))\n",
    "    archives.append(np.stack(trainer.novelty_history)); np.save(names[i]+'_archive.npy', np.stack(trainer.novelty_history))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "from shutil import make_archive\n",
    "model = {\"fcnet_hiddens\": [512,512],\n",
    "         \"fcnet_activation\": \"relu\",}\n",
    "\n",
    "names = [\"85_data_s\"]\n",
    "#names = [\"50_store\",\"150_store\",\"250_store\",\"350_store\"]\n",
    "means_all = []\n",
    "max_all = []\n",
    "novel_all = []\n",
    "archives = []\n",
    "for i, co in enumerate([.85]):\n",
    "    if os.path.isdir(os.path.join(ray._private.utils.get_user_temp_dir(), names[i])):\n",
    "        shutil.rmtree(os.path.join(ray._private.utils.get_user_temp_dir(), names[i]))\n",
    "\n",
    "    config = GAConfig().training(\n",
    "                             episodes_per_batch=10, \n",
    "                             reward_coefficient=co,\n",
    "                             model=model, \n",
    "                             noise_stdev=0.045,\n",
    "                             noise_decay=0.995,\n",
    "                             store_novelty_probs=0.1,\n",
    "                             individuals_per_worker=4,\n",
    "                             novelty_max_size=10000, #Go as high as you dare\n",
    "                             experience_sample_rate=1,\n",
    "                             tourney_size=12,\n",
    "                             novelty_k=30,\n",
    "                             novelty_type='state', #'action' or 'state'\n",
    "                             experience_store_dir=names[i])\\\n",
    "                             .resources(num_gpus=1).rollouts(num_rollout_workers=30).debugging(log_level='ERROR')\n",
    "    trainer = config.build(env=\"CybORG\")\n",
    "    print(co)\n",
    "    s = \"{:3d} reward mean: {:6.2f}, reward max: {:6.2f}, novelty mean: {:6.2f}\"\n",
    "    means = []; maxs = []; nov = []\n",
    "    for j in range(int(200)):\n",
    "        result = trainer.train()\n",
    "        means.append(result[\"episode_reward_mean\"])\n",
    "        maxs.append(result[\"episode_reward_max\"])\n",
    "        nov.append(result[\"episode_novelty_mean\"])\n",
    "        print(s.format(j,result[\"episode_reward_mean\"], result[\"episode_reward_max\"], result[\"episode_novelty_mean\"]))\n",
    "\n",
    "    #Collected data needs to be cleaned \n",
    "    result = subprocess.run(['ls', os.path.join(ray._private.utils.get_user_temp_dir(), names[i])], stdout=subprocess.PIPE)\n",
    "    removed = 0\n",
    "    for j, name in enumerate(str(result.stdout)[2:].split('\\\\n')[0:-1]):\n",
    "        f = open(os.path.join(ray._private.utils.get_user_temp_dir(), names[i], name))\n",
    "        try:\n",
    "            json.load(f)\n",
    "        except ValueError as err:\n",
    "            os.remove(os.path.join(ray._private.utils.get_user_temp_dir(), names[i], name)) \n",
    "            removed += 1\n",
    "    print('Removed ' + str(removed) + ' files, of ' + str(j) + 'files')\n",
    "    make_archive(names[i], 'zip', os.path.join(ray._private.utils.get_user_temp_dir(), names[i]))\n",
    "    \n",
    "    \n",
    "    means_all.append(means); np.save(names[i]+'_means.npy', np.array(means))\n",
    "    max_all.append(maxs); np.save(names[i]+'_maxs.npy', np.array(maxs))\n",
    "    novel_all.append(nov); np.save(names[i]+'_nov.npy', np.array(nov))\n",
    "    archives.append(np.stack(trainer.novelty_history)); np.save(names[i]+'_archive.npy', np.stack(trainer.novelty_history))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "\n",
    "archives = []\n",
    "archives.append(np.load('95_data_a_archive.npy'))\n",
    "archives.append(np.load('9_data_a_archive.npy'))\n",
    "archives.append(np.load('85_data_a_archive.npy'))\n",
    "archives.append(np.load('ppo_novel_actions.npy'))\n",
    "\n",
    "names = ['Reward CoEf: 0.95', 'Reward CoEf: 0.9', 'Reward CoEf: 0.85', 'PPO']\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "runner = 0\n",
    "#reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1)\n",
    "#umap_embedding = reducer.fit_transform(np.concatenate(archives))\n",
    "ax1 = fig.add_subplot(111)\n",
    "for i, a in enumerate(archives[:4]):\n",
    "    ax1.scatter(umap_embedding[runner:runner+len(a),0],umap_embedding[runner:runner+len(a),1], label=names[i], alpha=0.1)\n",
    "    runner += len(a)\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('uMAP Plot of Action Based Novelty Experiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fml.txt') as f:\n",
    "    lines = f.readlines()\n",
    "reward = []\n",
    "for l in lines:\n",
    "    if \"eps 100 is: \" in l: \n",
    "        reward.append(float((l[59:64])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,12))\n",
    "plt.plot(np.load('9_data_s_maxs.npy'), label=\"Reward CoEf: 0.9\")\n",
    "plt.plot(np.load('8_data_s_maxs.npy'), label=\"Reward CoEf: 0.8\")\n",
    "plt.plot(np.load('7_data_s_maxs.npy'), label=\"Reward CoEf: 0.7\")\n",
    "plt.plot(reward[0:200], label=\"PPO\")\n",
    "plt.title('Reward Accumilation Using Action Based Novelty')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim([0,200])\n",
    "plt.ylim([-60,-15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(archives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from matplotlib import animation, rc\n",
    "\n",
    "rewards = np.load('ppo_reward.npy')\n",
    "novel = np.load('ppo_novel.npy')\n",
    "\n",
    "whole = [archives[1]]\n",
    "whole.append(novel)\n",
    "\n",
    "#reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1)\n",
    "#umap_embedding = reducer.fit_transform(np.concatenate(whole))\n",
    "animation_frames = 150\n",
    "runner = 0\n",
    "samples = 0\n",
    "start = 0\n",
    "for i, a in enumerate(archives):\n",
    "    if i == 0:\n",
    "        start = runner\n",
    "        samples = len(a)\n",
    "    runner += len(a)\n",
    "filenames = []\n",
    "for i in range(150):\n",
    "    fig = plt.figure(figsize=(10, 8), dpi=100)\n",
    "    i_line = int(i*(150/animation_frames))\n",
    "    i_scatter = int(i*(samples/animation_frames))\n",
    "    ax = plt.subplot(1, 2, 1)\n",
    "    ax.set_xlim([-4,15])\n",
    "    ax.set_ylim([-4,11])\n",
    "    ax.set_title('uMAP of State Embedding')\n",
    "    ax1 = plt.subplot(1, 2, 2)\n",
    "    ax1.set_xlim([0,150])\n",
    "    ax1.set_ylim([-100,-20])\n",
    "    ax1.set_title('Reward Accumilation')\n",
    "    ax1.set_xlabel('Iteration')\n",
    "    ax1.set_ylabel('Reward')\n",
    "    ax1.plot(max_all[1][:i_line+1], label=\"GA\")\n",
    "    ax1.plot(rewards[:i+1], label=\"PPO\")\n",
    "    ax1.legend(loc='lower right')\n",
    "    ax.scatter(umap_embedding[0:i_scatter+1,0], umap_embedding[0:i_scatter+1,1])\n",
    "    ax.scatter(umap_embedding[samples:samples+i+1,0], umap_embedding[samples:samples+i+1,1])\n",
    "    plt.savefig('gagif/'+str(i)+'.png')\n",
    "    filenames.append('gagif/'+str(i)+'.png')\n",
    "\n",
    "import imageio\n",
    "images = []\n",
    "for filename in filenames:\n",
    "    images.append(imageio.imread(filename))\n",
    "imageio.mimsave('movie.gif', images, duration=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.mimsave('movie.gif', images, duration=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(16,12))\n",
    "ax1 = fig.add_subplot(111)\n",
    "#ax1.plot(max_all[0] ,label=\"Reward CoEf = 0.6, Max\")\n",
    "#ax1.plot(max_all[0] ,label=\"Reward CoEf = 0.7, Max\")\n",
    "ax1.plot(max_all[0] ,label=\"Reward CoEf = 0.7\")\n",
    "ax1.plot(max_all[1],label=\"Reward CoEf = 0.8\")\n",
    "ax1.plot(max_all[2], label=\"Reward CoEf = 0.9\")\n",
    "#ax1.plot(means_all[0], label=\"Reward CoEf = 0.7, Mean\", color='b')\n",
    "#ax1.plot(means_all[1], label=\"Reward CoEf = 0.8, Mean\", color='g')\n",
    "#ax1.plot(means_all[2], label=\"Reward CoEf = 0.9, Mean\", color='r')\n",
    "#ax1.plot(means_all[3], label=\"Reward CoEf = 1, Mean\", color='c')\n",
    "#ax1.plot(means_all[5], label=\"Reward CoEf = 1, Mean\", color='m')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Novelty Experiment (B_lineAgent)')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Max Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "from six.moves.urllib.parse import urlparse\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from smart_open import smart_open\n",
    "except ImportError:\n",
    "    smart_open = None\n",
    "\n",
    "from ray.rllib.policy.sample_batch import MultiAgentBatch\n",
    "from ray.rllib.offline.io_context import IOContext\n",
    "from ray.rllib.offline.output_writer import OutputWriter\n",
    "from ray.rllib.utils.annotations import override, PublicAPI\n",
    "from ray.rllib.utils.compression import pack, compression_supported\n",
    "from ray.rllib.utils.typing import FileType, SampleBatchType\n",
    "from ray.util.ml_utils.json import SafeFallbackEncoder\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "WINDOWS_DRIVES = [chr(i) for i in range(ord(\"c\"), ord(\"z\") + 1)]\n",
    "\n",
    "\n",
    "# TODO(jungong) : use DatasetWriter to back JsonWriter, so we reduce\n",
    "#     codebase complexity without losing existing functionality.\n",
    "@PublicAPI\n",
    "class JsonWriterEdit(OutputWriter):\n",
    "    \"\"\"Writer object that saves experiences in JSON file chunks.\"\"\"\n",
    "\n",
    "    @PublicAPI\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: str,\n",
    "        ioctx: IOContext = None,\n",
    "        max_file_size: int = 64 * 1024 * 1024,\n",
    "        compress_columns: List[str] = frozenset([]), #[\"obs\", \"new_obs\"]\n",
    "    ):\n",
    "        \"\"\"Initializes a JsonWriter instance.\n",
    "\n",
    "        Args:\n",
    "            path: a path/URI of the output directory to save files in.\n",
    "            ioctx: current IO context object.\n",
    "            max_file_size: max size of single files before rolling over.\n",
    "            compress_columns: list of sample batch columns to compress.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        self.ioctx = ioctx or IOContext()\n",
    "        self.max_file_size = max_file_size\n",
    "        self.compress_columns = compress_columns\n",
    "        if urlparse(path).scheme not in [\"\"] + WINDOWS_DRIVES:\n",
    "            self.path_is_uri = True\n",
    "        else:\n",
    "            path = os.path.abspath(os.path.expanduser(path))\n",
    "            # Try to create local dirs if they don't exist\n",
    "            try:\n",
    "                os.makedirs(path)\n",
    "            except OSError:\n",
    "                pass  # already exists\n",
    "            assert os.path.exists(path), \"Failed to create {}\".format(path)\n",
    "            self.path_is_uri = False\n",
    "        self.path = path\n",
    "        self.file_index = 0\n",
    "        self.bytes_written = 0\n",
    "        self.cur_file = None\n",
    "\n",
    "    @override(OutputWriter)\n",
    "    def write(self, sample_batch: SampleBatchType):\n",
    "        start = time.time()\n",
    "        data = _to_json(sample_batch, self.compress_columns)\n",
    "        f = self._get_file()\n",
    "        f.write(data)\n",
    "        if hasattr(f, \"flush\"):  # legacy smart_open impls\n",
    "            f.flush()\n",
    "        self.bytes_written += len(data)\n",
    "        f.close()\n",
    "        logger.debug(\n",
    "            \"Wrote {} bytes to {} in {}s\".format(len(data), f, time.time() - start)\n",
    "        )\n",
    "\n",
    "    def _get_file(self) -> FileType:\n",
    "        timestr = datetime.utcnow().strftime('%H:%M:%S:%f')[:-3]\n",
    "        path = os.path.join(\n",
    "            self.path,\n",
    "            \"output-{}_worker-{}_{}.json\".format(\n",
    "                timestr, self.ioctx.worker_index, self.file_index\n",
    "            ),\n",
    "        )\n",
    "        return open(path, \"w\")\n",
    "\n",
    "\n",
    "def _to_jsonable(v, compress: bool) -> Any:\n",
    "    if compress and compression_supported():\n",
    "        return str(pack(v))\n",
    "    elif isinstance(v, np.ndarray):\n",
    "        return v.tolist()\n",
    "    return v\n",
    "\n",
    "\n",
    "def _to_json_dict(batch: SampleBatchType, compress_columns: List[str]) -> Dict:\n",
    "    out = {}\n",
    "    if isinstance(batch, MultiAgentBatch):\n",
    "        out[\"type\"] = \"MultiAgentBatch\"\n",
    "        out[\"count\"] = batch.count\n",
    "        policy_batches = {}\n",
    "        for policy_id, sub_batch in batch.policy_batches.items():\n",
    "            policy_batches[policy_id] = {}\n",
    "            for k, v in sub_batch.items():\n",
    "                policy_batches[policy_id][k] = _to_jsonable(\n",
    "                    v, compress=k in compress_columns\n",
    "                )\n",
    "        out[\"policy_batches\"] = policy_batches\n",
    "    else:\n",
    "        out[\"type\"] = \"SampleBatch\"\n",
    "        for k, v in batch.items():\n",
    "            out[k] = _to_jsonable(v, compress=k in compress_columns)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _to_json(batch: SampleBatchType, compress_columns: List[str]) -> str:\n",
    "    out = _to_json_dict(batch, compress_columns)\n",
    "    return json.dumps(out, cls=SafeFallbackEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "os.mkdir(os.path.join(ray._private.utils.get_user_temp_dir(), \"8_data_s_large.\"))\n",
    "result = subprocess.run(['ls', os.path.join(ray._private.utils.get_user_temp_dir(), \"8_data_s\")], stdout=subprocess.PIPE)\n",
    "for i, s in enumerate(str(result.stdout)[2:].split('\\\\n')[0:-1]):\n",
    "    if i % 5 == 0:\n",
    "        shutil.copyfile(os.path.join(ray._private.utils.get_user_temp_dir(), \"8_data_s\",s), os.path.join(ray._private.utils.get_user_temp_dir(), \"8_data_s_smallish\",s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "result = subprocess.run(['ls', os.path.join(ray._private.utils.get_user_temp_dir(), \"8_data_s_smallish\")], stdout=subprocess.PIPE)\n",
    "len(str(result.stdout)[2:].split('\\\\n')[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import make_archive\n",
    "make_archive(\"8_data_s_small\", 'zip', os.path.join(ray._private.utils.get_user_temp_dir(), \"8_data_s_small\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
