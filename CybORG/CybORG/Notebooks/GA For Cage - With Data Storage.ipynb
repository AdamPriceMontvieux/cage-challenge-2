{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import time\n",
    "from statistics import mean, stdev\n",
    "from CybORG import CybORG\n",
    "from CybORG.Agents import B_lineAgent, SleepAgent, GreenAgent\n",
    "from CybORG.Agents.SimpleAgents.BaseAgent import BaseAgent\n",
    "from CybORG.Agents.SimpleAgents.BlueReactAgent import BlueReactRemoveAgent\n",
    "from CybORG.Agents.SimpleAgents.Meander import RedMeanderAgent\n",
    "from CybORG.Agents.Wrappers.FixedFlatWrapper import FixedFlatWrapper\n",
    "from CybORG.Agents.Wrappers.OpenAIGymWrapper import OpenAIGymWrapper\n",
    "from CybORG.Agents.Wrappers import ChallengeWrapper\n",
    "import os\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.tune.registry import register_env\n",
    "from CybORG.Agents.Wrappers.rllib_wrapper import RLlibWrapper\n",
    "import warnings\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy\n",
    "from collections import deque\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPS = 20\n",
    "agent_name = 'Blue'\n",
    "\n",
    "def wrap(env):\n",
    "    return RLlibWrapper(agent_name=\"Blue\", env=env)\n",
    "\n",
    "\n",
    "def evaluate(steps):\n",
    "    path = str(inspect.getfile(CybORG))\n",
    "    path = path[:-10] + '/Shared/Scenarios/Scenario2.yaml'\n",
    "\n",
    "    #print(f'using CybORG v{cyborg_version}, {scenario}\\n')\n",
    "    for num_steps in steps:\n",
    "        for red_agent in [B_lineAgent]:\n",
    "\n",
    "            cyborg = CybORG(path, 'sim', agents={'Red': red_agent})\n",
    "            wrapped_cyborg = wrap(cyborg)\n",
    "\n",
    "            observation = wrapped_cyborg.reset()\n",
    "            # observation = cyborg.reset().observation\n",
    "\n",
    "            action_space = wrapped_cyborg.get_action_space(agent_name)\n",
    "            # action_space = cyborg.get_action_space(agent_name)\n",
    "            total_reward = []\n",
    "            actions = []\n",
    "            for i in range(MAX_EPS):\n",
    "                r = []\n",
    "                a = []\n",
    "                # cyborg.env.env.tracker.render()\n",
    "                for j in range(num_steps):\n",
    "                    action, _, _ = trainer.compute_actions(observation)\n",
    "                    observation, rew, done, info = wrapped_cyborg.step(action)\n",
    "                    # result = cyborg.step(agent_name, action)\n",
    "                    r.append(rew)\n",
    "                    # r.append(result.reward)\n",
    "                    a.append((str(cyborg.get_last_action('Blue')), str(cyborg.get_last_action('Red'))))\n",
    "                total_reward.append(sum(r))\n",
    "                actions.append(a)\n",
    "                # observation = cyborg.reset().observation\n",
    "                observation = wrapped_cyborg.reset()\n",
    "            print(f'Average reward for red agent {red_agent.__name__} and steps {num_steps} is: {mean(total_reward):.1f} with a standard deviation of {stdev(total_reward):.1f}')\n",
    "    return mean(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tree  # pip install dm_tree\n",
    "from typing import Optional\n",
    "\n",
    "import ray\n",
    "import ray.experimental.tf_utils\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.filter import get_filter\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "from ray.rllib.utils.spaces.space_utils import get_base_struct_from_space, unbatch\n",
    "from ray.rllib.execution.replay_ops import Replay, StoreToReplayBuffer\n",
    "from decimal import Decimal\n",
    "tf1, tf, tfv = try_import_tf()\n",
    "\n",
    "def rollout(\n",
    "    policy: Policy,\n",
    "    env: gym.Env,\n",
    "    novelty_archive,\n",
    "    timestep_limit: Optional[int] = None,\n",
    "    add_noise: bool = False,\n",
    "    offset: float = 0.0,\n",
    "    novelty_type: str = \"action\"\n",
    "):\n",
    "    \"\"\"Do a rollout.\n",
    "    If add_noise is True, the rollout will take noisy actions with\n",
    "    noise drawn from that stream. Otherwise, no action noise will be added.\n",
    "    Args:\n",
    "        policy: RLlib Policy from which to draw actions.\n",
    "        env: Environment from which to draw rewards, done, and\n",
    "            next state.\n",
    "        timestep_limit: Steps after which to end the rollout.\n",
    "            If None, use `env.spec.max_episode_steps` or 999999.\n",
    "        add_noise: Indicates whether exploratory action noise should be\n",
    "            added.\n",
    "        offset: Value to subtract from the reward (e.g. survival bonus\n",
    "            from humanoid).\n",
    "    \"\"\"\n",
    "    max_timestep_limit = 999999\n",
    "    env_timestep_limit = (\n",
    "        env.spec.max_episode_steps\n",
    "        if (hasattr(env, \"spec\") and hasattr(env.spec, \"max_episode_steps\"))\n",
    "        else max_timestep_limit\n",
    "    )\n",
    "    timestep_limit = (\n",
    "        env_timestep_limit\n",
    "        if timestep_limit is None\n",
    "        else min(timestep_limit, env_timestep_limit)\n",
    "    )\n",
    "    t = 0\n",
    "    cur_obs = env.reset()\n",
    "    novel = []; returns = []\n",
    "    batch = SampleBatchBuilder() \n",
    "    for _ in range(timestep_limit or max_timestep_limit):\n",
    "        action, dist, _ = policy.compute_actions([cur_obs], add_noise=add_noise, update=True)\n",
    "        new_obs, r, done, _ = env.step(action[0])\n",
    "        if novelty_type == 'action':\n",
    "            action_vector = np.zeros(145)\n",
    "            action_vector[action] = 1\n",
    "            novel.append(action_vector); \n",
    "        else:\n",
    "            novel.append(cur_obs); \n",
    "        returns.append(r)          \n",
    "        batch.add_values(\n",
    "                obs=cur_obs,\n",
    "                actions=action[0],\n",
    "                rewards=r,\n",
    "                dones=done,\n",
    "                new_obs=new_obs)      \n",
    "        cur_obs = new_obs\n",
    "       # print(new_obs)\n",
    "        if offset != 0.0: r -= np.abs(offset)\n",
    "        t += 1\n",
    "        if done:\n",
    "            sample = batch.build_and_reset()\n",
    "            returns = np.array(returns)\n",
    "            sample[Postprocessing.ADVANTAGES] = scipy.signal.lfilter([1], [1, float(-0.9)], returns[::-1], axis=0)[::-1]\n",
    "            break\n",
    "        \n",
    "    \n",
    "    returns = np.array(returns, dtype=np.float64)\n",
    "    novel = np.mean(np.array(novel), axis=0)\n",
    "    return returns, t, novel, sample\n",
    "\n",
    "def eval_rollout(\n",
    "    policy: Policy,\n",
    "    env: gym.Env,\n",
    "):\n",
    "    max_timestep_limit = 999999\n",
    "    returns = []\n",
    "    cur_obs = env.reset()\n",
    "    for _ in range(max_timestep_limit):\n",
    "        action, _, _ = policy.compute_actions([cur_obs], add_noise=False, update=False)\n",
    "        new_obs, r, done, _ = env.step(action[0])\n",
    "        returns.append(r)          \n",
    "        cur_obs = new_obs\n",
    "        if done:\n",
    "            returns = np.array(returns)\n",
    "            break\n",
    "        \n",
    "    returns = np.array(returns, dtype=np.float64)\n",
    "    return returns\n",
    "\n",
    "def make_session(single_threaded):\n",
    "    if not single_threaded:\n",
    "        return tf1.Session()\n",
    "    return tf1.Session(\n",
    "        config=tf1.ConfigProto(\n",
    "            inter_op_parallelism_threads=1, intra_op_parallelism_threads=1\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "class GATFPolicy(Policy):\n",
    "    def __init__(self, obs_space, action_space, config):\n",
    "        super().__init__(obs_space, action_space, config)\n",
    "        self.action_space_struct = get_base_struct_from_space(action_space)\n",
    "        self.action_noise_std = self.config[\"action_noise_std\"]\n",
    "        self.preprocessor = ModelCatalog.get_preprocessor_for_space(obs_space)\n",
    "        self.observation_filter = get_filter(\n",
    "            self.config[\"observation_filter\"], self.preprocessor.shape\n",
    "        )\n",
    "        self.single_threaded = self.config.get(\"single_threaded\", False)\n",
    "        self.config[\"framework\"] = \"tfe\"\n",
    "        if self.config[\"framework\"] == \"tf2\":\n",
    "            self.sess = make_session(single_threaded=self.single_threaded)\n",
    "\n",
    "            # Set graph-level seed.\n",
    "            if config.get(\"seed\") is not None:\n",
    "                with self.sess.as_default():\n",
    "                    tf1.set_random_seed(config[\"seed\"])\n",
    "\n",
    "            self.inputs = tf1.placeholder(\n",
    "                tf.float32, [None] + list(self.preprocessor.shape)\n",
    "            )\n",
    "        else:\n",
    "            if not tf1.executing_eagerly():\n",
    "                tf1.enable_eager_execution()\n",
    "            self.sess = self.inputs = None\n",
    "            if config.get(\"seed\") is not None:\n",
    "                # Tf2.x.\n",
    "                if config.get(\"framework\") == \"tf2\":\n",
    "                    tf.random.set_seed(config[\"seed\"])\n",
    "                # Tf-eager.\n",
    "                elif tf1 and config.get(\"framework\") == \"tfe\":\n",
    "                    tf1.set_random_seed(config[\"seed\"])\n",
    "\n",
    "        # Policy network.\n",
    "        self.dist_class, dist_dim = ModelCatalog.get_action_dist(\n",
    "            self.action_space, self.config[\"model\"], dist_type=\"deterministic\"\n",
    "        )\n",
    "\n",
    "        self.model = ModelCatalog.get_model_v2(\n",
    "            obs_space=self.preprocessor.observation_space,\n",
    "            action_space=action_space,\n",
    "            num_outputs=dist_dim,\n",
    "            model_config=self.config[\"model\"],\n",
    "        )\n",
    "\n",
    "        self.sampler = None\n",
    "        if self.sess:\n",
    "            dist_inputs, _ = self.model({SampleBatch.CUR_OBS: self.inputs})\n",
    "            dist = self.dist_class(dist_inputs, self.model)\n",
    "            self.sampler = dist.sample()\n",
    "            self.variables = ray.experimental.tf_utils.TensorFlowVariables(\n",
    "                dist_inputs, self.sess\n",
    "            )\n",
    "            self.sess.run(tf1.global_variables_initializer())\n",
    "        else:\n",
    "            self.variables = ray.experimental.tf_utils.TensorFlowVariables(\n",
    "                [], None, self.model.variables()\n",
    "            )\n",
    "\n",
    "        self.num_params = sum(\n",
    "            np.prod(variable.shape.as_list())\n",
    "            for _, variable in self.variables.variables.items()\n",
    "        )\n",
    "\n",
    "    @override(Policy)\n",
    "    def compute_actions(self, observation, add_noise=False, update=True, **kwargs):\n",
    "        # Squeeze batch dimension (we always calculate actions for only a\n",
    "        # single obs).\n",
    "        observation = observation[0]\n",
    "        observation = self.preprocessor.transform(observation)\n",
    "        observation = self.observation_filter(observation[None], update=update)\n",
    "        # `actions` is a list of (component) batches.\n",
    "        # Eager mode.\n",
    "        dist_inputs, _ = self.model({SampleBatch.CUR_OBS: observation})\n",
    "        dist = self.dist_class(dist_inputs, self.model)\n",
    "        actions = dist.sample()\n",
    "        actions = tree.map_structure(lambda a: a.numpy(), actions)\n",
    "        return actions, dist_inputs.numpy()[0], {}\n",
    "\n",
    "    def compute_single_action(\n",
    "        self, observation, add_noise=False, update=True, **kwargs\n",
    "    ):\n",
    "        action, state_outs, extra_fetches = self.compute_actions(\n",
    "            [observation], add_noise=add_noise, update=update, **kwargs\n",
    "        )\n",
    "        return action[0], state_outs, extra_fetches\n",
    "\n",
    "    def _add_noise(self, single_action, single_action_space):\n",
    "        if isinstance(\n",
    "            single_action_space, gym.spaces.Box\n",
    "        ) and single_action_space.dtype.name.startswith(\"float\"):\n",
    "            single_action += (\n",
    "                np.random.randn(*single_action.shape) * self.action_noise_std\n",
    "            )\n",
    "        return single_action\n",
    "\n",
    "    def get_state(self):\n",
    "        return {\"state\": self.get_flat_weights()}\n",
    "\n",
    "    def set_state(self, state):\n",
    "        return self.set_flat_weights(state[\"state\"])\n",
    "\n",
    "    def set_flat_weights(self, x):\n",
    "        self.variables.set_flat(x)\n",
    "\n",
    "    def get_flat_weights(self):\n",
    "        return self.variables.get_flat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import namedtuple\n",
    "import logging\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import Trainer\n",
    "from CybORG.Agents.ES.RLLibFiles.trainer_config import TrainerConfig\n",
    "from CybORG.Agents.ES import optimizers, utils\n",
    "from ray.rllib.agents.dqn.dqn_tf_policy import DQNTFPolicy\n",
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "from ray.rllib.env.env_context import EnvContext\n",
    "from ray.rllib.policy.sample_batch import DEFAULT_POLICY_ID\n",
    "from ray.rllib.utils import FilterManager\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.deprecation import Deprecated\n",
    "from ray.rllib.utils.metrics import (\n",
    "    NUM_AGENT_STEPS_SAMPLED,\n",
    "    NUM_AGENT_STEPS_TRAINED,\n",
    "    NUM_ENV_STEPS_SAMPLED,\n",
    "    NUM_ENV_STEPS_TRAINED,\n",
    ")\n",
    "from ray.rllib.utils.torch_utils import set_torch_seed\n",
    "from ray.rllib.utils.typing import TrainerConfigDict\n",
    "from ray.rllib.evaluation.sample_batch_builder import SampleBatchBuilder\n",
    "from ray.rllib.evaluation.postprocessing import Postprocessing\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "Result = namedtuple(\n",
    "    \"Result\",\n",
    "    [\n",
    "        \"noise_indices\",\n",
    "        \"eval_returns\",\n",
    "        \"observations\",\n",
    "        \"novelties\",\n",
    "        \"experience\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "class GAConfig(TrainerConfig):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(trainer_class=GATrainer)\n",
    "\n",
    "        # ES specific settings:\n",
    "        self.action_noise_std = 0.0\n",
    "        self.l2_coeff = 0.005\n",
    "        self.noise_stdev = 0.02\n",
    "        self.noise_decay = 0.995\n",
    "        self.episodes_per_batch = 100\n",
    "        self.reward_coefficient = 0.5\n",
    "        self.store_novelty_probs = 0.03\n",
    "        self.stepsize = 0.01\n",
    "        self.noise_size = 250000000\n",
    "        self.num_workers = 30\n",
    "        self.individuals_per_worker = 1\n",
    "        self.observation_filter = \"MeanStdFilter\"\n",
    "        self.framework = \"tf\"\n",
    "        self.evaluation_config[\"num_envs_per_worker\"] = 1\n",
    "        self.evaluation_config[\"observation_filter\"] = \"NoFilter\"\n",
    "        self.experience_sample_rate = 0.1\n",
    "        self.experience_store_dir = \"tmp\"\n",
    "        self.tourney_size = 12\n",
    "        self.novelty_max_size = 150\n",
    "        self.novelty_k = 25\n",
    "        self.novelty_type = 'action'\n",
    "\n",
    "\n",
    "    @override(TrainerConfig)\n",
    "    def training(\n",
    "        self,\n",
    "        *,\n",
    "        action_noise_std: Optional[float] = None,\n",
    "        noise_stdev: Optional[int] = None,\n",
    "        noise_decay: Optional[float] = None,\n",
    "        episodes_per_batch: Optional[int] = None,\n",
    "        reward_coefficient: Optional[float] = None,\n",
    "        stepsize: Optional[float] = None,\n",
    "        noise_size: Optional[int] = None,\n",
    "        tourney_size: Optional[int] = None,\n",
    "        individuals_per_worker: Optional[int] = None,\n",
    "        store_novelty_probs: Optional[float] = None,\n",
    "        experience_sample_rate: Optional[float] = None,\n",
    "        experience_store_dir: Optional[str] = None,\n",
    "        novelty_max_size: Optional[int] = None,\n",
    "        novelty_k: Optional[int] = None,\n",
    "        novelty_type: Optional[str] = None,\n",
    "        **kwargs,\n",
    "    ) -> \"GAConfig\":\n",
    "        \"\"\"Sets the training related configuration.\n",
    "        Args:\n",
    "            action_noise_std: Std. deviation to be used when adding (standard normal)\n",
    "                noise to computed actions. Action noise is only added, if\n",
    "                `compute_actions` is called with the `add_noise` arg set to True.\n",
    "            l2_coeff: Coefficient to multiply current weights with inside the globalg\n",
    "                optimizer update term.\n",
    "            noise_stdev: Std. deviation of parameter noise.\n",
    "            episodes_per_batch: Minimum number of episodes to pack into the train batch.\n",
    "            store_novelty_probs: Probability of \n",
    "            stepsize: SGD step-size used for the Adam optimizer.\n",
    "            noise_size: Number of rows in the noise table (shared across workers).\n",
    "                Each row contains a gaussian noise value for each model parameter.\n",
    "        Returns:\n",
    "            This updated TrainerConfig object.\n",
    "        \"\"\"\n",
    "        # Pass kwargs onto super's `training()` method.\n",
    "        super().training(**kwargs)\n",
    "\n",
    "        if action_noise_std is not None:\n",
    "            self.action_noise_std = action_noise_std\n",
    "        if noise_stdev is not None:\n",
    "            self.noise_stdev = noise_stdev\n",
    "        if noise_decay is not None:\n",
    "            self.noise_decay = noise_decay\n",
    "        if episodes_per_batch is not None:\n",
    "            self.episodes_per_batch = episodes_per_batch\n",
    "        if reward_coefficient is not None:\n",
    "            self.reward_coefficient = reward_coefficient\n",
    "        if individuals_per_worker is not None:\n",
    "            self.individuals_per_worker = individuals_per_worker\n",
    "        if stepsize is not None:\n",
    "            self.stepsize = stepsize\n",
    "        if noise_size is not None:\n",
    "            self.noise_size = noise_size\n",
    "        if store_novelty_probs is not None:\n",
    "            self.store_novelty_probs = store_novelty_probs\n",
    "        if experience_sample_rate is not None:\n",
    "            self.experience_sample_rate = experience_sample_rate\n",
    "        if experience_store_dir is not None:\n",
    "            self.experience_store_dir = experience_store_dir\n",
    "        if tourney_size is not None:\n",
    "            self.tourney_size = tourney_size\n",
    "        if novelty_max_size is not None:\n",
    "            self.novelty_max_size = novelty_max_size\n",
    "        if novelty_k is not None:\n",
    "            self.novelty_k = novelty_k\n",
    "        if novelty_type is not None:\n",
    "            self.novelty_type = novelty_type\n",
    "        return self\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def create_shared_noise(count):\n",
    "    \"\"\"Create a large array of noise to be shared by all workers.\"\"\"\n",
    "    seed = 123\n",
    "    noise = np.random.RandomState(seed).randn(count).astype(np.float32)\n",
    "    return noise\n",
    "\n",
    "\n",
    "class SharedNoiseTable:\n",
    "    def __init__(self, noise):\n",
    "        self.noise = noise\n",
    "        assert self.noise.dtype == np.float32\n",
    "\n",
    "    def get(self, i, dim):\n",
    "        return self.noise[i : i + dim]\n",
    "\n",
    "    def sample_index(self, dim):\n",
    "        return np.random.randint(0, len(self.noise) - dim + 1)\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class Worker:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        policy_params,\n",
    "        env_creator,\n",
    "        noise,\n",
    "        worker_index,\n",
    "        min_task_runtime=0.2,\n",
    "    ):\n",
    "\n",
    "        # Set Python random, numpy, env, and torch/tf seeds.\n",
    "        seed = config.get(\"seed\")\n",
    "        if seed is not None:\n",
    "            # Python random module.\n",
    "            random.seed(seed)\n",
    "            # Numpy.\n",
    "            np.random.seed(seed)\n",
    "            # Torch.\n",
    "            if config.get(\"framework\") == \"torch\":\n",
    "                set_torch_seed(seed)\n",
    "\n",
    "        self.min_task_runtime = min_task_runtime\n",
    "        self.config = config\n",
    "        self.config.update(policy_params)\n",
    "        self.config[\"single_threaded\"] = False\n",
    "        self.noise = SharedNoiseTable(noise)\n",
    "\n",
    "        env_context = EnvContext(config[\"env_config\"] or {}, worker_index)\n",
    "        self.env = env_creator(env_context)\n",
    "        # Seed the env, if gym.Env.\n",
    "        if not hasattr(self.env, \"seed\"):\n",
    "            logger.info(\"Env doesn't support env.seed(): {}\".format(self.env))\n",
    "        else:\n",
    "            self.env.seed(seed)\n",
    "\n",
    "        from ray.rllib import models\n",
    "\n",
    "        _policy_class = get_policy_class(config)\n",
    "        self.policy = _policy_class(\n",
    "            self.env.observation_space, self.env.action_space, config\n",
    "        )\n",
    "\n",
    "    def rollout(self, timestep_limit, novelty_archive, add_noise=True):\n",
    "        rollout_reward, rollout_fragment_length, obs, batch = rollout(\n",
    "            self.policy, self.env, novelty_archive, timestep_limit=timestep_limit, add_noise=add_noise, novelty_type=self.config[\"novelty_type\"]\n",
    "        )\n",
    "        return rollout_reward, obs, batch\n",
    "    \n",
    "    def evaluation_rollout(self):\n",
    "        rollout_reward = eval_rollout(\n",
    "            self.policy, self.env)\n",
    "        return rollout_reward\n",
    "    \n",
    "    #optimise this? \n",
    "    def calculate_model_weights(self, noise_indexes):\n",
    "        weights = self.config[\"noise_stdev\"] * self.noise.get(noise_indexes[0], self.policy.num_params)\n",
    "        for i in range(1, len(noise_indexes)):\n",
    "            weights += (self.config[\"noise_stdev\"] * self.config[\"noise_decay\"]**i) * self.noise.get(noise_indexes[i], self.policy.num_params)    \n",
    "        return weights \n",
    "    \n",
    "    def euclidean_distance(self, x, y):\n",
    "        n, m = len(x), len(y)\n",
    "        if n > m:\n",
    "            a = np.linalg.norm(y - x[:m])\n",
    "            b = np.linalg.norm(y[-1] - x[m:])\n",
    "        else:\n",
    "            a = np.linalg.norm(x - y[:n])\n",
    "            b = np.linalg.norm(x[-1] - y[n:])\n",
    "        return np.sqrt(a**2 + b**2)\n",
    "\n",
    "    def compute_novelty_vs_archive(self, archive, novelty_vector, k):\n",
    "        if len(archive) < k:\n",
    "            return 0\n",
    "        distances = []\n",
    "        nov = novelty_vector.astype(np.float64)\n",
    "        for point in archive:\n",
    "            distances.append(self.euclidean_distance(point.astype(np.float64), nov))\n",
    "\n",
    "        # Pick k nearest neighbors\n",
    "        distances = np.array(distances)\n",
    "        top_k = np.sort(distances)[:k]\n",
    "        return top_k.mean()\n",
    "    \n",
    "    def do_evaluate(self, noise_indices):\n",
    "        weights = self.calculate_model_weights(noise_indices)\n",
    "        self.policy.set_flat_weights(weights)\n",
    "        reward = []\n",
    "        for i in range(100):\n",
    "            r = self.evaluation_rollout()\n",
    "            reward.append(np.sum(r))\n",
    "        return mean(reward)\n",
    "    \n",
    "    def do_rollouts(self, noise_indices, novelty_archive, timestep_limit=None):\n",
    "        # Set the network weights.\n",
    "        pop = []\n",
    "        rewards = []\n",
    "        observations = []\n",
    "        novelties = []\n",
    "        batch_builder = SampleBatchBuilder()\n",
    "        writer = JsonWriterEdit(os.path.join(ray._private.utils.get_user_temp_dir(), self.config[\"experience_store_dir\"])) \n",
    "        for ni in noise_indices:\n",
    "            weights = self.calculate_model_weights(ni)\n",
    "\n",
    "            # Do a regular run with parameter perturbations.\n",
    "            noise_index = self.noise.sample_index(self.policy.num_params)\n",
    "\n",
    "            perturbation = self.config[\"noise_stdev\"] * self.noise.get(\n",
    "                noise_index, self.policy.num_params\n",
    "            )\n",
    "\n",
    "            self.policy.set_flat_weights(weights + perturbation)\n",
    "            reward = 0; obs = []\n",
    "            for i in range(self.config[\"episodes_per_batch\"]):\n",
    "                r, o, batch = self.rollout(timestep_limit, novelty_archive)\n",
    "                if np.random.rand() <= self.config[\"experience_sample_rate\"]:  \n",
    "                    writer.write(batch)\n",
    "                reward += np.sum(r)\n",
    "                obs.append(o)\n",
    "            rewards.append(reward/self.config[\"episodes_per_batch\"])\n",
    "            obvs = np.mean(obs, axis=0)\n",
    "            observations.append(obvs)\n",
    "            novelties.append(self.compute_novelty_vs_archive(novelty_archive, obvs, self.config[\"novelty_k\"]))\n",
    "            ni.append(noise_index)\n",
    "            pop.append(ni)\n",
    "            \n",
    "        return Result(\n",
    "            noise_indices=pop,\n",
    "            eval_returns=rewards,\n",
    "            observations=observations,  \n",
    "            novelties=novelties,\n",
    "            experience=batch_builder.build_and_reset(),\n",
    "        )\n",
    "\n",
    "\n",
    "def get_policy_class(config):\n",
    "    return GATFPolicy\n",
    "\n",
    "class GATrainer(Trainer):\n",
    "    \"\"\"Large-scale implementation of Evolution Strategies in Ray.\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    @override(Trainer)\n",
    "    def get_default_config(cls) -> TrainerConfigDict:\n",
    "        return GAConfig().to_dict()\n",
    "\n",
    "    @override(Trainer)\n",
    "    def validate_config(self, config: TrainerConfigDict) -> None:\n",
    "        # Call super's validation method.\n",
    "        super().validate_config(config)\n",
    "\n",
    "        if config[\"num_gpus\"] > 1:\n",
    "            raise ValueError(\"`num_gpus` > 1 not yet supported for ES!\")\n",
    "        if config[\"num_workers\"] <= 0:\n",
    "            raise ValueError(\"`num_workers` must be > 0 for ES!\")\n",
    "        if config[\"evaluation_config\"][\"num_envs_per_worker\"] != 1:\n",
    "            raise ValueError(\n",
    "                \"`evaluation_config.num_envs_per_worker` must always be 1 for \"\n",
    "                \"ES! To parallelize evaluation, increase \"\n",
    "                \"`evaluation_num_workers` to > 1.\"\n",
    "            )\n",
    "\n",
    "    @override(Trainer)\n",
    "    def setup(self, config):\n",
    "        # Setup our config: Merge the user-supplied config (which could\n",
    "        # be a partial config dict with the class' default).\n",
    "        if isinstance(config, dict):\n",
    "            self.config = self.merge_trainer_configs(\n",
    "                self.get_default_config(), config, self._allow_unknown_configs\n",
    "            )\n",
    "        else:\n",
    "            self.config = config.to_dict()\n",
    "\n",
    "        # Call super's validation method.\n",
    "        self.validate_config(self.config)\n",
    "\n",
    "        # Generate `self.env_creator` callable to create an env instance.\n",
    "        self.env_creator = self._get_env_creator_from_env_id(self._env_id)\n",
    "        # Generate the local env.\n",
    "        env_context = EnvContext(self.config[\"env_config\"] or {}, worker_index=0)\n",
    "        env = self.env_creator(env_context)\n",
    "\n",
    "        self.callbacks = self.config[\"callbacks\"]()\n",
    "\n",
    "        self._policy_class = get_policy_class(self.config)\n",
    "        self.policy = self._policy_class(\n",
    "            obs_space=env.observation_space,\n",
    "            action_space=env.action_space,\n",
    "            config=self.config,\n",
    "        )\n",
    "\n",
    "        # Create the shared noise table.\n",
    "        logger.info(\"Creating shared noise table.\")\n",
    "        noise_id = create_shared_noise.remote(self.config[\"noise_size\"])\n",
    "        self.noise = SharedNoiseTable(ray.get(noise_id))\n",
    "\n",
    "        # Create the actors.\n",
    "        logger.info(\"Creating actors.\")\n",
    "        self.workers = [\n",
    "            Worker.remote(self.config, {}, self.env_creator, noise_id, idx + 1)\n",
    "            for idx in range(self.config[\"num_workers\"])\n",
    "        ]\n",
    "        \n",
    "        self.population = [[i] for i in range(int(self.config[\"num_workers\"] * self.config[\"individuals_per_worker\"]))]\n",
    "        self.novelty_archive = deque([], maxlen=self.config[\"novelty_max_size\"])\n",
    "        self.novelty_history = []\n",
    "        self.episodes_so_far = 0\n",
    "        self.reward_list = []\n",
    "        self.tstart = time.time()\n",
    "        self.elite = [0]\n",
    "\n",
    "    @override(Trainer)\n",
    "    def get_policy(self, policy=DEFAULT_POLICY_ID):\n",
    "        if policy != DEFAULT_POLICY_ID:\n",
    "            raise ValueError(\n",
    "                \"ES has no policy '{}'! Use {} \"\n",
    "                \"instead.\".format(policy, DEFAULT_POLICY_ID)\n",
    "            )\n",
    "        return self.policy\n",
    "\n",
    "    @override(Trainer)\n",
    "    def step_attempt(self):\n",
    "        config = self.config\n",
    "\n",
    "        #theta = self.policy.get_flat_weights()\n",
    "        #assert theta.dtype == np.float32\n",
    "        #assert len(theta.shape) == 1\n",
    "\n",
    "        # Put the current policy weights in the object store.\n",
    "       # theta_id = ray.put(theta)\n",
    "        # Use the actors to do rollouts. Note that we pass in the ID of the\n",
    "        # policy weights as these are shared.\n",
    "        results = self._collect_results(self.population, self.novelty_archive)\n",
    "        \n",
    "        # Update our sample steps counters.\n",
    "\n",
    "        # Loop over the results.\n",
    "        self.episodes_so_far += 1\n",
    "        # Assemble the results.\n",
    "        returns = []\n",
    "        novelty = []\n",
    "        individuals = []\n",
    "        for i, result in enumerate(results):\n",
    "            returns.extend(result.eval_returns)\n",
    "            novelty.extend(result.novelties)\n",
    "            individuals.extend(result.noise_indices)\n",
    "            if np.random.rand() <= self.config[\"store_novelty_probs\"]:\n",
    "                #print(result.observations)\n",
    "                self.novelty_archive.extend(result.observations)\n",
    "                self.novelty_history.extend(result.observations)\n",
    "            \n",
    "        #Learn GA\n",
    "        novelty = np.array(novelty); returns = np.array(returns)\n",
    "        values = []\n",
    "        for i in range(len(individuals)): \n",
    "            n = (novelty[i] / np.max(novelty)) * (1-self.config[\"reward_coefficient\"]) if np.max(novelty) > 0 else 0\n",
    "            r = ((returns[i]+np.abs(np.min(returns))) / (np.max(returns)+np.abs(np.min(returns)))) * self.config[\"reward_coefficient\"]\n",
    "            values.append(n + r)\n",
    "        values = np.array(values)\n",
    "        \n",
    "        \n",
    "        population = [self.elite]\n",
    "        self.elite = individuals[np.argmax(returns)]\n",
    "        \n",
    "        indexs = np.argpartition(returns, -30)[-30:]\n",
    "        \n",
    "        rollout_ids = [\n",
    "            worker.do_evaluate.remote(individuals[indexs[i]]) for i, worker in enumerate(self.workers)\n",
    "        ]\n",
    "        evals = []\n",
    "        for result in ray.get(rollout_ids):\n",
    "            evals.append(result)\n",
    "        print('eval:' + str(max(evals)))\n",
    "        \n",
    "        indexes = np.arange(len(individuals))\n",
    "        for i in range(len(individuals)-1):\n",
    "            np.random.shuffle(indexes)\n",
    "            winner = np.max(values[indexes[0:self.config[\"tourney_size\"]]])\n",
    "            index = np.where(values == winner)[0][0]\n",
    "            population.append(individuals[index])\n",
    "        \n",
    "        self.population = population\n",
    "        \n",
    "\n",
    "        info = {\n",
    "            \"episodes_so_far\": self.episodes_so_far,\n",
    "        }\n",
    "\n",
    "        result = dict(\n",
    "            episode_reward_mean=mean(returns),\n",
    "            episode_reward_max=max(returns),\n",
    "            episode_novelty_mean=mean(novelty),\n",
    "            episode_elite_eval=max(evals),\n",
    "            #episode_len_mean=eval_lengths.mean(),\n",
    "            #timesteps_this_iter=noisy_lengths.sum(),\n",
    "            info=info,\n",
    "        )\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def calculate_model_weights(self, noise_indexes):\n",
    "        weights = self.config[\"noise_stdev\"] * self.noise.get(noise_indexes[0], self.policy.num_params)\n",
    "        for i in range(1, len(noise_indexes)):\n",
    "            weights += (self.config[\"noise_stdev\"] * self.config[\"noise_decay\"]**i) * self.noise.get(noise_indexes[i], self.policy.num_params)    \n",
    "        return weights\n",
    "\n",
    "    @override(Trainer)\n",
    "    def compute_single_action(self, observation, *args, **kwargs):\n",
    "        action, _, _ = self.policy.compute_actions([observation], update=False)\n",
    "        if kwargs.get(\"full_fetch\"):\n",
    "            return action[0], [], {}\n",
    "        return action[0]\n",
    "\n",
    "    @override(Trainer)\n",
    "    def _sync_weights_to_workers(self, *, worker_set=None, workers=None):\n",
    "        # Broadcast the new policy weights to all evaluation workers.\n",
    "        assert worker_set is not None\n",
    "        logger.info(\"Synchronizing weights to evaluation workers.\")\n",
    "        weights = ray.put(self.policy.get_flat_weights())\n",
    "        worker_set.foreach_policy(lambda p, pid: p.set_flat_weights(ray.get(weights)))\n",
    "\n",
    "    @override(Trainer)\n",
    "    def cleanup(self):\n",
    "        # workaround for https://github.com/ray-project/ray/issues/1516\n",
    "        for w in self.workers:\n",
    "            w.__ray_terminate__.remote()\n",
    "\n",
    "    def _collect_results(self, population, novelty_archive):\n",
    "        num_timesteps = 0\n",
    "        results = []\n",
    "\n",
    "        ind = self.config['individuals_per_worker']\n",
    "        rollout_ids = [\n",
    "            worker.do_rollouts.remote(population[int(i*ind):int((i+1)*ind)], novelty_archive) for i, worker in enumerate(self.workers)\n",
    "        ]\n",
    "        # Get the results of the rollouts.\n",
    "        for result in ray.get(rollout_ids):\n",
    "            results.append(result)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return {\n",
    "            \"weights\": self.policy.get_flat_weights(),\n",
    "            \"filter\": self.policy.observation_filter,\n",
    "            \"episodes_so_far\": self.episodes_so_far,\n",
    "        }\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.episodes_so_far = state[\"episodes_so_far\"]\n",
    "        self.policy.set_flat_weights(state[\"weights\"])\n",
    "        self.policy.observation_filter = state[\"filter\"]\n",
    "        FilterManager.synchronize(\n",
    "            {DEFAULT_POLICY_ID: self.policy.observation_filter}, self.workers\n",
    "        )\n",
    "\n",
    "\n",
    "# Deprecated: Use ray.rllib.algorithms.es.ESConfig instead!\n",
    "class _deprecated_default_config(dict):\n",
    "    def __init__(self):\n",
    "        super().__init__(GAConfig().to_dict())\n",
    "\n",
    "    @Deprecated(\n",
    "        old=\"ray.rllib.algorithms.es.es.DEFAULT_CONFIG\",\n",
    "        new=\"ray.rllib.algorithms.es.es.ESConfig(...)\",\n",
    "        error=False,\n",
    "    )\n",
    "    def __getitem__(self, item):\n",
    "        return super().__getitem__(item)\n",
    "\n",
    "\n",
    "DEFAULT_CONFIG = _deprecated_default_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.offline.json_writer import JsonWriter\n",
    "from ray.rllib.offline.dataset_writer import DatasetWriter\n",
    "from ray.rllib.offline.io_context import IOContext\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "def env_creator(env_config: dict):\n",
    "    path = str(inspect.getfile(CybORG))\n",
    "    path = path[:-10] + '/Shared/Scenarios/Scenario2.yaml'\n",
    "    agents = {\"Red\": B_lineAgent, \"Green\": GreenAgent}\n",
    "    cyborg = CybORG(scenario_file=path, environment='sim', agents=agents)\n",
    "    env = RLlibWrapper(env=cyborg, agent_name=\"Blue\", max_steps=100)\n",
    "    return env\n",
    "\n",
    "def print_results(results_dict):\n",
    "    train_iter = results_dict[\"training_iteration\"]\n",
    "    r_mean = results_dict[\"episode_reward_mean\"]\n",
    "    r_max = results_dict[\"episode_reward_max\"]\n",
    "    r_min = results_dict[\"episode_reward_min\"]\n",
    "    print(f\"{train_iter:4d} \\tr_mean: {r_mean:.1f} \\tr_max: {r_max:.1f} \\tr_min: {r_min: .1f}\")\n",
    "    \n",
    "register_env(name=\"CybORG\", env_creator=env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-25 09:22:23,822\tINFO trainable.py:159 -- Trainable.setup took 10.761 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-06-25 09:22:23,825\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Worker pid=40639)\u001b[0m 2022-06-25 09:22:31,896\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=40644)\u001b[0m 2022-06-25 09:22:32,571\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=40654)\u001b[0m 2022-06-25 09:22:32,560\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=40645)\u001b[0m 2022-06-25 09:22:32,658\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=40660)\u001b[0m 2022-06-25 09:22:32,691\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=40679)\u001b[0m 2022-06-25 09:22:32,761\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=40650)\u001b[0m 2022-06-25 09:22:32,720\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=40646)\u001b[0m 2022-06-25 09:22:32,778\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=40671)\u001b[0m 2022-06-25 09:22:32,903\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=40651)\u001b[0m 2022-06-25 09:22:32,860\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=40667)\u001b[0m 2022-06-25 09:22:32,859\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=40655)\u001b[0m 2022-06-25 09:22:32,904\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=40657)\u001b[0m 2022-06-25 09:22:32,927\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=40662)\u001b[0m 2022-06-25 09:22:32,877\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=40641)\u001b[0m 2022-06-25 09:22:32,885\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=40642)\u001b[0m 2022-06-25 09:22:32,828\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=40652)\u001b[0m 2022-06-25 09:22:33,233\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=40663)\u001b[0m 2022-06-25 09:22:33,068\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=40677)\u001b[0m 2022-06-25 09:22:33,087\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=40675)\u001b[0m 2022-06-25 09:22:33,153\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=40653)\u001b[0m 2022-06-25 09:22:33,202\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=40670)\u001b[0m 2022-06-25 09:22:33,181\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=40656)\u001b[0m 2022-06-25 09:22:33,005\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=40658)\u001b[0m 2022-06-25 09:22:33,136\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=40666)\u001b[0m 2022-06-25 09:22:33,274\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=40648)\u001b[0m 2022-06-25 09:22:33,305\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=40640)\u001b[0m 2022-06-25 09:22:33,299\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=40647)\u001b[0m 2022-06-25 09:22:33,294\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=40643)\u001b[0m 2022-06-25 09:22:33,314\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=40676)\u001b[0m 2022-06-25 09:22:33,355\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval:-626.935\n",
      "  0 reward mean: -738.05, reward max: -554.27, novelty mean:   0.00\n",
      "eval:-534.838\n",
      "  1 reward mean: -702.08, reward max: -504.71, novelty mean:   0.00\n",
      "eval:-424.786\n",
      "  2 reward mean: -658.76, reward max: -381.70, novelty mean:   0.00\n",
      "eval:-289.38000000000005\n",
      "  3 reward mean: -539.00, reward max: -297.84, novelty mean:   0.06\n",
      "eval:-226.35100000000003\n",
      "  4 reward mean: -403.49, reward max: -215.41, novelty mean:   0.07\n",
      "eval:-216.58900000000003\n",
      "  5 reward mean: -307.77, reward max: -205.18, novelty mean:   0.09\n",
      "eval:-207.637\n",
      "  6 reward mean: -256.28, reward max: -201.69, novelty mean:   0.13\n",
      "eval:-197.285\n",
      "  7 reward mean: -232.79, reward max: -194.84, novelty mean:   0.19\n",
      "eval:-188.784\n",
      "  8 reward mean: -225.05, reward max: -183.34, novelty mean:   0.19\n",
      "eval:-182.543\n",
      "  9 reward mean: -210.73, reward max: -165.76, novelty mean:   0.21\n",
      "eval:-178.40300000000002\n",
      " 10 reward mean: -210.02, reward max: -167.48, novelty mean:   0.22\n",
      "eval:-175.657\n",
      " 11 reward mean: -207.38, reward max: -160.50, novelty mean:   0.19\n",
      "eval:-167.989\n",
      " 12 reward mean: -200.99, reward max: -152.26, novelty mean:   0.18\n",
      "eval:-165.00799999999998\n",
      " 13 reward mean: -205.01, reward max: -158.72, novelty mean:   0.19\n",
      "eval:-145.50199999999998\n",
      " 14 reward mean: -198.43, reward max: -131.33, novelty mean:   0.21\n",
      "eval:-143.791\n",
      " 15 reward mean: -200.53, reward max: -147.15, novelty mean:   0.24\n",
      "eval:-144.255\n",
      " 16 reward mean: -190.47, reward max: -130.82, novelty mean:   0.25\n",
      "eval:-138.85399999999998\n",
      " 17 reward mean: -177.82, reward max: -123.04, novelty mean:   0.26\n",
      "eval:-131.319\n",
      " 18 reward mean: -173.89, reward max: -122.58, novelty mean:   0.25\n",
      "eval:-128.19299999999998\n",
      " 19 reward mean: -167.89, reward max: -108.27, novelty mean:   0.26\n",
      "eval:-125.50299999999999\n",
      " 20 reward mean: -179.75, reward max: -126.13, novelty mean:   0.27\n",
      "eval:-127.79099999999998\n",
      " 21 reward mean: -170.41, reward max: -119.30, novelty mean:   0.25\n",
      "eval:-123.06299999999997\n",
      " 22 reward mean: -167.91, reward max: -111.58, novelty mean:   0.23\n",
      "eval:-112.33699999999997\n",
      " 23 reward mean: -165.54, reward max: -117.93, novelty mean:   0.21\n",
      "eval:-113.78099999999998\n",
      " 24 reward mean: -169.50, reward max: -116.06, novelty mean:   0.22\n",
      "eval:-114.57099999999997\n",
      " 25 reward mean: -165.46, reward max: -101.46, novelty mean:   0.22\n",
      "eval:-107.91299999999998\n",
      " 26 reward mean: -149.73, reward max: -100.79, novelty mean:   0.22\n",
      "eval:-107.75199999999998\n",
      " 27 reward mean: -142.73, reward max: -104.22, novelty mean:   0.21\n",
      "eval:-102.47499999999998\n",
      " 28 reward mean: -150.24, reward max: -105.03, novelty mean:   0.21\n",
      "eval:-106.78099999999998\n",
      " 29 reward mean: -143.50, reward max: -97.27, novelty mean:   0.19\n",
      "eval:-105.80799999999998\n",
      " 30 reward mean: -139.49, reward max: -100.58, novelty mean:   0.19\n",
      "eval:-104.68299999999996\n",
      " 31 reward mean: -144.65, reward max: -97.41, novelty mean:   0.19\n",
      "eval:-98.17299999999997\n",
      " 32 reward mean: -152.86, reward max: -94.98, novelty mean:   0.19\n",
      "eval:-103.36799999999998\n",
      " 33 reward mean: -149.19, reward max: -98.22, novelty mean:   0.19\n",
      "eval:-100.71299999999998\n",
      " 34 reward mean: -140.82, reward max: -97.26, novelty mean:   0.19\n",
      "eval:-100.06099999999999\n",
      " 35 reward mean: -143.34, reward max: -91.10, novelty mean:   0.21\n",
      "eval:-96.83599999999998\n",
      " 36 reward mean: -134.88, reward max: -91.12, novelty mean:   0.23\n",
      "eval:-99.31699999999996\n",
      " 37 reward mean: -133.72, reward max: -92.90, novelty mean:   0.22\n",
      "eval:-96.37299999999998\n",
      " 38 reward mean: -136.88, reward max: -86.43, novelty mean:   0.22\n",
      "eval:-100.39899999999997\n",
      " 39 reward mean: -134.79, reward max: -88.18, novelty mean:   0.20\n",
      "eval:-89.94799999999996\n",
      " 40 reward mean: -130.42, reward max: -83.43, novelty mean:   0.19\n",
      "eval:-87.23299999999998\n",
      " 41 reward mean: -134.77, reward max: -86.21, novelty mean:   0.20\n",
      "eval:-87.97999999999998\n",
      " 42 reward mean: -141.07, reward max: -75.59, novelty mean:   0.22\n",
      "eval:-94.25199999999997\n",
      " 43 reward mean: -147.20, reward max: -84.70, novelty mean:   0.21\n",
      "eval:-87.28899999999999\n",
      " 44 reward mean: -124.78, reward max: -87.74, novelty mean:   0.20\n",
      "eval:-82.66899999999998\n",
      " 45 reward mean: -124.58, reward max: -80.90, novelty mean:   0.20\n",
      "eval:-86.35099999999997\n",
      " 46 reward mean: -112.37, reward max: -82.02, novelty mean:   0.21\n",
      "eval:-85.78699999999998\n",
      " 47 reward mean: -113.23, reward max: -81.98, novelty mean:   0.20\n",
      "eval:-84.78599999999997\n",
      " 48 reward mean: -122.08, reward max: -71.35, novelty mean:   0.19\n",
      "eval:-81.56799999999998\n",
      " 49 reward mean: -109.56, reward max: -81.46, novelty mean:   0.18\n",
      "eval:-84.45999999999998\n",
      " 50 reward mean: -109.32, reward max: -79.93, novelty mean:   0.18\n",
      "eval:-82.57499999999997\n",
      " 51 reward mean: -118.88, reward max: -80.30, novelty mean:   0.18\n",
      "eval:-80.97699999999998\n",
      " 52 reward mean: -126.73, reward max: -76.45, novelty mean:   0.20\n",
      "eval:-69.51499999999999\n",
      " 53 reward mean: -139.70, reward max: -67.93, novelty mean:   0.20\n",
      "eval:-79.61099999999998\n",
      " 54 reward mean: -126.29, reward max: -59.99, novelty mean:   0.20\n",
      "eval:-78.124\n",
      " 55 reward mean: -123.96, reward max: -69.30, novelty mean:   0.20\n",
      "eval:-74.68599999999998\n",
      " 56 reward mean: -127.11, reward max: -72.89, novelty mean:   0.20\n",
      "eval:-83.27199999999996\n",
      " 57 reward mean: -134.68, reward max: -74.09, novelty mean:   0.21\n",
      "eval:-78.19899999999997\n",
      " 58 reward mean: -126.94, reward max: -70.82, novelty mean:   0.21\n",
      "eval:-70.45799999999998\n",
      " 59 reward mean: -118.77, reward max: -68.36, novelty mean:   0.19\n",
      "eval:-69.36899999999999\n",
      " 60 reward mean: -122.93, reward max: -67.33, novelty mean:   0.19\n",
      "eval:-73.99099999999999\n",
      " 61 reward mean: -117.46, reward max: -65.30, novelty mean:   0.19\n",
      "eval:-70.62299999999999\n",
      " 62 reward mean: -111.02, reward max: -57.05, novelty mean:   0.20\n",
      "eval:-69.03699999999998\n",
      " 63 reward mean: -117.82, reward max: -67.62, novelty mean:   0.19\n",
      "eval:-69.35799999999998\n",
      " 64 reward mean: -107.94, reward max: -63.13, novelty mean:   0.18\n",
      "eval:-66.77899999999998\n",
      " 65 reward mean: -113.40, reward max: -63.63, novelty mean:   0.19\n",
      "eval:-67.81899999999999\n",
      " 66 reward mean: -104.25, reward max: -60.20, novelty mean:   0.19\n",
      "eval:-68.11799999999998\n",
      " 67 reward mean: -107.08, reward max: -59.32, novelty mean:   0.20\n",
      "eval:-65.07799999999999\n",
      " 68 reward mean: -103.95, reward max: -65.71, novelty mean:   0.20\n",
      "eval:-66.86699999999999\n",
      " 69 reward mean: -107.15, reward max: -64.42, novelty mean:   0.20\n",
      "eval:-67.57999999999998\n",
      " 70 reward mean: -100.70, reward max: -63.10, novelty mean:   0.18\n",
      "eval:-63.082999999999984\n",
      " 71 reward mean: -107.45, reward max: -53.90, novelty mean:   0.18\n",
      "eval:-63.58499999999999\n",
      " 72 reward mean: -105.55, reward max: -52.74, novelty mean:   0.18\n",
      "eval:-65.00899999999999\n",
      " 73 reward mean: -106.76, reward max: -57.78, novelty mean:   0.18\n",
      "eval:-56.82899999999999\n",
      " 74 reward mean: -106.45, reward max: -50.14, novelty mean:   0.18\n",
      "eval:-61.71799999999999\n",
      " 75 reward mean: -106.84, reward max: -58.93, novelty mean:   0.19\n",
      "eval:-55.50699999999999\n",
      " 76 reward mean: -111.24, reward max: -58.74, novelty mean:   0.19\n",
      "eval:-60.887999999999984\n",
      " 77 reward mean: -96.43, reward max: -46.41, novelty mean:   0.19\n",
      "eval:-61.780999999999985\n",
      " 78 reward mean: -96.00, reward max: -50.37, novelty mean:   0.18\n",
      "eval:-58.91099999999999\n",
      " 79 reward mean: -92.83, reward max: -56.45, novelty mean:   0.19\n",
      "eval:-57.541999999999994\n",
      " 80 reward mean: -94.47, reward max: -46.97, novelty mean:   0.19\n",
      "eval:-52.19799999999999\n",
      " 81 reward mean: -97.75, reward max: -47.65, novelty mean:   0.19\n",
      "eval:-50.359\n",
      " 82 reward mean: -101.06, reward max: -46.01, novelty mean:   0.19\n",
      "eval:-51.51599999999999\n",
      " 83 reward mean: -96.29, reward max: -46.20, novelty mean:   0.19\n",
      "eval:-48.36\n",
      " 84 reward mean: -95.24, reward max: -45.30, novelty mean:   0.19\n",
      "eval:-47.31\n",
      " 85 reward mean: -103.09, reward max: -46.54, novelty mean:   0.19\n",
      "eval:-51.79299999999999\n",
      " 86 reward mean: -98.55, reward max: -37.73, novelty mean:   0.19\n",
      "eval:-52.004\n",
      " 87 reward mean: -96.49, reward max: -43.62, novelty mean:   0.18\n",
      "eval:-55.04799999999999\n",
      " 88 reward mean: -104.89, reward max: -49.90, novelty mean:   0.19\n",
      "eval:-58.94499999999999\n",
      " 89 reward mean: -98.06, reward max: -48.55, novelty mean:   0.18\n",
      "eval:-59.26399999999999\n",
      " 90 reward mean: -102.63, reward max: -51.40, novelty mean:   0.17\n",
      "eval:-62.571999999999996\n",
      " 91 reward mean: -110.59, reward max: -50.98, novelty mean:   0.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval:-54.879\n",
      " 92 reward mean: -105.44, reward max: -45.34, novelty mean:   0.17\n",
      "eval:-49.476\n",
      " 93 reward mean: -95.13, reward max: -40.46, novelty mean:   0.18\n",
      "eval:-48.371\n",
      " 94 reward mean: -90.38, reward max: -42.34, novelty mean:   0.18\n",
      "eval:-49.74399999999999\n",
      " 95 reward mean: -87.77, reward max: -42.20, novelty mean:   0.18\n",
      "eval:-44.416\n",
      " 96 reward mean: -86.53, reward max: -38.90, novelty mean:   0.18\n",
      "eval:-45.549\n",
      " 97 reward mean: -87.20, reward max: -35.38, novelty mean:   0.18\n",
      "eval:-43.36\n",
      " 98 reward mean: -78.75, reward max: -38.04, novelty mean:   0.18\n",
      "eval:-42.676\n",
      " 99 reward mean: -76.55, reward max: -35.22, novelty mean:   0.18\n",
      "eval:-43.282\n",
      "100 reward mean: -73.86, reward max: -38.60, novelty mean:   0.18\n",
      "eval:-39.431000000000004\n",
      "101 reward mean: -75.02, reward max: -34.52, novelty mean:   0.17\n",
      "eval:-43.528999999999996\n",
      "102 reward mean: -69.49, reward max: -31.46, novelty mean:   0.17\n",
      "eval:-36.77\n",
      "103 reward mean: -65.62, reward max: -36.29, novelty mean:   0.17\n",
      "eval:-41.657000000000004\n",
      "104 reward mean: -68.79, reward max: -33.96, novelty mean:   0.17\n",
      "eval:-34.873000000000005\n",
      "105 reward mean: -69.84, reward max: -32.54, novelty mean:   0.17\n",
      "eval:-39.214\n",
      "106 reward mean: -69.57, reward max: -31.02, novelty mean:   0.17\n",
      "eval:-39.13699999999999\n",
      "107 reward mean: -70.54, reward max: -30.61, novelty mean:   0.17\n",
      "eval:-35.796\n",
      "108 reward mean: -71.54, reward max: -30.25, novelty mean:   0.17\n",
      "eval:-38.278999999999996\n",
      "109 reward mean: -72.24, reward max: -31.26, novelty mean:   0.17\n",
      "eval:-36.99\n",
      "110 reward mean: -67.29, reward max: -29.83, novelty mean:   0.17\n",
      "eval:-38.922000000000004\n",
      "111 reward mean: -63.32, reward max: -33.57, novelty mean:   0.16\n",
      "eval:-39.515\n",
      "112 reward mean: -65.25, reward max: -28.26, novelty mean:   0.16\n",
      "eval:-36.916\n",
      "113 reward mean: -68.23, reward max: -31.39, novelty mean:   0.16\n",
      "eval:-37.0\n",
      "114 reward mean: -67.18, reward max: -26.61, novelty mean:   0.17\n",
      "eval:-32.021\n",
      "115 reward mean: -64.12, reward max: -27.73, novelty mean:   0.17\n",
      "eval:-36.339\n",
      "116 reward mean: -62.13, reward max: -29.39, novelty mean:   0.16\n",
      "eval:-29.860000000000003\n",
      "117 reward mean: -55.20, reward max: -25.08, novelty mean:   0.16\n",
      "eval:-27.842000000000002\n",
      "118 reward mean: -60.05, reward max: -25.58, novelty mean:   0.16\n",
      "eval:-31.377000000000002\n",
      "119 reward mean: -57.62, reward max: -19.98, novelty mean:   0.17\n",
      "eval:-29.843000000000004\n",
      "120 reward mean: -58.45, reward max: -25.45, novelty mean:   0.16\n",
      "eval:-32.018\n",
      "121 reward mean: -60.18, reward max: -26.21, novelty mean:   0.16\n",
      "eval:-32.617000000000004\n",
      "122 reward mean: -58.61, reward max: -23.94, novelty mean:   0.16\n",
      "eval:-33.063\n",
      "123 reward mean: -56.18, reward max: -28.86, novelty mean:   0.16\n",
      "eval:-31.608000000000004\n",
      "124 reward mean: -59.13, reward max: -23.94, novelty mean:   0.15\n",
      "eval:-27.894000000000005\n",
      "125 reward mean: -54.97, reward max: -25.93, novelty mean:   0.15\n",
      "eval:-26.783000000000005\n",
      "126 reward mean: -53.48, reward max: -22.30, novelty mean:   0.15\n",
      "eval:-28.378000000000004\n",
      "127 reward mean: -57.82, reward max: -25.15, novelty mean:   0.15\n",
      "eval:-27.850000000000005\n",
      "128 reward mean: -58.31, reward max: -26.97, novelty mean:   0.15\n",
      "eval:-28.047000000000004\n",
      "129 reward mean: -60.29, reward max: -25.85, novelty mean:   0.16\n",
      "eval:-29.092000000000002\n",
      "130 reward mean: -62.94, reward max: -25.58, novelty mean:   0.16\n",
      "eval:-29.140000000000004\n",
      "131 reward mean: -67.13, reward max: -19.46, novelty mean:   0.17\n",
      "eval:-26.608000000000004\n",
      "132 reward mean: -56.79, reward max: -21.33, novelty mean:   0.17\n",
      "eval:-24.959000000000003\n",
      "133 reward mean: -59.19, reward max: -24.50, novelty mean:   0.17\n",
      "eval:-27.898000000000007\n",
      "134 reward mean: -58.91, reward max: -25.06, novelty mean:   0.16\n",
      "eval:-26.647000000000006\n",
      "135 reward mean: -64.51, reward max: -25.54, novelty mean:   0.16\n",
      "eval:-26.514000000000006\n",
      "136 reward mean: -72.07, reward max: -24.11, novelty mean:   0.16\n",
      "eval:-26.558000000000007\n",
      "137 reward mean: -70.79, reward max: -22.92, novelty mean:   0.15\n",
      "eval:-26.829000000000004\n",
      "138 reward mean: -55.29, reward max: -24.23, novelty mean:   0.15\n",
      "eval:-26.499000000000006\n",
      "139 reward mean: -58.33, reward max: -24.84, novelty mean:   0.16\n",
      "eval:-24.067000000000007\n",
      "140 reward mean: -45.73, reward max: -22.66, novelty mean:   0.16\n",
      "eval:-23.628000000000007\n",
      "141 reward mean: -51.21, reward max: -23.26, novelty mean:   0.16\n",
      "eval:-25.074000000000005\n",
      "142 reward mean: -45.62, reward max: -23.27, novelty mean:   0.17\n",
      "eval:-24.458000000000006\n",
      "143 reward mean: -48.86, reward max: -22.82, novelty mean:   0.15\n",
      "eval:-25.037000000000006\n",
      "144 reward mean: -58.00, reward max: -21.68, novelty mean:   0.15\n",
      "eval:-25.826000000000004\n",
      "145 reward mean: -49.42, reward max: -22.53, novelty mean:   0.15\n",
      "eval:-26.160000000000007\n",
      "146 reward mean: -46.94, reward max: -21.56, novelty mean:   0.15\n",
      "eval:-25.671000000000006\n",
      "147 reward mean: -50.19, reward max: -22.27, novelty mean:   0.17\n",
      "eval:-25.759000000000004\n",
      "148 reward mean: -50.28, reward max: -22.74, novelty mean:   0.18\n",
      "eval:-26.015000000000008\n",
      "149 reward mean: -46.17, reward max: -22.01, novelty mean:   0.18\n",
      "eval:-26.606000000000005\n",
      "150 reward mean: -56.34, reward max: -22.30, novelty mean:   0.17\n",
      "eval:-24.753000000000004\n",
      "151 reward mean: -48.29, reward max: -21.78, novelty mean:   0.17\n",
      "eval:-23.786000000000005\n",
      "152 reward mean: -48.89, reward max: -21.29, novelty mean:   0.18\n",
      "eval:-25.961000000000006\n",
      "153 reward mean: -49.94, reward max: -22.18, novelty mean:   0.19\n",
      "eval:-25.561000000000007\n",
      "154 reward mean: -55.90, reward max: -19.02, novelty mean:   0.17\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [120]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m means \u001b[38;5;241m=\u001b[39m []; maxs \u001b[38;5;241m=\u001b[39m []; nov \u001b[38;5;241m=\u001b[39m []; evall \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m200\u001b[39m)):\n\u001b[0;32m---> 34\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     means\u001b[38;5;241m.\u001b[39mappend(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     36\u001b[0m     maxs\u001b[38;5;241m.\u001b[39mappend(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward_max\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/tune/trainable.py:360\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warmup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[1;32m    359\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 360\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep() needs to return a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/agents/trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m step_ctx\u001b[38;5;241m.\u001b[39mshould_stop(step_attempt_results):\n\u001b[1;32m   1110\u001b[0m     \u001b[38;5;66;03m# Try to train one step.\u001b[39;00m\n\u001b[1;32m   1111\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1112\u001b[0m         step_attempt_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_attempt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;66;03m# @ray.remote RolloutWorker failure.\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m RayError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1115\u001b[0m         \u001b[38;5;66;03m# Try to recover w/o the failed worker.\u001b[39;00m\n",
      "Input \u001b[0;32mIn [118]\u001b[0m, in \u001b[0;36mGATrainer.step_attempt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    391\u001b[0m \u001b[38;5;66;03m#theta = self.policy.get_flat_weights()\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m#assert theta.dtype == np.float32\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m#assert len(theta.shape) == 1\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;66;03m# Use the actors to do rollouts. Note that we pass in the ID of the\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;66;03m# policy weights as these are shared.\u001b[39;00m\n\u001b[0;32m--> 399\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect_results\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopulation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnovelty_archive\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# Update our sample steps counters.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \n\u001b[1;32m    403\u001b[0m \u001b[38;5;66;03m# Loop over the results.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisodes_so_far \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Input \u001b[0;32mIn [118]\u001b[0m, in \u001b[0;36mGATrainer._collect_results\u001b[0;34m(self, population, novelty_archive)\u001b[0m\n\u001b[1;32m    499\u001b[0m rollout_ids \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    500\u001b[0m     worker\u001b[38;5;241m.\u001b[39mdo_rollouts\u001b[38;5;241m.\u001b[39mremote(population[\u001b[38;5;28mint\u001b[39m(i\u001b[38;5;241m*\u001b[39mind):\u001b[38;5;28mint\u001b[39m((i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mind)], novelty_archive) \u001b[38;5;28;01mfor\u001b[39;00m i, worker \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers)\n\u001b[1;32m    501\u001b[0m ]\n\u001b[1;32m    502\u001b[0m \u001b[38;5;66;03m# Get the results of the rollouts.\u001b[39;00m\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrollout_ids\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    504\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/worker.py:1825\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   1819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1820\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject_refs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must either be an object ref \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1821\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a list of object refs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1822\u001b[0m     )\n\u001b[1;32m   1824\u001b[0m \u001b[38;5;66;03m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[39;00m\n\u001b[0;32m-> 1825\u001b[0m values, debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1826\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(values):\n\u001b[1;32m   1827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayError):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/worker.py:364\u001b[0m, in \u001b[0;36mWorker.get_objects\u001b[0;34m(self, object_refs, timeout)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    359\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to call `get` on the value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobject_ref\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich is not an ray.ObjectRef.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    361\u001b[0m         )\n\u001b[1;32m    363\u001b[0m timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 364\u001b[0m data_metadata_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_ms\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (data, metadata) \u001b[38;5;129;01min\u001b[39;00m data_metadata_pairs:\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:1200\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.get_objects\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:169\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import subprocess\n",
    "from shutil import make_archive\n",
    "model = {\"fcnet_hiddens\": [512,512],\n",
    "         \"fcnet_activation\": \"relu\",}\n",
    "\n",
    "names = [\"85_data_a\", \"9_data_a\"]\n",
    "#names = [\"50_store\",\"150_store\",\"250_store\",\"350_store\"]\n",
    "means_all = []; max_all = []; novel_all = []; archives = []; evals_all = []\n",
    "for i, co in enumerate([.85,.9]):\n",
    "    if os.path.isdir(os.path.join(ray._private.utils.get_user_temp_dir(), names[i])):\n",
    "        shutil.rmtree(os.path.join(ray._private.utils.get_user_temp_dir(), names[i]))\n",
    "\n",
    "    config = GAConfig().training(\n",
    "                             episodes_per_batch=25, \n",
    "                             reward_coefficient=co,\n",
    "                             model=model, \n",
    "                             noise_stdev=0.035,\n",
    "                             noise_decay=0.997,\n",
    "                             store_novelty_probs=0.1,\n",
    "                             individuals_per_worker=4,\n",
    "                             novelty_max_size=10000, #Go as high as you dare\n",
    "                             experience_sample_rate=0,\n",
    "                             tourney_size=12,\n",
    "                             novelty_k=30,\n",
    "                             novelty_type='action', #'action' or 'state'\n",
    "                             experience_store_dir=names[i])\\\n",
    "                             .resources(num_gpus=1).rollouts(num_rollout_workers=30).debugging(log_level='ERROR')\n",
    "    trainer = config.build(env=\"CybORG\")\n",
    "    print(co)\n",
    "    s = \"{:3d} reward mean: {:6.2f}, reward max: {:6.2f}, novelty mean: {:6.2f}\"\n",
    "    means = []; maxs = []; nov = []; evall = []\n",
    "    for j in range(int(200)):\n",
    "        result = trainer.train()\n",
    "        means.append(result[\"episode_reward_mean\"])\n",
    "        maxs.append(result[\"episode_reward_max\"])\n",
    "        nov.append(result[\"episode_novelty_mean\"])\n",
    "        evall.append(result[\"episode_elite_eval\"])\n",
    "        print(s.format(j,result[\"episode_reward_mean\"], result[\"episode_reward_max\"], result[\"episode_novelty_mean\"]))\n",
    "\n",
    "    #Collected data needs to be cleaned \n",
    "    result = subprocess.run(['ls', os.path.join(ray._private.utils.get_user_temp_dir(), names[i])], stdout=subprocess.PIPE)\n",
    "    removed = 0\n",
    "    for j, name in enumerate(str(result.stdout)[2:].split('\\\\n')[0:-1]):\n",
    "        f = open(os.path.join(ray._private.utils.get_user_temp_dir(), names[i], name))\n",
    "        try:\n",
    "            json.load(f)\n",
    "        except ValueError as err:\n",
    "            os.remove(os.path.join(ray._private.utils.get_user_temp_dir(), names[i], name)) \n",
    "            removed += 1\n",
    "    print('Removed ' + str(removed) + ' files, of ' + str(j) + 'files')\n",
    "    make_archive(names[i], 'zip', os.path.join(ray._private.utils.get_user_temp_dir(), names[i]))\n",
    "    \n",
    "    evals_all.append(evall); np.save(names[i]+'_evals.npy', np.array(evall))\n",
    "    means_all.append(means); np.save(names[i]+'_means.npy', np.array(means))\n",
    "    max_all.append(maxs); np.save(names[i]+'_maxs.npy', np.array(maxs))\n",
    "    novel_all.append(nov); np.save(names[i]+'_nov.npy', np.array(nov))\n",
    "    archives.append(np.stack(trainer.novelty_history)); np.save(names[i]+'_archive.npy', np.stack(trainer.novelty_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval:-25.243000000000006\n",
      "154 reward mean: -63.47, reward max: -23.13, novelty mean:   0.18\n",
      "eval:-25.742000000000008\n",
      "155 reward mean: -59.58, reward max: -21.60, novelty mean:   0.17\n",
      "eval:-26.033000000000005\n",
      "156 reward mean: -51.83, reward max: -22.67, novelty mean:   0.16\n",
      "eval:-24.213000000000005\n",
      "157 reward mean: -51.09, reward max: -21.78, novelty mean:   0.16\n",
      "eval:-25.050000000000004\n",
      "158 reward mean: -44.08, reward max: -22.42, novelty mean:   0.15\n",
      "eval:-23.449000000000005\n",
      "159 reward mean: -44.50, reward max: -20.79, novelty mean:   0.15\n",
      "eval:-23.742000000000008\n",
      "160 reward mean: -43.26, reward max: -21.74, novelty mean:   0.15\n",
      "eval:-23.117000000000004\n",
      "161 reward mean: -40.24, reward max: -20.31, novelty mean:   0.14\n",
      "eval:-24.746000000000006\n",
      "162 reward mean: -42.07, reward max: -22.05, novelty mean:   0.14\n",
      "eval:-22.944000000000006\n",
      "163 reward mean: -41.56, reward max: -21.14, novelty mean:   0.15\n",
      "eval:-23.78300000000001\n",
      "164 reward mean: -40.56, reward max: -22.82, novelty mean:   0.16\n",
      "eval:-25.321000000000005\n",
      "165 reward mean: -39.13, reward max: -21.27, novelty mean:   0.15\n",
      "eval:-25.631000000000007\n",
      "166 reward mean: -40.77, reward max: -21.94, novelty mean:   0.15\n",
      "eval:-23.846000000000007\n",
      "167 reward mean: -42.94, reward max: -22.78, novelty mean:   0.14\n",
      "eval:-23.825000000000006\n",
      "168 reward mean: -43.41, reward max: -22.82, novelty mean:   0.12\n",
      "eval:-25.925000000000004\n",
      "169 reward mean: -46.25, reward max: -21.22, novelty mean:   0.10\n",
      "eval:-23.991000000000007\n",
      "170 reward mean: -45.54, reward max: -23.26, novelty mean:   0.11\n",
      "eval:-24.883000000000006\n",
      "171 reward mean: -44.76, reward max: -23.18, novelty mean:   0.11\n",
      "eval:-25.097000000000005\n",
      "172 reward mean: -52.42, reward max: -19.94, novelty mean:   0.12\n",
      "eval:-25.659000000000006\n",
      "173 reward mean: -52.19, reward max: -23.30, novelty mean:   0.13\n",
      "eval:-22.708000000000006\n",
      "174 reward mean: -57.31, reward max: -19.26, novelty mean:   0.14\n",
      "eval:-22.754000000000005\n",
      "175 reward mean: -55.11, reward max: -19.67, novelty mean:   0.14\n",
      "eval:-23.697000000000006\n",
      "176 reward mean: -54.95, reward max: -20.92, novelty mean:   0.14\n",
      "eval:-23.462000000000007\n",
      "177 reward mean: -61.51, reward max: -21.24, novelty mean:   0.14\n",
      "eval:-23.685000000000006\n",
      "178 reward mean: -47.88, reward max: -21.40, novelty mean:   0.14\n",
      "eval:-22.324000000000005\n",
      "179 reward mean: -50.12, reward max: -20.63, novelty mean:   0.13\n",
      "eval:-24.933000000000007\n",
      "180 reward mean: -50.92, reward max: -21.81, novelty mean:   0.14\n",
      "eval:-23.099000000000007\n",
      "181 reward mean: -49.24, reward max: -18.71, novelty mean:   0.13\n",
      "eval:-22.590000000000003\n",
      "182 reward mean: -48.65, reward max: -20.74, novelty mean:   0.13\n",
      "eval:-21.627000000000006\n",
      "183 reward mean: -51.99, reward max: -19.89, novelty mean:   0.13\n",
      "eval:-23.804000000000006\n",
      "184 reward mean: -48.93, reward max: -19.82, novelty mean:   0.13\n",
      "eval:-21.977000000000007\n",
      "185 reward mean: -51.93, reward max: -20.21, novelty mean:   0.13\n",
      "eval:-21.938000000000006\n",
      "186 reward mean: -46.07, reward max: -20.03, novelty mean:   0.12\n",
      "eval:-21.703000000000007\n",
      "187 reward mean: -45.15, reward max: -19.34, novelty mean:   0.12\n",
      "eval:-21.929000000000006\n",
      "188 reward mean: -39.94, reward max: -20.46, novelty mean:   0.13\n",
      "eval:-21.559000000000005\n",
      "189 reward mean: -39.38, reward max: -19.53, novelty mean:   0.13\n",
      "eval:-23.064000000000007\n",
      "190 reward mean: -43.56, reward max: -21.06, novelty mean:   0.14\n",
      "eval:-22.487000000000005\n",
      "191 reward mean: -33.88, reward max: -18.42, novelty mean:   0.15\n",
      "eval:-22.210000000000004\n",
      "192 reward mean: -35.10, reward max: -19.41, novelty mean:   0.13\n",
      "eval:-20.946000000000005\n",
      "193 reward mean: -35.28, reward max: -18.20, novelty mean:   0.12\n",
      "eval:-22.067000000000007\n",
      "194 reward mean: -39.45, reward max: -19.21, novelty mean:   0.12\n",
      "eval:-22.249000000000006\n",
      "195 reward mean: -38.80, reward max: -19.07, novelty mean:   0.12\n",
      "eval:-20.654000000000007\n",
      "196 reward mean: -41.24, reward max: -18.77, novelty mean:   0.13\n",
      "eval:-21.642000000000007\n",
      "197 reward mean: -36.31, reward max: -19.42, novelty mean:   0.13\n",
      "eval:-22.483000000000008\n",
      "198 reward mean: -38.14, reward max: -18.05, novelty mean:   0.12\n",
      "eval:-22.856000000000005\n",
      "199 reward mean: -39.00, reward max: -19.60, novelty mean:   0.13\n",
      "Removed 0 files, of 199files\n"
     ]
    }
   ],
   "source": [
    "for j in range(154,200):\n",
    "    result = trainer.train()\n",
    "    means.append(result[\"episode_reward_mean\"])\n",
    "    maxs.append(result[\"episode_reward_max\"])\n",
    "    nov.append(result[\"episode_novelty_mean\"])\n",
    "    evall.append(result[\"episode_elite_eval\"])\n",
    "    print(s.format(j,result[\"episode_reward_mean\"], result[\"episode_reward_max\"], result[\"episode_novelty_mean\"]))\n",
    "\n",
    "#Collected data needs to be cleaned \n",
    "result = subprocess.run(['ls', os.path.join(ray._private.utils.get_user_temp_dir(), names[i])], stdout=subprocess.PIPE)\n",
    "removed = 0\n",
    "for j, name in enumerate(str(result.stdout)[2:].split('\\\\n')[0:-1]):\n",
    "    f = open(os.path.join(ray._private.utils.get_user_temp_dir(), names[i], name))\n",
    "    try:\n",
    "        json.load(f)\n",
    "    except ValueError as err:\n",
    "        os.remove(os.path.join(ray._private.utils.get_user_temp_dir(), names[i], name)) \n",
    "        removed += 1\n",
    "print('Removed ' + str(removed) + ' files, of ' + str(j) + 'files')\n",
    "make_archive(names[i], 'zip', os.path.join(ray._private.utils.get_user_temp_dir(), names[i]))\n",
    "\n",
    "evals_all.append(evall); np.save(names[i]+'_evals.npy', np.array(evall))\n",
    "means_all.append(means); np.save(names[i]+'_means.npy', np.array(means))\n",
    "max_all.append(maxs); np.save(names[i]+'_maxs.npy', np.array(maxs))\n",
    "novel_all.append(nov); np.save(names[i]+'_nov.npy', np.array(nov))\n",
    "archives.append(np.stack(trainer.novelty_history)); np.save(names[i]+'_archive.npy', np.stack(trainer.novelty_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-26 03:46:25,633\tINFO trainable.py:159 -- Trainable.setup took 11.095 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-06-26 03:46:25,636\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Worker pid=4519)\u001b[0m 2022-06-26 03:46:32,958\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=4495)\u001b[0m 2022-06-26 03:46:33,302\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=4512)\u001b[0m 2022-06-26 03:46:33,287\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=4524)\u001b[0m 2022-06-26 03:46:33,276\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=4493)\u001b[0m 2022-06-26 03:46:33,293\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=4490)\u001b[0m 2022-06-26 03:46:33,214\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=4507)\u001b[0m 2022-06-26 03:46:33,358\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=4489)\u001b[0m 2022-06-26 03:46:33,446\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=4496)\u001b[0m 2022-06-26 03:46:33,379\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=4516)\u001b[0m 2022-06-26 03:46:33,611\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=4494)\u001b[0m 2022-06-26 03:46:33,575\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=4503)\u001b[0m 2022-06-26 03:46:33,544\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=4492)\u001b[0m 2022-06-26 03:46:33,520\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=4502)\u001b[0m 2022-06-26 03:46:33,555\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=4505)\u001b[0m 2022-06-26 03:46:33,700\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=4506)\u001b[0m 2022-06-26 03:46:34,030\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=4501)\u001b[0m 2022-06-26 03:46:33,976\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=4499)\u001b[0m 2022-06-26 03:46:33,986\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=4500)\u001b[0m 2022-06-26 03:46:34,068\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=4498)\u001b[0m 2022-06-26 03:46:34,113\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=4517)\u001b[0m 2022-06-26 03:46:34,108\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=4491)\u001b[0m 2022-06-26 03:46:34,155\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=4525)\u001b[0m 2022-06-26 03:46:34,156\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=4487)\u001b[0m 2022-06-26 03:46:34,219\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=4488)\u001b[0m 2022-06-26 03:46:34,271\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=4510)\u001b[0m 2022-06-26 03:46:34,179\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=4518)\u001b[0m 2022-06-26 03:46:34,425\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=4504)\u001b[0m 2022-06-26 03:46:34,483\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=4514)\u001b[0m 2022-06-26 03:46:34,501\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=4520)\u001b[0m 2022-06-26 03:46:34,569\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval:-604.106\n",
      "  0 reward mean: -743.70, reward max: -567.11, novelty mean:   0.00\n",
      "eval:-557.18\n",
      "  1 reward mean: -707.55, reward max: -485.17, novelty mean:   0.00\n",
      "eval:-433.129\n",
      "  2 reward mean: -655.85, reward max: -414.99, novelty mean:   0.35\n",
      "eval:-310.934\n",
      "  3 reward mean: -523.53, reward max: -287.51, novelty mean:   0.35\n",
      "eval:-234.82500000000005\n",
      "  4 reward mean: -398.51, reward max: -228.35, novelty mean:   0.36\n",
      "eval:-210.80100000000002\n",
      "  5 reward mean: -290.99, reward max: -208.74, novelty mean:   0.38\n",
      "eval:-199.086\n",
      "  6 reward mean: -248.45, reward max: -193.57, novelty mean:   0.40\n",
      "eval:-186.377\n",
      "  7 reward mean: -237.41, reward max: -175.81, novelty mean:   0.39\n",
      "eval:-178.978\n",
      "  8 reward mean: -225.53, reward max: -172.55, novelty mean:   0.44\n",
      "eval:-180.81900000000002\n",
      "  9 reward mean: -217.36, reward max: -179.08, novelty mean:   0.41\n",
      "eval:-175.125\n",
      " 10 reward mean: -199.87, reward max: -164.30, novelty mean:   0.40\n",
      "eval:-169.766\n",
      " 11 reward mean: -199.10, reward max: -166.09, novelty mean:   0.39\n",
      "eval:-170.922\n",
      " 12 reward mean: -192.01, reward max: -163.16, novelty mean:   0.36\n",
      "eval:-164.212\n",
      " 13 reward mean: -192.74, reward max: -158.73, novelty mean:   0.36\n",
      "eval:-150.134\n",
      " 14 reward mean: -187.61, reward max: -161.30, novelty mean:   0.34\n",
      "eval:-154.452\n",
      " 15 reward mean: -192.93, reward max: -154.48, novelty mean:   0.34\n",
      "eval:-148.843\n",
      " 16 reward mean: -182.58, reward max: -151.15, novelty mean:   0.34\n",
      "eval:-149.19299999999998\n",
      " 17 reward mean: -180.47, reward max: -144.18, novelty mean:   0.35\n",
      "eval:-148.35199999999998\n",
      " 18 reward mean: -183.80, reward max: -140.26, novelty mean:   0.33\n",
      "eval:-141.71499999999997\n",
      " 19 reward mean: -181.46, reward max: -141.80, novelty mean:   0.33\n",
      "eval:-129.183\n",
      " 20 reward mean: -180.94, reward max: -132.97, novelty mean:   0.33\n",
      "eval:-136.62599999999998\n",
      " 21 reward mean: -188.24, reward max: -139.05, novelty mean:   0.32\n",
      "eval:-129.196\n",
      " 22 reward mean: -183.39, reward max: -127.50, novelty mean:   0.35\n",
      "eval:-133.23499999999999\n",
      " 23 reward mean: -182.66, reward max: -136.08, novelty mean:   0.35\n",
      "eval:-124.38499999999998\n",
      " 24 reward mean: -171.93, reward max: -117.21, novelty mean:   0.35\n",
      "eval:-126.83799999999998\n",
      " 25 reward mean: -174.83, reward max: -126.63, novelty mean:   0.36\n",
      "eval:-119.78699999999996\n",
      " 26 reward mean: -170.63, reward max: -124.74, novelty mean:   0.39\n",
      "eval:-124.11799999999998\n",
      " 27 reward mean: -165.56, reward max: -120.08, novelty mean:   0.35\n",
      "eval:-121.83899999999998\n",
      " 28 reward mean: -165.19, reward max: -123.55, novelty mean:   0.34\n",
      "eval:-112.36699999999998\n",
      " 29 reward mean: -160.12, reward max: -111.70, novelty mean:   0.34\n",
      "eval:-106.96199999999999\n",
      " 30 reward mean: -156.43, reward max: -104.54, novelty mean:   0.32\n",
      "eval:-108.51599999999998\n",
      " 31 reward mean: -147.12, reward max: -103.54, novelty mean:   0.32\n",
      "eval:-100.37999999999998\n",
      " 32 reward mean: -147.96, reward max: -99.61, novelty mean:   0.33\n",
      "eval:-92.48999999999998\n",
      " 33 reward mean: -137.58, reward max: -84.34, novelty mean:   0.34\n",
      "eval:-90.44699999999997\n",
      " 34 reward mean: -130.29, reward max: -95.40, novelty mean:   0.34\n",
      "eval:-89.96199999999999\n",
      " 35 reward mean: -121.27, reward max: -79.68, novelty mean:   0.35\n",
      "eval:-83.04599999999999\n",
      " 36 reward mean: -116.85, reward max: -77.26, novelty mean:   0.33\n",
      "eval:-80.30899999999998\n",
      " 37 reward mean: -113.97, reward max: -75.80, novelty mean:   0.33\n",
      "eval:-76.06599999999999\n",
      " 38 reward mean: -115.31, reward max: -77.66, novelty mean:   0.30\n",
      "eval:-78.91799999999999\n",
      " 39 reward mean: -107.79, reward max: -78.37, novelty mean:   0.29\n",
      "eval:-76.25299999999999\n",
      " 40 reward mean: -109.96, reward max: -70.44, novelty mean:   0.28\n",
      "eval:-72.57699999999998\n",
      " 41 reward mean: -109.57, reward max: -72.11, novelty mean:   0.28\n",
      "eval:-75.14699999999999\n",
      " 42 reward mean: -111.95, reward max: -64.05, novelty mean:   0.27\n",
      "eval:-73.19399999999999\n",
      " 43 reward mean: -108.20, reward max: -70.04, novelty mean:   0.27\n",
      "eval:-73.803\n",
      " 44 reward mean: -112.79, reward max: -65.88, novelty mean:   0.26\n",
      "eval:-73.689\n",
      " 45 reward mean: -104.41, reward max: -64.10, novelty mean:   0.25\n",
      "eval:-72.448\n",
      " 46 reward mean: -104.52, reward max: -63.51, novelty mean:   0.25\n",
      "eval:-75.72899999999998\n",
      " 47 reward mean: -105.53, reward max: -63.90, novelty mean:   0.25\n",
      "eval:-65.127\n",
      " 48 reward mean: -101.68, reward max: -62.50, novelty mean:   0.24\n",
      "eval:-69.335\n",
      " 49 reward mean: -102.88, reward max: -67.11, novelty mean:   0.24\n",
      "eval:-69.94\n",
      " 50 reward mean: -100.75, reward max: -65.47, novelty mean:   0.24\n",
      "eval:-65.98499999999999\n",
      " 51 reward mean: -94.71, reward max: -53.27, novelty mean:   0.25\n",
      "eval:-62.083999999999996\n",
      " 52 reward mean: -93.13, reward max: -58.94, novelty mean:   0.25\n",
      "eval:-55.337999999999994\n",
      " 53 reward mean: -88.15, reward max: -52.00, novelty mean:   0.24\n",
      "eval:-61.435999999999986\n",
      " 54 reward mean: -89.51, reward max: -52.00, novelty mean:   0.23\n",
      "eval:-58.293\n",
      " 55 reward mean: -87.87, reward max: -55.10, novelty mean:   0.24\n",
      "eval:-59.41799999999999\n",
      " 56 reward mean: -93.43, reward max: -57.44, novelty mean:   0.23\n",
      "eval:-65.592\n",
      " 57 reward mean: -95.73, reward max: -59.22, novelty mean:   0.23\n",
      "eval:-53.214\n",
      " 58 reward mean: -97.40, reward max: -57.41, novelty mean:   0.24\n",
      "eval:-59.73599999999999\n",
      " 59 reward mean: -96.10, reward max: -54.65, novelty mean:   0.24\n",
      "eval:-59.105\n",
      " 60 reward mean: -93.50, reward max: -53.08, novelty mean:   0.24\n",
      "eval:-61.318999999999996\n",
      " 61 reward mean: -91.88, reward max: -50.14, novelty mean:   0.24\n",
      "eval:-62.95599999999998\n",
      " 62 reward mean: -87.55, reward max: -51.66, novelty mean:   0.23\n",
      "eval:-59.84799999999999\n",
      " 63 reward mean: -90.13, reward max: -47.45, novelty mean:   0.23\n",
      "eval:-53.666\n",
      " 64 reward mean: -88.79, reward max: -44.94, novelty mean:   0.23\n",
      "eval:-47.333\n",
      " 65 reward mean: -87.20, reward max: -51.96, novelty mean:   0.22\n",
      "eval:-56.340999999999994\n",
      " 66 reward mean: -94.68, reward max: -46.05, novelty mean:   0.22\n",
      "eval:-46.267999999999994\n",
      " 67 reward mean: -90.16, reward max: -38.03, novelty mean:   0.22\n",
      "eval:-46.518\n",
      " 68 reward mean: -77.21, reward max: -41.39, novelty mean:   0.22\n",
      "eval:-45.841\n",
      " 69 reward mean: -76.73, reward max: -45.63, novelty mean:   0.21\n",
      "eval:-40.662\n",
      " 70 reward mean: -74.38, reward max: -43.54, novelty mean:   0.21\n",
      "eval:-41.271\n",
      " 71 reward mean: -75.23, reward max: -36.00, novelty mean:   0.21\n",
      "eval:-41.144\n",
      " 72 reward mean: -75.20, reward max: -33.28, novelty mean:   0.21\n",
      "eval:-41.307\n",
      " 73 reward mean: -75.93, reward max: -26.76, novelty mean:   0.21\n",
      "eval:-39.134\n",
      " 74 reward mean: -78.72, reward max: -33.65, novelty mean:   0.20\n",
      "eval:-37.29\n",
      " 75 reward mean: -78.55, reward max: -33.58, novelty mean:   0.20\n",
      "eval:-33.81\n",
      " 76 reward mean: -80.98, reward max: -29.99, novelty mean:   0.20\n",
      "eval:-37.63100000000001\n",
      " 77 reward mean: -85.96, reward max: -29.07, novelty mean:   0.20\n",
      "eval:-38.516000000000005\n",
      " 78 reward mean: -78.47, reward max: -33.70, novelty mean:   0.21\n",
      "eval:-39.733000000000004\n",
      " 79 reward mean: -76.62, reward max: -34.69, novelty mean:   0.20\n",
      "eval:-36.86900000000001\n",
      " 80 reward mean: -79.37, reward max: -27.40, novelty mean:   0.21\n",
      "eval:-36.984\n",
      " 81 reward mean: -79.75, reward max: -28.65, novelty mean:   0.22\n",
      "eval:-30.652000000000005\n",
      " 82 reward mean: -81.73, reward max: -21.34, novelty mean:   0.22\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import subprocess\n",
    "from shutil import make_archive\n",
    "model = {\"fcnet_hiddens\": [512,512],\n",
    "         \"fcnet_activation\": \"relu\",}\n",
    "\n",
    "names = [\"8_data_s\"]\n",
    "#names = [\"50_store\",\"150_store\",\"250_store\",\"350_store\"]\n",
    "means_all = []; max_all = []; novel_all = []; archives = []; evals_all = []\n",
    "for i, co in enumerate([.8]):\n",
    "    if os.path.isdir(os.path.join(ray._private.utils.get_user_temp_dir(), names[i])):\n",
    "        shutil.rmtree(os.path.join(ray._private.utils.get_user_temp_dir(), names[i]))\n",
    "\n",
    "    config = GAConfig().training(\n",
    "                             episodes_per_batch=25, \n",
    "                             reward_coefficient=co,\n",
    "                             model=model, \n",
    "                             noise_stdev=0.035,\n",
    "                             noise_decay=0.997,\n",
    "                             store_novelty_probs=0.1,\n",
    "                             individuals_per_worker=4,\n",
    "                             novelty_max_size=10000, #Go as high as you dare\n",
    "                             experience_sample_rate=0,\n",
    "                             tourney_size=12,\n",
    "                             novelty_k=30,\n",
    "                             novelty_type='state', #'action' or 'state'\n",
    "                             experience_store_dir=names[i])\\\n",
    "                             .resources(num_gpus=1).rollouts(num_rollout_workers=30).debugging(log_level='ERROR')\n",
    "    trainer = config.build(env=\"CybORG\")\n",
    "    print(co)\n",
    "    s = \"{:3d} reward mean: {:6.2f}, reward max: {:6.2f}, novelty mean: {:6.2f}\"\n",
    "    means = []; maxs = []; nov = []; evall = []\n",
    "    for j in range(int(200)):\n",
    "        result = trainer.train()\n",
    "        means.append(result[\"episode_reward_mean\"])\n",
    "        maxs.append(result[\"episode_reward_max\"])\n",
    "        nov.append(result[\"episode_novelty_mean\"])\n",
    "        evall.append(result[\"episode_elite_eval\"])\n",
    "        print(s.format(j,result[\"episode_reward_mean\"], result[\"episode_reward_max\"], result[\"episode_novelty_mean\"]))\n",
    "\n",
    "    #Collected data needs to be cleaned \n",
    "    result = subprocess.run(['ls', os.path.join(ray._private.utils.get_user_temp_dir(), names[i])], stdout=subprocess.PIPE)\n",
    "    removed = 0\n",
    "    for j, name in enumerate(str(result.stdout)[2:].split('\\\\n')[0:-1]):\n",
    "        f = open(os.path.join(ray._private.utils.get_user_temp_dir(), names[i], name))\n",
    "        try:\n",
    "            json.load(f)\n",
    "        except ValueError as err:\n",
    "            os.remove(os.path.join(ray._private.utils.get_user_temp_dir(), names[i], name)) \n",
    "            removed += 1\n",
    "    print('Removed ' + str(removed) + ' files, of ' + str(j) + 'files')\n",
    "    make_archive(names[i], 'zip', os.path.join(ray._private.utils.get_user_temp_dir(), names[i]))\n",
    "    \n",
    "    evals_all.append(evall); np.save(names[i]+'_evals.npy', np.array(evall))\n",
    "    means_all.append(means); np.save(names[i]+'_means.npy', np.array(means))\n",
    "    max_all.append(maxs); np.save(names[i]+'_maxs.npy', np.array(maxs))\n",
    "    novel_all.append(nov); np.save(names[i]+'_nov.npy', np.array(nov))\n",
    "    archives.append(np.stack(trainer.novelty_history)); np.save(names[i]+'_archive.npy', np.stack(trainer.novelty_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-25 09:08:13,362\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_evaluate()\u001b[39m (pid=24499, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f9f6b78e2e0>)\n",
      "  File \"<ipython-input-103-b83cfbe6498e>\", line 256, in do_evaluate\n",
      "TypeError: evaluation_rollout() takes 1 positional argument but 2 were given\n",
      "2022-06-25 09:08:13,363\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_evaluate()\u001b[39m (pid=24495, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f71b2f9e2e0>)\n",
      "  File \"<ipython-input-103-b83cfbe6498e>\", line 256, in do_evaluate\n",
      "TypeError: evaluation_rollout() takes 1 positional argument but 2 were given\n",
      "2022-06-25 09:08:13,364\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_evaluate()\u001b[39m (pid=24514, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f83d16c72e0>)\n",
      "  File \"<ipython-input-103-b83cfbe6498e>\", line 256, in do_evaluate\n",
      "TypeError: evaluation_rollout() takes 1 positional argument but 2 were given\n",
      "2022-06-25 09:08:13,365\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_evaluate()\u001b[39m (pid=24518, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f550ec6d2e0>)\n",
      "  File \"<ipython-input-103-b83cfbe6498e>\", line 256, in do_evaluate\n",
      "TypeError: evaluation_rollout() takes 1 positional argument but 2 were given\n",
      "2022-06-25 09:08:13,366\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_evaluate()\u001b[39m (pid=24524, ip=192.168.16.2, repr=<__main__.Worker object at 0x7fca919a1220>)\n",
      "  File \"<ipython-input-103-b83cfbe6498e>\", line 256, in do_evaluate\n",
      "TypeError: evaluation_rollout() takes 1 positional argument but 2 were given\n",
      "2022-06-25 09:08:13,368\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_evaluate()\u001b[39m (pid=24509, ip=192.168.16.2, repr=<__main__.Worker object at 0x7fe0209392b0>)\n",
      "  File \"<ipython-input-103-b83cfbe6498e>\", line 256, in do_evaluate\n",
      "TypeError: evaluation_rollout() takes 1 positional argument but 2 were given\n",
      "2022-06-25 09:08:13,368\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_evaluate()\u001b[39m (pid=24506, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f88bf25d280>)\n",
      "  File \"<ipython-input-103-b83cfbe6498e>\", line 256, in do_evaluate\n",
      "TypeError: evaluation_rollout() takes 1 positional argument but 2 were given\n",
      "2022-06-25 09:08:13,369\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_evaluate()\u001b[39m (pid=24523, ip=192.168.16.2, repr=<__main__.Worker object at 0x7fd2876aa250>)\n",
      "  File \"<ipython-input-103-b83cfbe6498e>\", line 256, in do_evaluate\n",
      "TypeError: evaluation_rollout() takes 1 positional argument but 2 were given\n",
      "2022-06-25 09:08:13,370\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_evaluate()\u001b[39m (pid=24505, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f82b85d8250>)\n",
      "  File \"<ipython-input-103-b83cfbe6498e>\", line 256, in do_evaluate\n",
      "TypeError: evaluation_rollout() takes 1 positional argument but 2 were given\n",
      "2022-06-25 09:08:13,371\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_evaluate()\u001b[39m (pid=24508, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f34fb1b2220>)\n",
      "  File \"<ipython-input-103-b83cfbe6498e>\", line 256, in do_evaluate\n",
      "TypeError: evaluation_rollout() takes 1 positional argument but 2 were given\n",
      "2022-06-25 09:08:13,372\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_evaluate()\u001b[39m (pid=24492, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f6055667220>)\n",
      "  File \"<ipython-input-103-b83cfbe6498e>\", line 256, in do_evaluate\n",
      "TypeError: evaluation_rollout() takes 1 positional argument but 2 were given\n",
      "2022-06-25 09:08:13,372\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_evaluate()\u001b[39m (pid=24512, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f84715491c0>)\n",
      "  File \"<ipython-input-103-b83cfbe6498e>\", line 256, in do_evaluate\n",
      "TypeError: evaluation_rollout() takes 1 positional argument but 2 were given\n",
      "2022-06-25 09:08:13,373\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_evaluate()\u001b[39m (pid=24493, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f2771ebb2e0>)\n",
      "  File \"<ipython-input-103-b83cfbe6498e>\", line 256, in do_evaluate\n",
      "TypeError: evaluation_rollout() takes 1 positional argument but 2 were given\n",
      "2022-06-25 09:08:13,374\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_evaluate()\u001b[39m (pid=24526, ip=192.168.16.2, repr=<__main__.Worker object at 0x7fa0ce888280>)\n",
      "  File \"<ipython-input-103-b83cfbe6498e>\", line 256, in do_evaluate\n",
      "TypeError: evaluation_rollout() takes 1 positional argument but 2 were given\n",
      "2022-06-25 09:08:13,375\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_evaluate()\u001b[39m (pid=24503, ip=192.168.16.2, repr=<__main__.Worker object at 0x7fa416b892e0>)\n",
      "  File \"<ipython-input-103-b83cfbe6498e>\", line 256, in do_evaluate\n",
      "TypeError: evaluation_rollout() takes 1 positional argument but 2 were given\n",
      "2022-06-25 09:08:13,376\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_evaluate()\u001b[39m (pid=24525, ip=192.168.16.2, repr=<__main__.Worker object at 0x7fbbb2aa8220>)\n",
      "  File \"<ipython-input-103-b83cfbe6498e>\", line 256, in do_evaluate\n",
      "TypeError: evaluation_rollout() takes 1 positional argument but 2 were given\n",
      "2022-06-25 09:08:13,377\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_evaluate()\u001b[39m (pid=24497, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f9ce09942b0>)\n",
      "  File \"<ipython-input-103-b83cfbe6498e>\", line 256, in do_evaluate\n",
      "TypeError: evaluation_rollout() takes 1 positional argument but 2 were given\n",
      "2022-06-25 09:08:13,378\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_evaluate()\u001b[39m (pid=24534, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f70cc650280>)\n",
      "  File \"<ipython-input-103-b83cfbe6498e>\", line 256, in do_evaluate\n",
      "TypeError: evaluation_rollout() takes 1 positional argument but 2 were given\n",
      "2022-06-25 09:08:13,378\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_evaluate()\u001b[39m (pid=24522, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f66f91dd280>)\n",
      "  File \"<ipython-input-103-b83cfbe6498e>\", line 256, in do_evaluate\n",
      "TypeError: evaluation_rollout() takes 1 positional argument but 2 were given\n",
      "2022-06-25 09:08:13,379\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_evaluate()\u001b[39m (pid=24496, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f0a067212e0>)\n",
      "  File \"<ipython-input-103-b83cfbe6498e>\", line 256, in do_evaluate\n",
      "TypeError: evaluation_rollout() takes 1 positional argument but 2 were given\n",
      "2022-06-25 09:08:13,380\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_evaluate()\u001b[39m (pid=24507, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f52073d42e0>)\n",
      "  File \"<ipython-input-103-b83cfbe6498e>\", line 256, in do_evaluate\n",
      "TypeError: evaluation_rollout() takes 1 positional argument but 2 were given\n",
      "2022-06-25 09:08:13,381\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_evaluate()\u001b[39m (pid=24498, ip=192.168.16.2, repr=<__main__.Worker object at 0x7fbba61922e0>)\n",
      "  File \"<ipython-input-103-b83cfbe6498e>\", line 256, in do_evaluate\n",
      "TypeError: evaluation_rollout() takes 1 positional argument but 2 were given\n",
      "2022-06-25 09:08:13,382\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_evaluate()\u001b[39m (pid=24528, ip=192.168.16.2, repr=<__main__.Worker object at 0x7efb158af2e0>)\n",
      "  File \"<ipython-input-103-b83cfbe6498e>\", line 256, in do_evaluate\n",
      "TypeError: evaluation_rollout() takes 1 positional argument but 2 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-25 09:08:13,383\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_evaluate()\u001b[39m (pid=24520, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f73e6d472e0>)\n",
      "  File \"<ipython-input-103-b83cfbe6498e>\", line 256, in do_evaluate\n",
      "TypeError: evaluation_rollout() takes 1 positional argument but 2 were given\n",
      "2022-06-25 09:08:13,386\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_evaluate()\u001b[39m (pid=24504, ip=192.168.16.2, repr=<__main__.Worker object at 0x7ff109615220>)\n",
      "  File \"<ipython-input-103-b83cfbe6498e>\", line 256, in do_evaluate\n",
      "TypeError: evaluation_rollout() takes 1 positional argument but 2 were given\n",
      "2022-06-25 09:08:13,387\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_evaluate()\u001b[39m (pid=24511, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f9da34fb2e0>)\n",
      "  File \"<ipython-input-103-b83cfbe6498e>\", line 256, in do_evaluate\n",
      "TypeError: evaluation_rollout() takes 1 positional argument but 2 were given\n",
      "2022-06-25 09:08:13,388\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_evaluate()\u001b[39m (pid=24531, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f277c1fd280>)\n",
      "  File \"<ipython-input-103-b83cfbe6498e>\", line 256, in do_evaluate\n",
      "TypeError: evaluation_rollout() takes 1 positional argument but 2 were given\n",
      "2022-06-25 09:08:13,389\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_evaluate()\u001b[39m (pid=24500, ip=192.168.16.2, repr=<__main__.Worker object at 0x7feb742b81c0>)\n",
      "  File \"<ipython-input-103-b83cfbe6498e>\", line 256, in do_evaluate\n",
      "TypeError: evaluation_rollout() takes 1 positional argument but 2 were given\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "\n",
    "archives = []\n",
    "archives.append(np.load('95_data_a_archive.npy'))\n",
    "archives.append(np.load('9_data_a_archive.npy'))\n",
    "archives.append(np.load('85_data_a_archive.npy'))\n",
    "archives.append(np.load('ppo_novel_actions.npy'))\n",
    "\n",
    "names = ['Reward CoEf: 0.95', 'Reward CoEf: 0.9', 'Reward CoEf: 0.85', 'PPO']\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "runner = 0\n",
    "#reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1)\n",
    "#umap_embedding = reducer.fit_transform(np.concatenate(archives))\n",
    "ax1 = fig.add_subplot(111)\n",
    "for i, a in enumerate(archives[:4]):\n",
    "    ax1.scatter(umap_embedding[runner:runner+len(a),0],umap_embedding[runner:runner+len(a),1], label=names[i], alpha=0.1)\n",
    "    runner += len(a)\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('uMAP Plot of Action Based Novelty Experiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fml.txt') as f:\n",
    "    lines = f.readlines()\n",
    "reward = []\n",
    "for l in lines:\n",
    "    if \"eps 100 is: \" in l: \n",
    "        reward.append(float((l[59:64])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,12))\n",
    "plt.plot(np.load('9_data_s_maxs.npy'), label=\"Reward CoEf: 0.9\")\n",
    "plt.plot(np.load('8_data_s_maxs.npy'), label=\"Reward CoEf: 0.8\")\n",
    "plt.plot(np.load('7_data_s_maxs.npy'), label=\"Reward CoEf: 0.7\")\n",
    "plt.plot(reward[0:200], label=\"PPO\")\n",
    "plt.title('Reward Accumilation Using Action Based Novelty')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim([0,200])\n",
    "plt.ylim([-60,-15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(archives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from matplotlib import animation, rc\n",
    "\n",
    "rewards = np.load('ppo_reward.npy')\n",
    "novel = np.load('ppo_novel.npy')\n",
    "\n",
    "whole = [archives[1]]\n",
    "whole.append(novel)\n",
    "\n",
    "#reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1)\n",
    "#umap_embedding = reducer.fit_transform(np.concatenate(whole))\n",
    "animation_frames = 150\n",
    "runner = 0\n",
    "samples = 0\n",
    "start = 0\n",
    "for i, a in enumerate(archives):\n",
    "    if i == 0:\n",
    "        start = runner\n",
    "        samples = len(a)\n",
    "    runner += len(a)\n",
    "filenames = []\n",
    "for i in range(150):\n",
    "    fig = plt.figure(figsize=(10, 8), dpi=100)\n",
    "    i_line = int(i*(150/animation_frames))\n",
    "    i_scatter = int(i*(samples/animation_frames))\n",
    "    ax = plt.subplot(1, 2, 1)\n",
    "    ax.set_xlim([-4,15])\n",
    "    ax.set_ylim([-4,11])\n",
    "    ax.set_title('uMAP of State Embedding')\n",
    "    ax1 = plt.subplot(1, 2, 2)\n",
    "    ax1.set_xlim([0,150])\n",
    "    ax1.set_ylim([-100,-20])\n",
    "    ax1.set_title('Reward Accumilation')\n",
    "    ax1.set_xlabel('Iteration')\n",
    "    ax1.set_ylabel('Reward')\n",
    "    ax1.plot(max_all[1][:i_line+1], label=\"GA\")\n",
    "    ax1.plot(rewards[:i+1], label=\"PPO\")\n",
    "    ax1.legend(loc='lower right')\n",
    "    ax.scatter(umap_embedding[0:i_scatter+1,0], umap_embedding[0:i_scatter+1,1])\n",
    "    ax.scatter(umap_embedding[samples:samples+i+1,0], umap_embedding[samples:samples+i+1,1])\n",
    "    plt.savefig('gagif/'+str(i)+'.png')\n",
    "    filenames.append('gagif/'+str(i)+'.png')\n",
    "\n",
    "import imageio\n",
    "images = []\n",
    "for filename in filenames:\n",
    "    images.append(imageio.imread(filename))\n",
    "imageio.mimsave('movie.gif', images, duration=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.mimsave('movie.gif', images, duration=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(16,12))\n",
    "ax1 = fig.add_subplot(111)\n",
    "#ax1.plot(max_all[0] ,label=\"Reward CoEf = 0.6, Max\")\n",
    "#ax1.plot(max_all[0] ,label=\"Reward CoEf = 0.7, Max\")\n",
    "ax1.plot(max_all[0] ,label=\"Reward CoEf = 0.7\")\n",
    "ax1.plot(max_all[1],label=\"Reward CoEf = 0.8\")\n",
    "ax1.plot(max_all[2], label=\"Reward CoEf = 0.9\")\n",
    "#ax1.plot(means_all[0], label=\"Reward CoEf = 0.7, Mean\", color='b')\n",
    "#ax1.plot(means_all[1], label=\"Reward CoEf = 0.8, Mean\", color='g')\n",
    "#ax1.plot(means_all[2], label=\"Reward CoEf = 0.9, Mean\", color='r')\n",
    "#ax1.plot(means_all[3], label=\"Reward CoEf = 1, Mean\", color='c')\n",
    "#ax1.plot(means_all[5], label=\"Reward CoEf = 1, Mean\", color='m')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Novelty Experiment (B_lineAgent)')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Max Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "from six.moves.urllib.parse import urlparse\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from smart_open import smart_open\n",
    "except ImportError:\n",
    "    smart_open = None\n",
    "\n",
    "from ray.rllib.policy.sample_batch import MultiAgentBatch\n",
    "from ray.rllib.offline.io_context import IOContext\n",
    "from ray.rllib.offline.output_writer import OutputWriter\n",
    "from ray.rllib.utils.annotations import override, PublicAPI\n",
    "from ray.rllib.utils.compression import pack, compression_supported\n",
    "from ray.rllib.utils.typing import FileType, SampleBatchType\n",
    "from ray.util.ml_utils.json import SafeFallbackEncoder\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "WINDOWS_DRIVES = [chr(i) for i in range(ord(\"c\"), ord(\"z\") + 1)]\n",
    "\n",
    "\n",
    "# TODO(jungong) : use DatasetWriter to back JsonWriter, so we reduce\n",
    "#     codebase complexity without losing existing functionality.\n",
    "@PublicAPI\n",
    "class JsonWriterEdit(OutputWriter):\n",
    "    \"\"\"Writer object that saves experiences in JSON file chunks.\"\"\"\n",
    "\n",
    "    @PublicAPI\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: str,\n",
    "        ioctx: IOContext = None,\n",
    "        max_file_size: int = 64 * 1024 * 1024,\n",
    "        compress_columns: List[str] = frozenset([]), #[\"obs\", \"new_obs\"]\n",
    "    ):\n",
    "        \"\"\"Initializes a JsonWriter instance.\n",
    "\n",
    "        Args:\n",
    "            path: a path/URI of the output directory to save files in.\n",
    "            ioctx: current IO context object.\n",
    "            max_file_size: max size of single files before rolling over.\n",
    "            compress_columns: list of sample batch columns to compress.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        self.ioctx = ioctx or IOContext()\n",
    "        self.max_file_size = max_file_size\n",
    "        self.compress_columns = compress_columns\n",
    "        if urlparse(path).scheme not in [\"\"] + WINDOWS_DRIVES:\n",
    "            self.path_is_uri = True\n",
    "        else:\n",
    "            path = os.path.abspath(os.path.expanduser(path))\n",
    "            # Try to create local dirs if they don't exist\n",
    "            try:\n",
    "                os.makedirs(path)\n",
    "            except OSError:\n",
    "                pass  # already exists\n",
    "            assert os.path.exists(path), \"Failed to create {}\".format(path)\n",
    "            self.path_is_uri = False\n",
    "        self.path = path\n",
    "        self.file_index = 0\n",
    "        self.bytes_written = 0\n",
    "        self.cur_file = None\n",
    "\n",
    "    @override(OutputWriter)\n",
    "    def write(self, sample_batch: SampleBatchType):\n",
    "        start = time.time()\n",
    "        data = _to_json(sample_batch, self.compress_columns)\n",
    "        f = self._get_file()\n",
    "        f.write(data)\n",
    "        if hasattr(f, \"flush\"):  # legacy smart_open impls\n",
    "            f.flush()\n",
    "        self.bytes_written += len(data)\n",
    "        f.close()\n",
    "        logger.debug(\n",
    "            \"Wrote {} bytes to {} in {}s\".format(len(data), f, time.time() - start)\n",
    "        )\n",
    "\n",
    "    def _get_file(self) -> FileType:\n",
    "        timestr = datetime.utcnow().strftime('%H:%M:%S:%f')[:-3]\n",
    "        path = os.path.join(\n",
    "            self.path,\n",
    "            \"output-{}_worker-{}_{}.json\".format(\n",
    "                timestr, self.ioctx.worker_index, self.file_index\n",
    "            ),\n",
    "        )\n",
    "        return open(path, \"w\")\n",
    "\n",
    "\n",
    "def _to_jsonable(v, compress: bool) -> Any:\n",
    "    if compress and compression_supported():\n",
    "        return str(pack(v))\n",
    "    elif isinstance(v, np.ndarray):\n",
    "        return v.tolist()\n",
    "    return v\n",
    "\n",
    "\n",
    "def _to_json_dict(batch: SampleBatchType, compress_columns: List[str]) -> Dict:\n",
    "    out = {}\n",
    "    if isinstance(batch, MultiAgentBatch):\n",
    "        out[\"type\"] = \"MultiAgentBatch\"\n",
    "        out[\"count\"] = batch.count\n",
    "        policy_batches = {}\n",
    "        for policy_id, sub_batch in batch.policy_batches.items():\n",
    "            policy_batches[policy_id] = {}\n",
    "            for k, v in sub_batch.items():\n",
    "                policy_batches[policy_id][k] = _to_jsonable(\n",
    "                    v, compress=k in compress_columns\n",
    "                )\n",
    "        out[\"policy_batches\"] = policy_batches\n",
    "    else:\n",
    "        out[\"type\"] = \"SampleBatch\"\n",
    "        for k, v in batch.items():\n",
    "            out[k] = _to_jsonable(v, compress=k in compress_columns)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _to_json(batch: SampleBatchType, compress_columns: List[str]) -> str:\n",
    "    out = _to_json_dict(batch, compress_columns)\n",
    "    return json.dumps(out, cls=SafeFallbackEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "os.mkdir(os.path.join(ray._private.utils.get_user_temp_dir(), \"8_data_s_large.\"))\n",
    "result = subprocess.run(['ls', os.path.join(ray._private.utils.get_user_temp_dir(), \"8_data_s\")], stdout=subprocess.PIPE)\n",
    "for i, s in enumerate(str(result.stdout)[2:].split('\\\\n')[0:-1]):\n",
    "    if i % 5 == 0:\n",
    "        shutil.copyfile(os.path.join(ray._private.utils.get_user_temp_dir(), \"8_data_s\",s), os.path.join(ray._private.utils.get_user_temp_dir(), \"8_data_s_smallish\",s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "result = subprocess.run(['ls', os.path.join(ray._private.utils.get_user_temp_dir(), \"8_data_s_smallish\")], stdout=subprocess.PIPE)\n",
    "len(str(result.stdout)[2:].split('\\\\n')[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Worker pid=8196)\u001b[0m 2022-06-24 14:55:03,071\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8184)\u001b[0m 2022-06-24 14:55:03,139\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8182)\u001b[0m 2022-06-24 14:55:03,318\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8197)\u001b[0m 2022-06-24 14:55:03,456\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8199)\u001b[0m 2022-06-24 14:55:03,669\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8213)\u001b[0m 2022-06-24 14:55:03,658\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8201)\u001b[0m 2022-06-24 14:55:03,750\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8204)\u001b[0m 2022-06-24 14:55:03,842\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8176)\u001b[0m 2022-06-24 14:55:03,964\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8202)\u001b[0m 2022-06-24 14:55:03,955\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8174)\u001b[0m 2022-06-24 14:55:03,919\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8185)\u001b[0m 2022-06-24 14:55:04,005\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8181)\u001b[0m 2022-06-24 14:55:04,075\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8183)\u001b[0m 2022-06-24 14:55:04,112\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8187)\u001b[0m 2022-06-24 14:55:04,132\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8195)\u001b[0m 2022-06-24 14:55:04,197\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8179)\u001b[0m 2022-06-24 14:55:04,206\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8198)\u001b[0m 2022-06-24 14:55:04,226\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8188)\u001b[0m 2022-06-24 14:55:04,207\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8216)\u001b[0m 2022-06-24 14:55:04,233\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8212)\u001b[0m 2022-06-24 14:55:04,243\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8180)\u001b[0m 2022-06-24 14:55:04,320\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8175)\u001b[0m 2022-06-24 14:55:04,341\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8203)\u001b[0m 2022-06-24 14:55:04,422\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "2022-06-24 14:55:08,408\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8184, ip=192.168.16.2, repr=<__main__.Worker object at 0x7fddf218e940>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 92 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:08,410\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8178, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f705f06da00>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 131 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:08,411\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8196, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f7cab16aa30>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 90 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:08,412\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8208, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f3558158970>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 118 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:08,413\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8211, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f7ebcd5da30>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 59 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:08,413\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8205, ip=192.168.16.2, repr=<__main__.Worker object at 0x7fe2493899d0>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 75 is out of bounds for axis 0 with size 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-24 14:55:08,414\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8200, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f6ec43458e0>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 124 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:09,408\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8181, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f1a8648b970>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 81 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:09,409\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8182, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f964581da30>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 94 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:09,410\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8204, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f0856f16a30>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 67 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:09,411\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8176, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f3a074199d0>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 99 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:09,412\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8197, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f31dc361970>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 84 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:09,413\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8185, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f83ff3cc910>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 91 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:09,414\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8201, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f0d58d0e9d0>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 69 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:09,415\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8174, ip=192.168.16.2, repr=<__main__.Worker object at 0x7fa9103db970>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 119 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:09,416\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8183, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f3d5cded9d0>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 121 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:09,418\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8195, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f555a6dea30>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 69 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:09,419\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8199, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f83e6d14a30>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 140 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:09,420\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8202, ip=192.168.16.2, repr=<__main__.Worker object at 0x7fafaa90ea30>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 141 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:09,420\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8213, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f59330b2970>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 116 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:10,409\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8198, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f16d9da5910>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 129 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:10,410\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8179, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f85b2dbea30>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 80 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:10,411\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8212, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f71322f7970>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 66 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:10,412\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8216, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f7f59dbaa00>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 120 is out of bounds for axis 0 with size 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-24 14:55:10,412\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8203, ip=192.168.16.2, repr=<__main__.Worker object at 0x7feb2f493a30>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 135 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:10,413\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8188, ip=192.168.16.2, repr=<__main__.Worker object at 0x7ff0b83ac9d0>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 117 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:10,414\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8180, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f0947c6c9d0>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 88 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:10,415\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8187, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f75eb80aa30>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 81 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:10,416\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8175, ip=192.168.16.2, repr=<__main__.Worker object at 0x7ef778ba2970>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 101 is out of bounds for axis 0 with size 54\n"
     ]
    }
   ],
   "source": [
    "from shutil import make_archive\n",
    "make_archive(\"8_data_s_small\", 'zip', os.path.join(ray._private.utils.get_user_temp_dir(), \"8_data_s_small\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
