{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Box\n",
    "import numpy as np\n",
    "\n",
    "from ray.rllib.models.modelv2 import ModelV2\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models.tf.fcnet import FullyConnectedNetwork\n",
    "from ray.rllib.models.torch.misc import SlimFC\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "from ray.rllib.agents.ppo.ppo import PPOTrainer\n",
    "from ray.rllib.agents.ppo import ppo\n",
    "from ray.rllib.models.tf.tf_action_dist import Categorical\n",
    "\n",
    "tf1, tf, tfv = try_import_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HieracrchyModel(TFModelV2):\n",
    "    \n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        super(HieracrchyModel, self).__init__(\n",
    "            obs_space, action_space, num_outputs, model_config, name)\n",
    "        # Base of the model for PPO agents\n",
    "        self.model = FullyConnectedNetwork(\n",
    "            obs_space, 2, 2, model_config, name\n",
    "        )\n",
    "\n",
    "        ppo_config = ppo.DEFAULT_CONFIG.copy()\n",
    "        ppo_config.update({\"num_gpus\": 0,\"num_workers\": 0,\n",
    "                # Also, use \"framework: tf2\" for tfe eager execution.\n",
    "                \"framework\": \"tf2\",\n",
    "                \"train_batch_size\": batch_size,\n",
    "                \"horizon\": 100,\n",
    "                \"gamma\": 0.95,\n",
    "                \"model\": {\n",
    "                    \"fcnet_hiddens\": [512, 512],\n",
    "                    \"fcnet_activation\": \"relu\",\n",
    "                }})\n",
    "        \n",
    "        b_line = PPOTrainer(config=ppo_config,env=\"CybORG\")\n",
    "        b_line.restore(\"b_line_agent/checkpoint_000109/checkpoint-109\")\n",
    "        meander = PPOTrainer(config=ppo_config,env=\"CybORG\")\n",
    "        meander.restore(\"supervisor_ppo/checkpoint_000183/checkpoint-183\")\n",
    "        self.sub_agents = [b_line.get_policy().model, meander.get_policy().model]\n",
    "        \n",
    "        self.action = 0\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        action_logits = self.model.forward({'obs_flat': input_dict[SampleBatch.CUR_OBS]}, state, seq_lens)[0]\n",
    "        self.action = tf.random.categorical(action_logits, 1, dtype=tf.int32)[0].numpy()[0]\n",
    "        sub_logits = self.sub_agents[self.action].forward({'obs_flat': input_dict[SampleBatch.CUR_OBS]}, state, seq_lens)[0]\n",
    "        return sub_logits, state\n",
    "    \n",
    "    @override(ModelV2)\n",
    "    def value_function(self):\n",
    "        return self.model.value_function()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'HieracrchyModel' has no attribute 'build'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild\u001b[39m(policy, obs_space, action_space, config) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ModelV2:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ModelCatalog\u001b[38;5;241m.\u001b[39mget_model_v2(obs_space,\n\u001b[1;32m     20\u001b[0m             \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     21\u001b[0m             \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m             framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m             model_interface\u001b[38;5;241m=\u001b[39mHieracrchyModel)\n\u001b[1;32m     27\u001b[0m HierarchyPolicy \u001b[38;5;241m=\u001b[39m PPOTFPolicy\u001b[38;5;241m.\u001b[39mwith_updates(\n\u001b[1;32m     28\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHierarchyPPOPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     29\u001b[0m     loss_fn\u001b[38;5;241m=\u001b[39mloss,\n\u001b[0;32m---> 30\u001b[0m     make_model\u001b[38;5;241m=\u001b[39m\u001b[43mHieracrchyModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m,\n\u001b[1;32m     31\u001b[0m     extra_action_out_fn\u001b[38;5;241m=\u001b[39mafter_action)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mHieracrchyTrainer\u001b[39;00m(PPOTrainer):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_default_policy_class\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'HieracrchyModel' has no attribute 'build'"
     ]
    }
   ],
   "source": [
    "from ray.rllib.agents.ppo.ppo_tf_policy import PPOTFPolicy\n",
    "from ray.rllib.agents.ppo.ppo_tf_policy import ppo_surrogate_loss\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.agents.ppo.ppo import PPOTrainer\n",
    "\n",
    "def after_action(policy):\n",
    "    if isinstance(policy.model, tf.keras.Model):\n",
    "        return {}\n",
    "    return {\n",
    "        SampleBatch.AGENT_INDEX: [policy.model.action],\n",
    "        SampleBatch.VF_PREDS: policy.model.value_function(),\n",
    "    } \n",
    "\n",
    "def loss(self, model, dist_class, train_batch):\n",
    "    train_batch[SampleBatch.ACTIONS] = train_batch[SampleBatch.AGENT_INDEX]\n",
    "    return ppo_surrogate_loss(self, model, dist_class, train_batch)\n",
    "\n",
    "def build_model(policy, obs_space, action_space, config) -> ModelV2:\n",
    "    return ModelCatalog.get_model_v2(obs_space,\n",
    "            2,\n",
    "            2,\n",
    "            config[\"model\"],\n",
    "            name=\"option_critic_model\",\n",
    "                                    \n",
    "            framework=\"tf\",\n",
    "            model_interface=HieracrchyModel)\n",
    "\n",
    "HierarchyPolicy = PPOTFPolicy.with_updates(\n",
    "    name=\"HierarchyPPOPolicy\",\n",
    "    loss_fn=loss,\n",
    "    make_model=build_model,\n",
    "    extra_action_out_fn=after_action)\n",
    "\n",
    "class HieracrchyTrainer(PPOTrainer):\n",
    "    def get_default_policy_class(self, config):\n",
    "        return HierarchyPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2945)\u001b[0m 2022-08-04 15:57:26,781\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2985)\u001b[0m 2022-08-04 15:57:26,779\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2943)\u001b[0m 2022-08-04 15:57:26,762\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2945)\u001b[0m 2022-08-04 15:57:26,904\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=2945, ip=172.28.0.2, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f38288c10a0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2945)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2945)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2945)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2945)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2945)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2945)\u001b[0m     self[policy_id] = class_(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2945)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/eager_tf_policy.py\", line 405, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2945)\u001b[0m     self.model = make_model(self, observation_space, action_space, config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2945)\u001b[0m   File \"<ipython-input-9-952e9e988812>\", line 43, in build\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2945)\u001b[0m   File \"<ipython-input-12-f25df86380b2>\", line 7, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2945)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/models/tf/fcnet.py\", line 56, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2945)\u001b[0m     shape=(int(np.product(obs_space.shape)),), name=\"observations\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2945)\u001b[0m AttributeError: 'HierarchyPPOPolicy_eager' object has no attribute 'shape'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2985)\u001b[0m 2022-08-04 15:57:26,912\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=2985, ip=172.28.0.2, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f41c5d530d0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2985)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2985)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2985)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2985)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2985)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2985)\u001b[0m     self[policy_id] = class_(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2985)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/eager_tf_policy.py\", line 405, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2985)\u001b[0m     self.model = make_model(self, observation_space, action_space, config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2985)\u001b[0m   File \"<ipython-input-9-952e9e988812>\", line 43, in build\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2985)\u001b[0m   File \"<ipython-input-12-f25df86380b2>\", line 7, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2985)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/models/tf/fcnet.py\", line 56, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2985)\u001b[0m     shape=(int(np.product(obs_space.shape)),), name=\"observations\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2985)\u001b[0m AttributeError: 'HierarchyPPOPolicy_eager' object has no attribute 'shape'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2943)\u001b[0m 2022-08-04 15:57:26,884\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=2943, ip=172.28.0.2, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f797d9190d0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2943)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2943)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2943)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2943)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2943)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2943)\u001b[0m     self[policy_id] = class_(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2943)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/eager_tf_policy.py\", line 405, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2943)\u001b[0m     self.model = make_model(self, observation_space, action_space, config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2943)\u001b[0m   File \"<ipython-input-9-952e9e988812>\", line 43, in build\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2943)\u001b[0m   File \"<ipython-input-12-f25df86380b2>\", line 7, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2943)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/models/tf/fcnet.py\", line 56, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2943)\u001b[0m     shape=(int(np.product(obs_space.shape)),), name=\"observations\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2943)\u001b[0m AttributeError: 'HierarchyPPOPolicy_eager' object has no attribute 'shape'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2952)\u001b[0m 2022-08-04 15:57:26,868\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3114)\u001b[0m 2022-08-04 15:57:26,854\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "ename": "RayActorError",
     "evalue": "The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=2943, ip=172.28.0.2, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f797d9190d0>)\n  File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n    self._build_policy_map(\n  File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n    self.policy_map.create_policy(\n  File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/policy_map.py\", line 146, in create_policy\n    self[policy_id] = class_(\n  File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/eager_tf_policy.py\", line 405, in __init__\n    self.model = make_model(self, observation_space, action_space, config)\n  File \"<ipython-input-9-952e9e988812>\", line 43, in build\n  File \"<ipython-input-12-f25df86380b2>\", line 7, in __init__\n  File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/models/tf/fcnet.py\", line 56, in __init__\n    shape=(int(np.product(obs_space.shape)),), name=\"observations\"\nAttributeError: 'HierarchyPPOPolicy_eager' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/agents/trainer.py:935\u001b[0m, in \u001b[0;36mTrainer.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 935\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;66;03m# New design: Override `Trainable.setup()` (as indented by Trainable)\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;66;03m# and do or don't call super().setup() from within your override.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# By default, `super().setup()` will create both worker sets:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;66;03m# parallel to training.\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;66;03m# TODO: Deprecate `_init()` and remove this try/except block.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/agents/trainer.py:1074\u001b[0m, in \u001b[0;36mTrainer._init\u001b[0;34m(self, config, env_creator)\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_init\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: TrainerConfigDict, env_creator: EnvCreator) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1074\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRayActorError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m allrewards \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     46\u001b[0m config\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_gpus\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_workers\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m     47\u001b[0m                 \u001b[38;5;66;03m# Also, use \"framework: tf2\" for tfe eager execution.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframework\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m \n\u001b[1;32m     58\u001b[0m                 }) \n\u001b[0;32m---> 59\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mHieracrchyTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCybORG\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m reward \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     62\u001b[0m novel_obs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/agents/trainer.py:870\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, config, env, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;66;03m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;66;03m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;66;03m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;66;03m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward_max\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    867\u001b[0m     }\n\u001b[1;32m    868\u001b[0m }\n\u001b[0;32m--> 870\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremote_checkpoint_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msync_function_tpl\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/tune/trainable.py:156\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[1;32m    154\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_ip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_current_ip()\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/agents/trainer.py:950\u001b[0m, in \u001b[0;36mTrainer.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;66;03m# New design: Override `Trainable.setup()` (as indented by Trainable)\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;66;03m# and do or don't call super().setup() from within your override.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# By default, `super().setup()` will create both worker sets:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;66;03m# parallel to training.\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;66;03m# TODO: Deprecate `_init()` and remove this try/except block.\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;66;03m# Only if user did not override `_init()`:\u001b[39;00m\n\u001b[1;32m    945\u001b[0m     \u001b[38;5;66;03m# - Create rollout workers here automatically.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;66;03m# This matches the behavior of using `build_trainer()`, which\u001b[39;00m\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;66;03m# has been deprecated.\u001b[39;00m\n\u001b[0;32m--> 950\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m=\u001b[39m \u001b[43mWorkerSet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_workers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;66;03m# By default, collect metrics for all remote workers.\u001b[39;00m\n\u001b[1;32m    960\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remote_workers_for_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\u001b[38;5;241m.\u001b[39mremote_workers()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/worker_set.py:142\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, policy_class, trainer_config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Create a local worker, if needed.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# If num_workers > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# the first remote worker (which does have an env).\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    134\u001b[0m     local_worker\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remote_workers\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m     )\n\u001b[1;32m    141\u001b[0m ):\n\u001b[0;32m--> 142\u001b[0m     remote_spaces \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_policy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpid\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     spaces \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    148\u001b[0m         e[\u001b[38;5;241m0\u001b[39m]: (\u001b[38;5;28mgetattr\u001b[39m(e[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_space\u001b[39m\u001b[38;5;124m\"\u001b[39m, e[\u001b[38;5;241m1\u001b[39m]), e[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m remote_spaces\n\u001b[1;32m    150\u001b[0m     }\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# Try to add the actual env's obs/action spaces.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/worker.py:1833\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   1831\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m   1832\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1833\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m   1835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_individual_id:\n\u001b[1;32m   1836\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mRayActorError\u001b[0m: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=2943, ip=172.28.0.2, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f797d9190d0>)\n  File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n    self._build_policy_map(\n  File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n    self.policy_map.create_policy(\n  File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/policy_map.py\", line 146, in create_policy\n    self[policy_id] = class_(\n  File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/eager_tf_policy.py\", line 405, in __init__\n    self.model = make_model(self, observation_space, action_space, config)\n  File \"<ipython-input-9-952e9e988812>\", line 43, in build\n  File \"<ipython-input-12-f25df86380b2>\", line 7, in __init__\n  File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/models/tf/fcnet.py\", line 56, in __init__\n    shape=(int(np.product(obs_space.shape)),), name=\"observations\"\nAttributeError: 'HierarchyPPOPolicy_eager' object has no attribute 'shape'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2984)\u001b[0m 2022-08-04 15:57:26,936\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2952)\u001b[0m 2022-08-04 15:57:26,987\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=2952, ip=172.28.0.2, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f0bc6325070>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2952)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2952)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2952)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2952)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2952)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2952)\u001b[0m     self[policy_id] = class_(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2952)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/eager_tf_policy.py\", line 405, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2952)\u001b[0m     self.model = make_model(self, observation_space, action_space, config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2952)\u001b[0m   File \"<ipython-input-9-952e9e988812>\", line 43, in build\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2952)\u001b[0m   File \"<ipython-input-12-f25df86380b2>\", line 7, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2952)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/models/tf/fcnet.py\", line 56, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2952)\u001b[0m     shape=(int(np.product(obs_space.shape)),), name=\"observations\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2952)\u001b[0m AttributeError: 'HierarchyPPOPolicy_eager' object has no attribute 'shape'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2962)\u001b[0m 2022-08-04 15:57:27,034\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3114)\u001b[0m 2022-08-04 15:57:26,950\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3114, ip=172.28.0.2, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fa3a44b10d0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3114)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3114)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3114)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3114)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3114)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3114)\u001b[0m     self[policy_id] = class_(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3114)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/eager_tf_policy.py\", line 405, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3114)\u001b[0m     self.model = make_model(self, observation_space, action_space, config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3114)\u001b[0m   File \"<ipython-input-9-952e9e988812>\", line 43, in build\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3114)\u001b[0m   File \"<ipython-input-12-f25df86380b2>\", line 7, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3114)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/models/tf/fcnet.py\", line 56, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3114)\u001b[0m     shape=(int(np.product(obs_space.shape)),), name=\"observations\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3114)\u001b[0m AttributeError: 'HierarchyPPOPolicy_eager' object has no attribute 'shape'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2950)\u001b[0m 2022-08-04 15:57:26,982\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2984)\u001b[0m 2022-08-04 15:57:27,065\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=2984, ip=172.28.0.2, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7f591370d0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2984)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2984)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2984)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2984)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2984)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2984)\u001b[0m     self[policy_id] = class_(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2984)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/eager_tf_policy.py\", line 405, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2984)\u001b[0m     self.model = make_model(self, observation_space, action_space, config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2984)\u001b[0m   File \"<ipython-input-9-952e9e988812>\", line 43, in build\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2984)\u001b[0m   File \"<ipython-input-12-f25df86380b2>\", line 7, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2984)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/models/tf/fcnet.py\", line 56, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2984)\u001b[0m     shape=(int(np.product(obs_space.shape)),), name=\"observations\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2984)\u001b[0m AttributeError: 'HierarchyPPOPolicy_eager' object has no attribute 'shape'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2946)\u001b[0m 2022-08-04 15:57:27,067\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2962)\u001b[0m 2022-08-04 15:57:27,140\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=2962, ip=172.28.0.2, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fc3181e30d0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2962)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2962)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2962)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2962)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2962)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2962)\u001b[0m     self[policy_id] = class_(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2962)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/eager_tf_policy.py\", line 405, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2962)\u001b[0m     self.model = make_model(self, observation_space, action_space, config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2962)\u001b[0m   File \"<ipython-input-9-952e9e988812>\", line 43, in build\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2962)\u001b[0m   File \"<ipython-input-12-f25df86380b2>\", line 7, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2962)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/models/tf/fcnet.py\", line 56, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2962)\u001b[0m     shape=(int(np.product(obs_space.shape)),), name=\"observations\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2962)\u001b[0m AttributeError: 'HierarchyPPOPolicy_eager' object has no attribute 'shape'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2950)\u001b[0m 2022-08-04 15:57:27,092\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=2950, ip=172.28.0.2, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fb0a40dc070>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2950)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2950)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2950)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2950)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2950)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2950)\u001b[0m     self[policy_id] = class_(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2950)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/eager_tf_policy.py\", line 405, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2950)\u001b[0m     self.model = make_model(self, observation_space, action_space, config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2950)\u001b[0m   File \"<ipython-input-9-952e9e988812>\", line 43, in build\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2950)\u001b[0m   File \"<ipython-input-12-f25df86380b2>\", line 7, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2950)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/models/tf/fcnet.py\", line 56, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2950)\u001b[0m     shape=(int(np.product(obs_space.shape)),), name=\"observations\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2950)\u001b[0m AttributeError: 'HierarchyPPOPolicy_eager' object has no attribute 'shape'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3102)\u001b[0m 2022-08-04 15:57:27,075\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3199)\u001b[0m 2022-08-04 15:57:27,071\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2946)\u001b[0m 2022-08-04 15:57:27,155\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=2946, ip=172.28.0.2, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fe3da5ac040>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2946)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2946)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2946)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2946)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2946)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2946)\u001b[0m     self[policy_id] = class_(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2946)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/eager_tf_policy.py\", line 405, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2946)\u001b[0m     self.model = make_model(self, observation_space, action_space, config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2946)\u001b[0m   File \"<ipython-input-9-952e9e988812>\", line 43, in build\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2946)\u001b[0m   File \"<ipython-input-12-f25df86380b2>\", line 7, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2946)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/models/tf/fcnet.py\", line 56, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2946)\u001b[0m     shape=(int(np.product(obs_space.shape)),), name=\"observations\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2946)\u001b[0m AttributeError: 'HierarchyPPOPolicy_eager' object has no attribute 'shape'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3102)\u001b[0m 2022-08-04 15:57:27,164\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3102, ip=172.28.0.2, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f1977b7b0d0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3102)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3102)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3102)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3102)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3102)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3102)\u001b[0m     self[policy_id] = class_(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3102)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/eager_tf_policy.py\", line 405, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3102)\u001b[0m     self.model = make_model(self, observation_space, action_space, config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3102)\u001b[0m   File \"<ipython-input-9-952e9e988812>\", line 43, in build\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3102)\u001b[0m   File \"<ipython-input-12-f25df86380b2>\", line 7, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3102)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/models/tf/fcnet.py\", line 56, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3102)\u001b[0m     shape=(int(np.product(obs_space.shape)),), name=\"observations\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3102)\u001b[0m AttributeError: 'HierarchyPPOPolicy_eager' object has no attribute 'shape'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3159)\u001b[0m 2022-08-04 15:57:27,160\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3199)\u001b[0m 2022-08-04 15:57:27,162\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3199, ip=172.28.0.2, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f8c2d9ae100>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3199)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3199)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3199)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3199)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3199)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3199)\u001b[0m     self[policy_id] = class_(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3199)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/eager_tf_policy.py\", line 405, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3199)\u001b[0m     self.model = make_model(self, observation_space, action_space, config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3199)\u001b[0m   File \"<ipython-input-9-952e9e988812>\", line 43, in build\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3199)\u001b[0m   File \"<ipython-input-12-f25df86380b2>\", line 7, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3199)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/models/tf/fcnet.py\", line 56, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3199)\u001b[0m     shape=(int(np.product(obs_space.shape)),), name=\"observations\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3199)\u001b[0m AttributeError: 'HierarchyPPOPolicy_eager' object has no attribute 'shape'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=3159)\u001b[0m 2022-08-04 15:57:27,250\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3159, ip=172.28.0.2, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f85f8b730d0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3159)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3159)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3159)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3159)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3159)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3159)\u001b[0m     self[policy_id] = class_(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3159)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/eager_tf_policy.py\", line 405, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3159)\u001b[0m     self.model = make_model(self, observation_space, action_space, config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3159)\u001b[0m   File \"<ipython-input-9-952e9e988812>\", line 43, in build\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3159)\u001b[0m   File \"<ipython-input-12-f25df86380b2>\", line 7, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3159)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/models/tf/fcnet.py\", line 56, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3159)\u001b[0m     shape=(int(np.product(obs_space.shape)),), name=\"observations\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3159)\u001b[0m AttributeError: 'HierarchyPPOPolicy_eager' object has no attribute 'shape'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3156)\u001b[0m 2022-08-04 15:57:27,304\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3156)\u001b[0m 2022-08-04 15:57:27,392\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3156, ip=172.28.0.2, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fdf275040d0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3156)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3156)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3156)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3156)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3156)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3156)\u001b[0m     self[policy_id] = class_(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3156)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/eager_tf_policy.py\", line 405, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3156)\u001b[0m     self.model = make_model(self, observation_space, action_space, config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3156)\u001b[0m   File \"<ipython-input-9-952e9e988812>\", line 43, in build\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3156)\u001b[0m   File \"<ipython-input-12-f25df86380b2>\", line 7, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3156)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/models/tf/fcnet.py\", line 56, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3156)\u001b[0m     shape=(int(np.product(obs_space.shape)),), name=\"observations\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3156)\u001b[0m AttributeError: 'HierarchyPPOPolicy_eager' object has no attribute 'shape'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2971)\u001b[0m 2022-08-04 15:57:27,419\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3019)\u001b[0m 2022-08-04 15:57:27,453\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2971)\u001b[0m 2022-08-04 15:57:27,519\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=2971, ip=172.28.0.2, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f783a411070>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2971)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2971)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2971)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2971)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2971)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2971)\u001b[0m     self[policy_id] = class_(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2971)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/eager_tf_policy.py\", line 405, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2971)\u001b[0m     self.model = make_model(self, observation_space, action_space, config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2971)\u001b[0m   File \"<ipython-input-9-952e9e988812>\", line 43, in build\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2971)\u001b[0m   File \"<ipython-input-12-f25df86380b2>\", line 7, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2971)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/models/tf/fcnet.py\", line 56, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2971)\u001b[0m     shape=(int(np.product(obs_space.shape)),), name=\"observations\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2971)\u001b[0m AttributeError: 'HierarchyPPOPolicy_eager' object has no attribute 'shape'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2949)\u001b[0m 2022-08-04 15:57:27,580\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3019)\u001b[0m 2022-08-04 15:57:27,541\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3019, ip=172.28.0.2, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fb1ebe62fa0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3019)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3019)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3019)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3019)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3019)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3019)\u001b[0m     self[policy_id] = class_(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3019)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/eager_tf_policy.py\", line 405, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3019)\u001b[0m     self.model = make_model(self, observation_space, action_space, config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3019)\u001b[0m   File \"<ipython-input-9-952e9e988812>\", line 43, in build\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3019)\u001b[0m   File \"<ipython-input-12-f25df86380b2>\", line 7, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3019)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/models/tf/fcnet.py\", line 56, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3019)\u001b[0m     shape=(int(np.product(obs_space.shape)),), name=\"observations\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3019)\u001b[0m AttributeError: 'HierarchyPPOPolicy_eager' object has no attribute 'shape'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2949)\u001b[0m 2022-08-04 15:57:27,681\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=2949, ip=172.28.0.2, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fc20c9ef0a0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2949)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2949)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2949)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2949)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2949)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2949)\u001b[0m     self[policy_id] = class_(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2949)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/eager_tf_policy.py\", line 405, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2949)\u001b[0m     self.model = make_model(self, observation_space, action_space, config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2949)\u001b[0m   File \"<ipython-input-9-952e9e988812>\", line 43, in build\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2949)\u001b[0m   File \"<ipython-input-12-f25df86380b2>\", line 7, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2949)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/models/tf/fcnet.py\", line 56, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2949)\u001b[0m     shape=(int(np.product(obs_space.shape)),), name=\"observations\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2949)\u001b[0m AttributeError: 'HierarchyPPOPolicy_eager' object has no attribute 'shape'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3048)\u001b[0m 2022-08-04 15:57:27,775\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3048)\u001b[0m 2022-08-04 15:57:27,879\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3048, ip=172.28.0.2, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f36ea93b0d0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3048)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3048)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3048)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3048)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3048)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3048)\u001b[0m     self[policy_id] = class_(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3048)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/eager_tf_policy.py\", line 405, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3048)\u001b[0m     self.model = make_model(self, observation_space, action_space, config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3048)\u001b[0m   File \"<ipython-input-9-952e9e988812>\", line 43, in build\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3048)\u001b[0m   File \"<ipython-input-12-f25df86380b2>\", line 7, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3048)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/models/tf/fcnet.py\", line 56, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3048)\u001b[0m     shape=(int(np.product(obs_space.shape)),), name=\"observations\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3048)\u001b[0m AttributeError: 'HierarchyPPOPolicy_eager' object has no attribute 'shape'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3075)\u001b[0m 2022-08-04 15:57:27,882\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3075)\u001b[0m 2022-08-04 15:57:27,976\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3075, ip=172.28.0.2, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f55c8d55fd0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3075)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3075)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3075)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3075)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3075)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3075)\u001b[0m     self[policy_id] = class_(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3075)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/eager_tf_policy.py\", line 405, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3075)\u001b[0m     self.model = make_model(self, observation_space, action_space, config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3075)\u001b[0m   File \"<ipython-input-9-952e9e988812>\", line 43, in build\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3075)\u001b[0m   File \"<ipython-input-12-f25df86380b2>\", line 7, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3075)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/models/tf/fcnet.py\", line 56, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3075)\u001b[0m     shape=(int(np.product(obs_space.shape)),), name=\"observations\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3075)\u001b[0m AttributeError: 'HierarchyPPOPolicy_eager' object has no attribute 'shape'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2987)\u001b[0m 2022-08-04 15:57:27,964\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2987)\u001b[0m 2022-08-04 15:57:28,095\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=2987, ip=172.28.0.2, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7cdf217f70>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2987)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2987)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2987)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2987)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2987)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2987)\u001b[0m     self[policy_id] = class_(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2987)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/eager_tf_policy.py\", line 405, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2987)\u001b[0m     self.model = make_model(self, observation_space, action_space, config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2987)\u001b[0m   File \"<ipython-input-9-952e9e988812>\", line 43, in build\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2987)\u001b[0m   File \"<ipython-input-12-f25df86380b2>\", line 7, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2987)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/models/tf/fcnet.py\", line 56, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2987)\u001b[0m     shape=(int(np.product(obs_space.shape)),), name=\"observations\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2987)\u001b[0m AttributeError: 'HierarchyPPOPolicy_eager' object has no attribute 'shape'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3001)\u001b[0m 2022-08-04 15:57:28,332\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3001)\u001b[0m 2022-08-04 15:57:28,441\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3001, ip=172.28.0.2, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7ef38744f0a0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3001)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3001)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3001)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3001)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3001)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/policy_map.py\", line 146, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3001)\u001b[0m     self[policy_id] = class_(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3001)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/eager_tf_policy.py\", line 405, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3001)\u001b[0m     self.model = make_model(self, observation_space, action_space, config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3001)\u001b[0m   File \"<ipython-input-9-952e9e988812>\", line 43, in build\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3001)\u001b[0m   File \"<ipython-input-12-f25df86380b2>\", line 7, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3001)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/models/tf/fcnet.py\", line 56, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3001)\u001b[0m     shape=(int(np.product(obs_space.shape)),), name=\"observations\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3001)\u001b[0m AttributeError: 'HierarchyPPOPolicy_eager' object has no attribute 'shape'\n"
     ]
    }
   ],
   "source": [
    "from CybORG import CybORG\n",
    "from CybORG.Agents import B_lineAgent, SleepAgent, GreenAgent\n",
    "from CybORG.Agents.SimpleAgents.BaseAgent import BaseAgent\n",
    "from CybORG.Agents.SimpleAgents.BlueReactAgent import BlueReactRemoveAgent\n",
    "from CybORG.Agents.SimpleAgents.Meander import RedMeanderAgent\n",
    "from CybORG.Agents.Wrappers.EnumActionWrapper import EnumActionWrapper\n",
    "from CybORG.Agents.Wrappers.FixedFlatWrapper import FixedFlatWrapper\n",
    "from CybORG.Agents.Wrappers.OpenAIGymWrapper import OpenAIGymWrapper\n",
    "from CybORG.Agents.Wrappers.ReduceActionSpaceWrapper import ReduceActionSpaceWrapper\n",
    "from CybORG.Agents.Wrappers import ChallengeWrapper\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.agents import ppo\n",
    "import inspect\n",
    "from CybORG.Agents.Wrappers.rllib_wrapper import RLlibWrapper\n",
    "\n",
    "def env_creator(env_config: dict):\n",
    "    path = str(inspect.getfile(CybORG))\n",
    "    path = path[:-10] + '/Shared/Scenarios/Scenario2.yaml'\n",
    "    agents = {\"Red\": B_lineAgent, \"Green\": GreenAgent}\n",
    "    cyborg = CybORG(scenario_file=path, environment='sim', agents=agents)\n",
    "    env = RLlibWrapper(env=cyborg, agent_name=\"Blue\", max_steps=100)\n",
    "    return env\n",
    "\n",
    "def print_results(results_dict):\n",
    "    train_iter = results_dict[\"training_iteration\"]\n",
    "    r_mean = results_dict[\"episode_reward_mean\"]\n",
    "    r_max = results_dict[\"episode_reward_max\"]\n",
    "    r_min = results_dict[\"episode_reward_min\"]\n",
    "    print(f\"{train_iter:4d} \\tr_mean: {r_mean:.1f} \\tr_max: {r_max:.1f} \\tr_min: {r_min: .1f}\")\n",
    "\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "\n",
    "batch_size = 4000\n",
    "# Set up CybORG\n",
    "register_env(name=\"CybORG\", env_creator=env_creator)\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "\n",
    "ModelCatalog.register_custom_model(\"h_model\", HieracrchyModel)\n",
    "\n",
    "from shutil import make_archive\n",
    "allrewards = []\n",
    "\n",
    "config.update({\"num_gpus\": 1,\"num_workers\": 0,\n",
    "                # Also, use \"framework: tf2\" for tfe eager execution.\n",
    "                \"framework\": \"tf2\",\n",
    "                \"train_batch_size\": batch_size,\n",
    "                \"horizon\": 100,\n",
    "                \"gamma\": 0.9,\n",
    "                \"model\": {\n",
    "                    \"custom_model\": \"h_model\",\n",
    "                    \"fcnet_hiddens\": [512, 512],\n",
    "                    \"fcnet_activation\": \"relu\",\n",
    "                },\n",
    "\n",
    "                }) \n",
    "trainer = HieracrchyTrainer(config=config, env=\"CybORG\")\n",
    "\n",
    "reward = []\n",
    "novel_obs = []\n",
    "novel_actions = []\n",
    "for i in range(200):\n",
    "    results_dict = trainer.train()\n",
    "    print_results(results_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
