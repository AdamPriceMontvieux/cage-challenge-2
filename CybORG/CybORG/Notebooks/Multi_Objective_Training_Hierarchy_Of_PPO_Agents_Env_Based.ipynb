{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Box\n",
    "import numpy as np\n",
    "\n",
    "from ray.rllib.models.modelv2 import ModelV2\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models.tf.fcnet import FullyConnectedNetwork\n",
    "from ray.rllib.models.torch.misc import SlimFC\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "from ray.rllib.agents.ppo.ppo import PPOTrainer\n",
    "from ray.rllib.agents.ppo import ppo\n",
    "from ray.rllib.models.tf.tf_action_dist import Categorical\n",
    "from ray.rllib.models.tf.misc import normc_initializer\n",
    "from ray.rllib.models.tf.recurrent_net import RecurrentNetwork\n",
    "import gym\n",
    "\n",
    "tf1, tf, tfv = try_import_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import time\n",
    "from statistics import mean, stdev\n",
    "from CybORG import CybORG\n",
    "from CybORG.Agents import B_lineAgent, SleepAgent, GreenAgent\n",
    "from CybORG.Agents.SimpleAgents.BaseAgent import BaseAgent\n",
    "from CybORG.Agents.SimpleAgents.BlueReactAgent import BlueReactRemoveAgent\n",
    "from CybORG.Agents.SimpleAgents.Meander import RedMeanderAgent\n",
    "from CybORG.Agents.Wrappers.EnumActionWrapper import EnumActionWrapper\n",
    "from CybORG.Agents.Wrappers.FixedFlatWrapper import FixedFlatWrapper\n",
    "from CybORG.Agents.Wrappers.OpenAIGymWrapper import OpenAIGymWrapper\n",
    "from CybORG.Agents.Wrappers.ReduceActionSpaceWrapper import ReduceActionSpaceWrapper\n",
    "from CybORG.Agents.Wrappers import ChallengeWrapper\n",
    "import os\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.tune.registry import register_env\n",
    "from CybORG.Agents.Wrappers.rllib_wrapper import RLlibWrapper\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPS = 50\n",
    "agent_name = 'Blue'\n",
    "\n",
    "def wrap(env):\n",
    "    return HierarchyWrapperEval(agent_name=\"Blue\", env=env)\n",
    "\n",
    "def evaluate(steps):\n",
    "    path = str(inspect.getfile(CybORG))\n",
    "    path = path[:-10] + '/Shared/Scenarios/Scenario2.yaml'\n",
    "\n",
    "    #print(f'using CybORG v{cyborg_version}, {scenario}\\n')\n",
    "    for num_steps in steps:\n",
    "        #for red_agent in [B_lineAgent, RedMeanderAgent, SleepAgent]:\n",
    "        rs = []\n",
    "        for red_agent in [RedMeanderAgent, B_lineAgent]:\n",
    "\n",
    "            cyborg = CybORG(path, 'sim', agents={'Red': red_agent})\n",
    "            wrapped_cyborg = wrap(cyborg)\n",
    "\n",
    "            observation = wrapped_cyborg.reset()\n",
    "\n",
    "            action_space = wrapped_cyborg.get_action_space(agent_name)\n",
    "            \n",
    "            cell_size=256\n",
    "            state=[np.zeros(cell_size, np.float32),\n",
    "                   np.zeros(cell_size, np.float32)]\n",
    "\n",
    "            total_reward = []\n",
    "            actions = []\n",
    "            for i in range(MAX_EPS):\n",
    "                r = []\n",
    "                a = []\n",
    "                # cyborg.env.env.tracker.render()\n",
    "                for j in range(num_steps):\n",
    "                    action, state, logits = agent.compute_action(observation, state, explore=False)\n",
    "                    #action = agent.get_action(observation, action_space)\n",
    "                    observation, rew, done, info = wrapped_cyborg.step(action)\n",
    "                    # result = cyborg.step(agent_name, action)\n",
    "                    r.append(rew)\n",
    "                    # r.append(result.reward)\n",
    "                    a.append((str(cyborg.get_last_action('Blue')), str(cyborg.get_last_action('Red'))))\n",
    "                total_reward.append(sum(r))\n",
    "                actions.append(a)\n",
    "                # observation = cyborg.reset().observation\n",
    "                observation = wrapped_cyborg.reset()\n",
    "            rs.append(mean(total_reward))\n",
    "            print(f'Average reward for red agent {red_agent.__name__} at steps {num_steps} is: {mean(total_reward):.1f} with a standard deviation of {stdev(total_reward):.1f}')\n",
    "    return rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CybORG.Agents.Wrappers.rllib_wrapper import RLlibWrapper\n",
    "\n",
    "class HierarchyWrapper(RLlibWrapper):\n",
    "    def __init__(self, agent_name, env, agent=None, reward_threshold=None, max_steps=None, env_id=None):\n",
    "        super().__init__(agent_name, env, agent, reward_threshold, max_steps)\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        ppo_config = ppo.DEFAULT_CONFIG.copy()\n",
    "        ppo_config.update({\"num_gpus\": 0,\"num_workers\": 0,\n",
    "            \"framework\": \"tf2\",\n",
    "            \"model\": {\n",
    "                \"fcnet_hiddens\": [512, 512],\n",
    "                \"fcnet_activation\": \"relu\",\n",
    "            }})\n",
    "        \n",
    "        b_line = PPOTrainer(config=ppo_config,env=\"CybORG\")\n",
    "        b_line.restore(\"supervisor_ppo/checkpoint_000394/checkpoint-394\")\n",
    "        meander = PPOTrainer(config=ppo_config,env=\"CybORG\")\n",
    "        meander.restore(\"supervisor_ppo/checkpoint_000487/checkpoint-487\")\n",
    "        self.sub_agents = [b_line.get_policy().model, meander.get_policy().model]\n",
    "        self.prev_obs = self.reset()\n",
    "        self.env_id = env_id\n",
    "        \n",
    "    def step(self, action=None):\n",
    "        \n",
    "        reward = -1\n",
    "        if self.env_id == 0:\n",
    "            if action == 0:\n",
    "                reward = 0\n",
    "        else:\n",
    "            if action == 1:\n",
    "                reward = 0\n",
    "                \n",
    "        logits = self.sub_agents[action].forward({'obs_flat': np.array([self.prev_obs])}, None, None)[0]\n",
    "        action = tf.math.argmax(logits, axis=1)[0]\n",
    "        self.prev_obs, r, done, info = self.env.step(action=action)\n",
    "        self.step_counter += 1\n",
    "        if self.max_steps is not None and self.step_counter >= self.max_steps:\n",
    "            done = True\n",
    "            self.reset()\n",
    "            \n",
    "        return np.float32(self.prev_obs), reward, done, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CybORG.Agents.Wrappers.rllib_wrapper import RLlibWrapper\n",
    "\n",
    "class HierarchyWrapperEval(HierarchyWrapper):\n",
    "    def __init__(self, agent_name, env, agent=None, reward_threshold=None, max_steps=None, env_id=None):\n",
    "        super().__init__(agent_name, env, agent, reward_threshold, max_steps)\n",
    "\n",
    "    def step(self, action=None):\n",
    "\n",
    "        logits = self.sub_agents[action].forward({'obs_flat': np.array([self.prev_obs])}, None, None)[0]\n",
    "        action = tf.math.argmax(logits, axis=1)[0]\n",
    "        self.prev_obs, reward, done, info = self.env.step(action=action)\n",
    "        self.step_counter += 1\n",
    "        if self.max_steps is not None and self.step_counter >= self.max_steps:\n",
    "            done = True\n",
    "            self.reset()\n",
    "            \n",
    "        return np.float32(self.prev_obs), reward, done, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.tune.registry import register_env\n",
    "import time \n",
    "\n",
    "def print_results(results_dict):\n",
    "    train_iter = results_dict[\"training_iteration\"]\n",
    "    r_mean = results_dict[\"episode_reward_mean\"]\n",
    "    r_max = results_dict[\"episode_reward_max\"]\n",
    "    r_min = results_dict[\"episode_reward_min\"]\n",
    "    eval_dic = results_dict['evaluation']\n",
    "    e_mean = eval_dic[\"episode_reward_mean\"]\n",
    "    e_max = eval_dic[\"episode_reward_max\"]\n",
    "    e_min = eval_dic[\"episode_reward_min\"]\n",
    "    print(f\"{train_iter:4d} \\tr_mean: {r_mean:.1f} \\tr_max: {r_max:.1f} \\tr_min: {r_min: .1f} \\te_mean: {e_mean:.1f} \\te_max: {e_max:.1f} \\te_min: {e_min: .1f}\")\n",
    "    \n",
    "\n",
    "class MultiEnv(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        # pick actual env based on worker and env indexes\n",
    "        self.env = self.choose_env_for(env_config.worker_index)\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.observation_space = self.env.observation_space\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)   \n",
    "    def choose_env_for(self, index):\n",
    "        if index > 40:\n",
    "            path = str(inspect.getfile(CybORG))\n",
    "            path = path[:-10] + '/Shared/Scenarios/Scenario2.yaml'\n",
    "            agents = {\"Red\": SleepAgent, \"Green\": GreenAgent}\n",
    "            cyborg = CybORG(scenario_file=path, environment='sim', agents=agents)\n",
    "            return HierarchyWrapper(env=cyborg, agent_name=\"Blue\", max_steps=100, env_id=2)\n",
    "        elif index % 2 == 0:\n",
    "            path = str(inspect.getfile(CybORG))\n",
    "            path = path[:-10] + '/Shared/Scenarios/Scenario2.yaml'\n",
    "            agents = {\"Red\": B_lineAgent, \"Green\": GreenAgent}\n",
    "            cyborg = CybORG(scenario_file=path, environment='sim', agents=agents)\n",
    "            return HierarchyWrapper(env=cyborg, agent_name=\"Blue\", max_steps=100, env_id=0)\n",
    "        else:\n",
    "            path = str(inspect.getfile(CybORG))\n",
    "            path = path[:-10] + '/Shared/Scenarios/Scenario2.yaml'\n",
    "            agents = {\"Red\": RedMeanderAgent, \"Green\": GreenAgent}\n",
    "            cyborg = CybORG(scenario_file=path, environment='sim', agents=agents)\n",
    "            return HierarchyWrapper(env=cyborg, agent_name=\"Blue\", max_steps=100, env_id=1)\n",
    "        \n",
    "def env_creator(env_config: dict):\n",
    "    path = str(inspect.getfile(CybORG))\n",
    "    path = path[:-10] + '/Shared/Scenarios/Scenario2.yaml'\n",
    "    agents = {\"Red\": B_lineAgent, \"Green\": GreenAgent}\n",
    "    cyborg = CybORG(scenario_file=path, environment='sim', agents=agents)\n",
    "    env = RLlibWrapper(env=cyborg, agent_name=\"Blue\", max_steps=100)\n",
    "    return env\n",
    "\n",
    "register_env(\"CybORG\", env_creator=env_creator)\n",
    "register_env(\"multienv\", lambda config: MultiEnv(config))\n",
    "\n",
    "batch_size = 2000\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "\n",
    "config.update({\"num_gpus\": 1, \"num_workers\": 20,\n",
    "               'num_cpus_per_worker':1,\n",
    "               \"env\": \"multienv\",\n",
    "                # Also, use \"framework: tf2\" for tfe eager execution.\n",
    "                \"framework\": \"tf2\",\n",
    "                \"train_batch_size\": batch_size,\n",
    "                \"horizon\": 100,\n",
    "                \"sgd_minibatch_size\": 100,\n",
    "                \"gamma\": 0.9,\n",
    "\n",
    "                \"model\": {\n",
    "                    \"use_lstm\": True,\n",
    "                   \"max_seq_len\": 3,\n",
    "                    \"lstm_cell_size\": 256,\n",
    "                    \"fcnet_hiddens\": [256],\n",
    "                   \"fcnet_activation\": \"relu\",\n",
    "                },\n",
    "               \n",
    "                 'evaluation_interval': 1,\n",
    "                 'evaluation_duration': 100,\n",
    "                 'evaluation_duration_unit': 'episodes',\n",
    "                 'evaluation_parallel_to_training': True,\n",
    "                 'evaluation_num_workers': 5,\n",
    "                }) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 15:48:55,502\tWARNING ppo.py:386 -- `train_batch_size` (2000) cannot be achieved with your other settings (num_workers=20 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15431)\u001b[0m 2022-08-15 15:49:07,689\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15441)\u001b[0m 2022-08-15 15:49:07,744\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15458)\u001b[0m 2022-08-15 15:49:08,151\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15452)\u001b[0m 2022-08-15 15:49:08,383\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15429)\u001b[0m 2022-08-15 15:49:08,842\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15439)\u001b[0m 2022-08-15 15:49:09,078\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15435)\u001b[0m 2022-08-15 15:49:09,251\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15445)\u001b[0m 2022-08-15 15:49:09,205\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15443)\u001b[0m 2022-08-15 15:49:09,177\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15433)\u001b[0m 2022-08-15 15:49:09,271\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15428)\u001b[0m 2022-08-15 15:49:09,290\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15456)\u001b[0m 2022-08-15 15:49:09,297\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15431)\u001b[0m 2022-08-15 15:49:09,374\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15431)\u001b[0m 2022-08-15 15:49:09,415\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15431)\u001b[0m 2022-08-15 15:49:09,416\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15448)\u001b[0m 2022-08-15 15:49:09,485\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15449)\u001b[0m 2022-08-15 15:49:09,385\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15477)\u001b[0m 2022-08-15 15:49:09,475\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15453)\u001b[0m 2022-08-15 15:49:09,534\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15437)\u001b[0m 2022-08-15 15:49:09,637\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15460)\u001b[0m 2022-08-15 15:49:09,604\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15431)\u001b[0m 2022-08-15 15:49:09,853\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15431)\u001b[0m 2022-08-15 15:49:10,024\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15487)\u001b[0m 2022-08-15 15:49:09,939\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15522)\u001b[0m 2022-08-15 15:49:09,996\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15431)\u001b[0m 2022-08-15 15:49:10,060\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15431)\u001b[0m 2022-08-15 15:49:10,060\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=15431)\u001b[0m 2022-08-15 15:49:10,349\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15441)\u001b[0m 2022-08-15 15:49:10,465\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15458)\u001b[0m 2022-08-15 15:49:10,477\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15441)\u001b[0m 2022-08-15 15:49:10,514\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15441)\u001b[0m 2022-08-15 15:49:10,514\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15452)\u001b[0m 2022-08-15 15:49:10,498\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15452)\u001b[0m 2022-08-15 15:49:10,566\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15452)\u001b[0m 2022-08-15 15:49:10,566\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15458)\u001b[0m 2022-08-15 15:49:10,524\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15458)\u001b[0m 2022-08-15 15:49:10,524\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15429)\u001b[0m 2022-08-15 15:49:10,529\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15429)\u001b[0m 2022-08-15 15:49:10,576\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15429)\u001b[0m 2022-08-15 15:49:10,576\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15433)\u001b[0m 2022-08-15 15:49:10,581\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15439)\u001b[0m 2022-08-15 15:49:10,535\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15439)\u001b[0m 2022-08-15 15:49:10,602\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15439)\u001b[0m 2022-08-15 15:49:10,602\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15428)\u001b[0m 2022-08-15 15:49:10,523\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15428)\u001b[0m 2022-08-15 15:49:10,579\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15428)\u001b[0m 2022-08-15 15:49:10,579\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15435)\u001b[0m 2022-08-15 15:49:10,588\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15445)\u001b[0m 2022-08-15 15:49:10,526\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15445)\u001b[0m 2022-08-15 15:49:10,574\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15445)\u001b[0m 2022-08-15 15:49:10,574\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15443)\u001b[0m 2022-08-15 15:49:10,526\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15443)\u001b[0m 2022-08-15 15:49:10,587\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15443)\u001b[0m 2022-08-15 15:49:10,587\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15456)\u001b[0m 2022-08-15 15:49:10,548\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15456)\u001b[0m 2022-08-15 15:49:10,605\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15456)\u001b[0m 2022-08-15 15:49:10,606\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15453)\u001b[0m 2022-08-15 15:49:10,531\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15453)\u001b[0m 2022-08-15 15:49:10,581\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15453)\u001b[0m 2022-08-15 15:49:10,582\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15449)\u001b[0m 2022-08-15 15:49:10,536\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15449)\u001b[0m 2022-08-15 15:49:10,584\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15449)\u001b[0m 2022-08-15 15:49:10,584\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15477)\u001b[0m 2022-08-15 15:49:10,557\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15477)\u001b[0m 2022-08-15 15:49:10,611\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15487)\u001b[0m 2022-08-15 15:49:10,584\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15522)\u001b[0m 2022-08-15 15:49:10,543\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15522)\u001b[0m 2022-08-15 15:49:10,605\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15522)\u001b[0m 2022-08-15 15:49:10,606\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15448)\u001b[0m 2022-08-15 15:49:10,610\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15477)\u001b[0m 2022-08-15 15:49:10,611\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15433)\u001b[0m 2022-08-15 15:49:10,644\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15433)\u001b[0m 2022-08-15 15:49:10,644\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15435)\u001b[0m 2022-08-15 15:49:10,640\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15435)\u001b[0m 2022-08-15 15:49:10,641\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15448)\u001b[0m 2022-08-15 15:49:10,670\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15448)\u001b[0m 2022-08-15 15:49:10,670\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15487)\u001b[0m 2022-08-15 15:49:10,631\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15487)\u001b[0m 2022-08-15 15:49:10,631\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=15437)\u001b[0m 2022-08-15 15:49:10,860\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15437)\u001b[0m 2022-08-15 15:49:10,920\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15437)\u001b[0m 2022-08-15 15:49:10,920\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15460)\u001b[0m 2022-08-15 15:49:10,889\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15460)\u001b[0m 2022-08-15 15:49:10,950\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15460)\u001b[0m 2022-08-15 15:49:10,950\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15441)\u001b[0m 2022-08-15 15:49:10,978\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15458)\u001b[0m 2022-08-15 15:49:10,977\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15443)\u001b[0m 2022-08-15 15:49:11,062\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15487)\u001b[0m 2022-08-15 15:49:11,064\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15522)\u001b[0m 2022-08-15 15:49:11,061\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15441)\u001b[0m 2022-08-15 15:49:11,153\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15452)\u001b[0m 2022-08-15 15:49:11,143\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15458)\u001b[0m 2022-08-15 15:49:11,151\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15429)\u001b[0m 2022-08-15 15:49:11,122\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15439)\u001b[0m 2022-08-15 15:49:11,114\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15428)\u001b[0m 2022-08-15 15:49:11,143\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15435)\u001b[0m 2022-08-15 15:49:11,077\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15445)\u001b[0m 2022-08-15 15:49:11,142\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15456)\u001b[0m 2022-08-15 15:49:11,154\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15453)\u001b[0m 2022-08-15 15:49:11,114\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15449)\u001b[0m 2022-08-15 15:49:11,102\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15477)\u001b[0m 2022-08-15 15:49:11,180\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15441)\u001b[0m 2022-08-15 15:49:11,194\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15441)\u001b[0m 2022-08-15 15:49:11,194\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15458)\u001b[0m 2022-08-15 15:49:11,190\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15458)\u001b[0m 2022-08-15 15:49:11,190\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15433)\u001b[0m 2022-08-15 15:49:11,211\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15435)\u001b[0m 2022-08-15 15:49:11,263\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15435)\u001b[0m 2022-08-15 15:49:11,302\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15435)\u001b[0m 2022-08-15 15:49:11,303\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15443)\u001b[0m 2022-08-15 15:49:11,255\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15443)\u001b[0m 2022-08-15 15:49:11,293\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15443)\u001b[0m 2022-08-15 15:49:11,294\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15448)\u001b[0m 2022-08-15 15:49:11,245\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15487)\u001b[0m 2022-08-15 15:49:11,253\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15487)\u001b[0m 2022-08-15 15:49:11,291\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15487)\u001b[0m 2022-08-15 15:49:11,291\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15522)\u001b[0m 2022-08-15 15:49:11,257\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15522)\u001b[0m 2022-08-15 15:49:11,302\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15522)\u001b[0m 2022-08-15 15:49:11,303\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=15452)\u001b[0m 2022-08-15 15:49:11,398\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15429)\u001b[0m 2022-08-15 15:49:11,357\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15429)\u001b[0m 2022-08-15 15:49:11,415\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15429)\u001b[0m 2022-08-15 15:49:11,416\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15439)\u001b[0m 2022-08-15 15:49:11,351\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15439)\u001b[0m 2022-08-15 15:49:11,416\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15439)\u001b[0m 2022-08-15 15:49:11,416\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15428)\u001b[0m 2022-08-15 15:49:11,390\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15445)\u001b[0m 2022-08-15 15:49:11,391\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15456)\u001b[0m 2022-08-15 15:49:11,386\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15453)\u001b[0m 2022-08-15 15:49:11,330\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15453)\u001b[0m 2022-08-15 15:49:11,380\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15453)\u001b[0m 2022-08-15 15:49:11,380\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15449)\u001b[0m 2022-08-15 15:49:11,347\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15449)\u001b[0m 2022-08-15 15:49:11,401\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15449)\u001b[0m 2022-08-15 15:49:11,401\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15477)\u001b[0m 2022-08-15 15:49:11,425\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15437)\u001b[0m 2022-08-15 15:49:11,429\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15441)\u001b[0m 2022-08-15 15:49:11,443\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15428)\u001b[0m 2022-08-15 15:49:11,443\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15428)\u001b[0m 2022-08-15 15:49:11,443\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15445)\u001b[0m 2022-08-15 15:49:11,445\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15445)\u001b[0m 2022-08-15 15:49:11,445\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15456)\u001b[0m 2022-08-15 15:49:11,444\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15456)\u001b[0m 2022-08-15 15:49:11,444\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15452)\u001b[0m 2022-08-15 15:49:11,447\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15452)\u001b[0m 2022-08-15 15:49:11,448\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15458)\u001b[0m 2022-08-15 15:49:11,457\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15433)\u001b[0m 2022-08-15 15:49:11,460\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15433)\u001b[0m 2022-08-15 15:49:11,509\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15433)\u001b[0m 2022-08-15 15:49:11,509\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15435)\u001b[0m 2022-08-15 15:49:11,575\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15448)\u001b[0m 2022-08-15 15:49:11,491\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15448)\u001b[0m 2022-08-15 15:49:11,535\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15448)\u001b[0m 2022-08-15 15:49:11,536\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15460)\u001b[0m 2022-08-15 15:49:11,509\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15477)\u001b[0m 2022-08-15 15:49:11,474\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15477)\u001b[0m 2022-08-15 15:49:11,474\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15443)\u001b[0m 2022-08-15 15:49:11,625\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15437)\u001b[0m 2022-08-15 15:49:11,695\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15487)\u001b[0m 2022-08-15 15:49:11,670\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15522)\u001b[0m 2022-08-15 15:49:11,688\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15429)\u001b[0m 2022-08-15 15:49:11,732\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15439)\u001b[0m 2022-08-15 15:49:11,710\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15428)\u001b[0m 2022-08-15 15:49:11,714\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15445)\u001b[0m 2022-08-15 15:49:11,763\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15456)\u001b[0m 2022-08-15 15:49:11,777\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15437)\u001b[0m 2022-08-15 15:49:11,744\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15437)\u001b[0m 2022-08-15 15:49:11,744\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15453)\u001b[0m 2022-08-15 15:49:11,721\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15449)\u001b[0m 2022-08-15 15:49:11,725\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15460)\u001b[0m 2022-08-15 15:49:11,773\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15460)\u001b[0m 2022-08-15 15:49:11,810\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15460)\u001b[0m 2022-08-15 15:49:11,810\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15477)\u001b[0m 2022-08-15 15:49:11,800\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15452)\u001b[0m 2022-08-15 15:49:11,807\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=15433)\u001b[0m 2022-08-15 15:49:11,827\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15448)\u001b[0m 2022-08-15 15:49:11,847\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15437)\u001b[0m 2022-08-15 15:49:12,057\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15460)\u001b[0m 2022-08-15 15:49:12,099\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-08-15 15:49:18,904\tWARNING deprecation.py:46 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "2022-08-15 15:49:19,178\tINFO trainable.py:159 -- Trainable.setup took 23.679 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-08-15 15:49:19,180\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "2022-08-15 15:49:23,600\tWARNING deprecation.py:46 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n",
      "2022-08-15 15:49:23,605\tWARNING deprecation.py:46 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18099)\u001b[0m 2022-08-15 15:49:26,367\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18099)\u001b[0m 2022-08-15 15:49:26,633\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18104)\u001b[0m 2022-08-15 15:49:26,637\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18099)\u001b[0m 2022-08-15 15:49:26,673\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18099)\u001b[0m 2022-08-15 15:49:26,674\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18100)\u001b[0m 2022-08-15 15:49:26,695\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18104)\u001b[0m 2022-08-15 15:49:26,884\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18100)\u001b[0m 2022-08-15 15:49:26,945\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18100)\u001b[0m 2022-08-15 15:49:26,984\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18100)\u001b[0m 2022-08-15 15:49:26,985\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18104)\u001b[0m 2022-08-15 15:49:26,923\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18104)\u001b[0m 2022-08-15 15:49:26,923\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18099)\u001b[0m 2022-08-15 15:49:27,071\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18107)\u001b[0m 2022-08-15 15:49:27,087\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18099)\u001b[0m 2022-08-15 15:49:27,238\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18099)\u001b[0m 2022-08-15 15:49:27,271\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18099)\u001b[0m 2022-08-15 15:49:27,271\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18104)\u001b[0m 2022-08-15 15:49:27,319\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18102)\u001b[0m 2022-08-15 15:49:27,326\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18100)\u001b[0m 2022-08-15 15:49:27,377\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18107)\u001b[0m 2022-08-15 15:49:27,351\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18107)\u001b[0m 2022-08-15 15:49:27,392\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18107)\u001b[0m 2022-08-15 15:49:27,393\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18099)\u001b[0m 2022-08-15 15:49:27,538\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18104)\u001b[0m 2022-08-15 15:49:27,490\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18104)\u001b[0m 2022-08-15 15:49:27,525\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18104)\u001b[0m 2022-08-15 15:49:27,525\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18100)\u001b[0m 2022-08-15 15:49:27,552\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18100)\u001b[0m 2022-08-15 15:49:27,587\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18100)\u001b[0m 2022-08-15 15:49:27,588\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18102)\u001b[0m 2022-08-15 15:49:27,588\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18102)\u001b[0m 2022-08-15 15:49:27,629\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18102)\u001b[0m 2022-08-15 15:49:27,629\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=18100)\u001b[0m 2022-08-15 15:49:27,811\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18104)\u001b[0m 2022-08-15 15:49:27,796\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18107)\u001b[0m 2022-08-15 15:49:27,784\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18107)\u001b[0m 2022-08-15 15:49:27,947\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18107)\u001b[0m 2022-08-15 15:49:27,981\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18107)\u001b[0m 2022-08-15 15:49:27,981\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18102)\u001b[0m 2022-08-15 15:49:28,016\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18107)\u001b[0m 2022-08-15 15:49:28,182\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18102)\u001b[0m 2022-08-15 15:49:28,185\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18102)\u001b[0m 2022-08-15 15:49:28,220\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18102)\u001b[0m 2022-08-15 15:49:28,220\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18102)\u001b[0m 2022-08-15 15:49:28,471\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1 \tr_mean: -50.4 \tr_max: -43.0 \tr_min: -60.0 \te_mean: -50.6 \te_max: -40.0 \te_min: -63.0\n",
      "   2 \tr_mean: -46.2 \tr_max: -33.0 \tr_min: -60.0 \te_mean: -42.1 \te_max: -30.0 \te_min: -63.0\n",
      "   3 \tr_mean: -39.2 \tr_max: -6.0 \tr_min: -65.0 \te_mean: -33.0 \te_max: -7.0 \te_min: -74.0\n",
      "   4 \tr_mean: -36.3 \tr_max: -5.0 \tr_min: -79.0 \te_mean: -25.1 \te_max: -6.0 \te_min: -85.0\n",
      "   5 \tr_mean: -32.5 \tr_max: -2.0 \tr_min: -79.0 \te_mean: -18.2 \te_max: -2.0 \te_min: -82.0\n",
      "   6 \tr_mean: -25.1 \tr_max: -2.0 \tr_min: -79.0 \te_mean: -18.8 \te_max: -2.0 \te_min: -76.0\n",
      "   7 \tr_mean: -24.0 \tr_max: -2.0 \tr_min: -100.0 \te_mean: -25.7 \te_max: -2.0 \te_min: -98.0\n",
      "   8 \tr_mean: -28.4 \tr_max: -1.0 \tr_min: -100.0 \te_mean: -37.8 \te_max: 0.0 \te_min: -100.0\n",
      "   9 \tr_mean: -32.8 \tr_max: 0.0 \tr_min: -100.0 \te_mean: -40.3 \te_max: 0.0 \te_min: -100.0\n",
      "  10 \tr_mean: -39.3 \tr_max: 0.0 \tr_min: -100.0 \te_mean: -40.0 \te_max: 0.0 \te_min: -100.0\n",
      "  11 \tr_mean: -46.5 \tr_max: 0.0 \tr_min: -100.0 \te_mean: -40.2 \te_max: 0.0 \te_min: -100.0\n",
      "  12 \tr_mean: -49.2 \tr_max: 0.0 \tr_min: -100.0 \te_mean: -40.2 \te_max: 0.0 \te_min: -100.0\n",
      "  13 \tr_mean: -49.9 \tr_max: 0.0 \tr_min: -100.0 \te_mean: -40.2 \te_max: 0.0 \te_min: -100.0\n",
      "  14 \tr_mean: -47.0 \tr_max: 0.0 \tr_min: -100.0 \te_mean: -29.7 \te_max: -4.0 \te_min: -96.0\n",
      "  15 \tr_mean: -38.3 \tr_max: 0.0 \tr_min: -100.0 \te_mean: -17.9 \te_max: -1.0 \te_min: -96.0\n",
      "  16 \tr_mean: -32.1 \tr_max: 0.0 \tr_min: -100.0 \te_mean: -22.4 \te_max: 0.0 \te_min: -100.0\n",
      "  17 \tr_mean: -28.0 \tr_max: 0.0 \tr_min: -100.0 \te_mean: -16.8 \te_max: -1.0 \te_min: -99.0\n",
      "  18 \tr_mean: -21.6 \tr_max: 0.0 \tr_min: -100.0 \te_mean: -15.2 \te_max: -1.0 \te_min: -99.0\n",
      "  19 \tr_mean: -18.6 \tr_max: 0.0 \tr_min: -100.0 \te_mean: -18.0 \te_max: 0.0 \te_min: -100.0\n",
      "  20 \tr_mean: -21.9 \tr_max: 0.0 \tr_min: -100.0 \te_mean: -17.8 \te_max: 0.0 \te_min: -100.0\n",
      "  21 \tr_mean: -24.7 \tr_max: 0.0 \tr_min: -100.0 \te_mean: -28.3 \te_max: 0.0 \te_min: -100.0\n",
      "  22 \tr_mean: -23.2 \tr_max: 0.0 \tr_min: -100.0 \te_mean: -16.2 \te_max: 0.0 \te_min: -100.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/agents/trainer.py:1243\u001b[0m, in \u001b[0;36mTrainer.step_attempt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m     step_results\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1244\u001b[0m \u001b[38;5;66;03m# Collect the training results from the future.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/agents/trainer.py:1394\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, episodes_left_fn, duration_fn)\u001b[0m\n\u001b[1;32m   1393\u001b[0m round_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1394\u001b[0m batches \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1395\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m   1396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1397\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1398\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluation_workers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1399\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1400\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepisodes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrollout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1401\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43munits_left_to_do\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[38;5;66;03m# 1 episode per returned batch.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/worker.py:1825\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   1824\u001b[0m \u001b[38;5;66;03m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[39;00m\n\u001b[0;32m-> 1825\u001b[0m values, debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1826\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(values):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/worker.py:364\u001b[0m, in \u001b[0;36mWorker.get_objects\u001b[0;34m(self, object_refs, timeout)\u001b[0m\n\u001b[1;32m    363\u001b[0m timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 364\u001b[0m data_metadata_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_ms\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:1200\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.get_objects\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:169\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m t \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m500\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     results_dict \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     print_results(results_dict)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#evaluate([100])\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/tune/trainable.py:360\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warmup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[1;32m    359\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 360\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep() needs to return a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/agents/trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m step_ctx\u001b[38;5;241m.\u001b[39mshould_stop(step_attempt_results):\n\u001b[1;32m   1110\u001b[0m     \u001b[38;5;66;03m# Try to train one step.\u001b[39;00m\n\u001b[1;32m   1111\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1112\u001b[0m         step_attempt_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_attempt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;66;03m# @ray.remote RolloutWorker failure.\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m RayError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1115\u001b[0m         \u001b[38;5;66;03m# Try to recover w/o the failed worker.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/agents/trainer.py:1245\u001b[0m, in \u001b[0;36mTrainer.step_attempt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1243\u001b[0m             step_results\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate())\n\u001b[1;32m   1244\u001b[0m         \u001b[38;5;66;03m# Collect the training results from the future.\u001b[39;00m\n\u001b[0;32m-> 1245\u001b[0m         step_results\u001b[38;5;241m.\u001b[39mupdate(train_future\u001b[38;5;241m.\u001b[39mresult())\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;66;03m# Sequential: train (already done above), then eval.\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1248\u001b[0m     step_results\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate())\n",
      "File \u001b[0;32m/usr/lib/python3.8/concurrent/futures/_base.py:644\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 644\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/concurrent/futures/thread.py:236\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[0;32m--> 236\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:1011\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:1027\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# already determined that the C code is done\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_stopped\n\u001b[0;32m-> 1027\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1028\u001b[0m     lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = PPOTrainer(config=config)\n",
    "\n",
    "t = time.time()\n",
    "for i in range(500):\n",
    "    results_dict = agent.train()\n",
    "    print_results(results_dict)\n",
    "    #evaluate([100])\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.DEFAULT_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hierarchy/checkpoint_000022/checkpoint-22\n"
     ]
    }
   ],
   "source": [
    "print(agent.save(\"hierarchy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 16:14:17,018\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-08-15 16:14:17,187\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "2022-08-15 16:14:17,225\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "2022-08-15 16:14:17,226\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "2022-08-15 16:14:17,893\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-08-15 16:14:18,058\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "2022-08-15 16:14:18,093\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "2022-08-15 16:14:18,094\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "2022-08-15 16:14:18,142\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-08-15 16:14:19,387\tWARNING deprecation.py:46 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "2022-08-15 16:14:19,662\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "2022-08-15 16:14:19,706\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: hierarchy/checkpoint_000022/checkpoint-22\n",
      "2022-08-15 16:14:19,707\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 22, '_timesteps_total': None, '_time_total': 1400.5436375141144, '_episodes_total': 440}\n",
      "2022-08-15 16:14:20,479\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-08-15 16:14:20,906\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "2022-08-15 16:14:20,954\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "2022-08-15 16:14:20,955\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "2022-08-15 16:14:21,420\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-08-15 16:14:21,591\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "2022-08-15 16:14:21,627\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "2022-08-15 16:14:21,628\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19906)\u001b[0m 2022-08-15 16:14:26,496\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19914)\u001b[0m 2022-08-15 16:14:26,542\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19911)\u001b[0m 2022-08-15 16:14:26,519\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19907)\u001b[0m 2022-08-15 16:14:26,581\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19909)\u001b[0m 2022-08-15 16:14:26,686\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19906)\u001b[0m 2022-08-15 16:14:26,776\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19906)\u001b[0m 2022-08-15 16:14:26,818\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19906)\u001b[0m 2022-08-15 16:14:26,819\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19914)\u001b[0m 2022-08-15 16:14:26,811\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19911)\u001b[0m 2022-08-15 16:14:26,784\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19911)\u001b[0m 2022-08-15 16:14:26,824\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19911)\u001b[0m 2022-08-15 16:14:26,825\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19907)\u001b[0m 2022-08-15 16:14:26,843\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19914)\u001b[0m 2022-08-15 16:14:26,853\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19914)\u001b[0m 2022-08-15 16:14:26,854\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19907)\u001b[0m 2022-08-15 16:14:26,884\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19907)\u001b[0m 2022-08-15 16:14:26,884\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19909)\u001b[0m 2022-08-15 16:14:26,953\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19909)\u001b[0m 2022-08-15 16:14:26,993\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19909)\u001b[0m 2022-08-15 16:14:26,993\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19906)\u001b[0m 2022-08-15 16:14:27,206\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19914)\u001b[0m 2022-08-15 16:14:27,270\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19911)\u001b[0m 2022-08-15 16:14:27,208\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19907)\u001b[0m 2022-08-15 16:14:27,267\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19906)\u001b[0m 2022-08-15 16:14:27,386\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19911)\u001b[0m 2022-08-15 16:14:27,372\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19911)\u001b[0m 2022-08-15 16:14:27,406\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19911)\u001b[0m 2022-08-15 16:14:27,406\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19909)\u001b[0m 2022-08-15 16:14:27,381\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=19906)\u001b[0m 2022-08-15 16:14:27,432\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19906)\u001b[0m 2022-08-15 16:14:27,433\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19914)\u001b[0m 2022-08-15 16:14:27,437\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19914)\u001b[0m 2022-08-15 16:14:27,473\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19914)\u001b[0m 2022-08-15 16:14:27,473\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19907)\u001b[0m 2022-08-15 16:14:27,435\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19907)\u001b[0m 2022-08-15 16:14:27,468\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19907)\u001b[0m 2022-08-15 16:14:27,468\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19911)\u001b[0m 2022-08-15 16:14:27,622\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19909)\u001b[0m 2022-08-15 16:14:27,558\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19909)\u001b[0m 2022-08-15 16:14:27,593\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19909)\u001b[0m 2022-08-15 16:14:27,593\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19906)\u001b[0m 2022-08-15 16:14:27,667\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19914)\u001b[0m 2022-08-15 16:14:27,688\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19907)\u001b[0m 2022-08-15 16:14:27,683\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19909)\u001b[0m 2022-08-15 16:14:27,799\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for red agent RedMeanderAgent at steps 100 is: -26.8 with a standard deviation of 7.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 16:16:41,226\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-08-15 16:16:41,396\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "2022-08-15 16:16:41,435\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000394/checkpoint-394\n",
      "2022-08-15 16:16:41,436\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 394, '_timesteps_total': None, '_time_total': 13182.43779706955, '_episodes_total': 14180}\n",
      "2022-08-15 16:16:41,820\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-08-15 16:16:42,195\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "2022-08-15 16:16:42,230\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000487/checkpoint-487\n",
      "2022-08-15 16:16:42,231\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 487, '_timesteps_total': None, '_time_total': 15745.162350177765, '_episodes_total': 17520}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for red agent B_lineAgent at steps 100 is: -17.8 with a standard deviation of 9.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-26.84000000000002, -17.821999999999992]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.update({\"num_gpus\": 0, \"num_workers\": 0,})\n",
    "agent = PPOTrainer(config=config, env=\"multienv\")\n",
    "agent.restore(\"hierarchy/checkpoint_000022/checkpoint-22\")\n",
    "evaluate([100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint_000046  checkpoint_000107  checkpoint_000177  checkpoint_000338\r\n",
      "checkpoint_000050  checkpoint_000108  checkpoint_000178  checkpoint_000343\r\n",
      "checkpoint_000052  checkpoint_000113  checkpoint_000182  checkpoint_000365\r\n",
      "checkpoint_000067  checkpoint_000120  checkpoint_000183  checkpoint_000370\r\n",
      "checkpoint_000068  checkpoint_000133  checkpoint_000192  checkpoint_000394\r\n",
      "checkpoint_000069  checkpoint_000138  checkpoint_000200  checkpoint_000400\r\n",
      "checkpoint_000070  checkpoint_000140  checkpoint_000202  checkpoint_000435\r\n",
      "checkpoint_000071  checkpoint_000143  checkpoint_000203  checkpoint_000436\r\n",
      "checkpoint_000073  checkpoint_000144  checkpoint_000205  checkpoint_000487\r\n",
      "checkpoint_000074  checkpoint_000153  checkpoint_000214  checkpoint_000491\r\n",
      "checkpoint_000076  checkpoint_000154  checkpoint_000216  checkpoint_000498\r\n",
      "checkpoint_000077  checkpoint_000156  checkpoint_000239  checkpoint_000561\r\n",
      "checkpoint_000085  checkpoint_000157  checkpoint_000272  checkpoint_000569\r\n",
      "checkpoint_000098  checkpoint_000165  checkpoint_000318  checkpoint_000571\r\n",
      "checkpoint_000099  checkpoint_000169  checkpoint_000325  checkpoint_000576\r\n",
      "checkpoint_000104  checkpoint_000175  checkpoint_000326  checkpoint_000577\r\n",
      "checkpoint_000106  checkpoint_000176  checkpoint_000327  checkpoint_000668\r\n"
     ]
    }
   ],
   "source": [
    "!ls supervisor_ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -r supervisor_ppo/checkpoint_000070"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        b_line.restore(\"supervisor_ppo/checkpoint_000394/checkpoint-394\")\n",
    "        meander = PPOTrainer(config=ppo_config,env=\"CybORG\")\n",
    "        meander.restore(\"supervisor_ppo/checkpoint_000487/checkpoint-487\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-27-3957115643d1>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [27]\u001b[0;36m\u001b[0m\n\u001b[0;31m    t = \"checkpoint_000046  checkpoint_000107  checkpoint_000177  checkpoint_000338\u001b[0m\n\u001b[0m                                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "t = \"checkpoint_000046  checkpoint_000107  checkpoint_000177  checkpoint_000338\n",
    "checkpoint_000050  checkpoint_000108  checkpoint_000178  checkpoint_000343\n",
    "checkpoint_000052  checkpoint_000113  checkpoint_000182  checkpoint_000365\n",
    "checkpoint_000067  checkpoint_000120  checkpoint_000183  checkpoint_000370\n",
    "checkpoint_000068  checkpoint_000133  checkpoint_000192  checkpoint_000394\n",
    "checkpoint_000069  checkpoint_000138  checkpoint_000200  checkpoint_000400\n",
    "checkpoint_000070  checkpoint_000140  checkpoint_000202  checkpoint_000435\n",
    "checkpoint_000071  checkpoint_000143  checkpoint_000203  checkpoint_000436\n",
    "checkpoint_000073  checkpoint_000144  checkpoint_000205  checkpoint_000487\n",
    "checkpoint_000074  checkpoint_000153  checkpoint_000214  checkpoint_000491\n",
    "checkpoint_000076  checkpoint_000154  checkpoint_000216  checkpoint_000498\n",
    "checkpoint_000077  checkpoint_000156  checkpoint_000239  checkpoint_000561\n",
    "checkpoint_000085  checkpoint_000157  checkpoint_000272  checkpoint_000569\n",
    "checkpoint_000098  checkpoint_000165  checkpoint_000318  checkpoint_000571\n",
    "checkpoint_000099  checkpoint_000169  checkpoint_000325  checkpoint_000576\n",
    "checkpoint_000104  checkpoint_000175  checkpoint_000326  checkpoint_000577\n",
    "checkpoint_000106  checkpoint_000176  checkpoint_000327  checkpoint_000668\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shutil' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m l\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint_000487\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m l:\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mshutil\u001b[49m\u001b[38;5;241m.\u001b[39mrmtree(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupervisor_ppo/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39mp)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'shutil' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "l = os.listdir('supervisor_ppo')\n",
    "l.remove('checkpoint_000394')\n",
    "l.remove('checkpoint_000487')\n",
    "for p in l:\n",
    "    shutil.rmtree('supervisor_ppo/' +p) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
