{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Box\n",
    "import numpy as np\n",
    "\n",
    "from ray.rllib.models.modelv2 import ModelV2\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models.tf.fcnet import FullyConnectedNetwork\n",
    "from ray.rllib.models.torch.misc import SlimFC\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "from ray.rllib.agents.ppo.ppo import PPOTrainer\n",
    "from ray.rllib.agents.ppo import ppo\n",
    "from ray.rllib.models.tf.tf_action_dist import Categorical\n",
    "\n",
    "tf1, tf, tfv = try_import_tf()\n",
    "torch, nn = try_import_torch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HieracrchyModel(TFModelV2):\n",
    "    \n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "\n",
    "        super(HieracrchyModel, self).__init__(\n",
    "            obs_space, action_space, num_outputs, model_config, name)\n",
    "        # Base of the model for PPO agents\n",
    "        self.model = FullyConnectedNetwork(\n",
    "            obs_space, 2, num_outputs, model_config, name\n",
    "        )\n",
    "\n",
    "        # Upper DQN for action masking\n",
    "        #obs = tf.keras.layers.Input(shape=(obs_space.shape[0],), name=\"obs\")\n",
    "        #hidden_1 = tf.keras.layers.Dense(512, activation=tf.nn.tanh, name=\"hidden_1\")(obs)\n",
    "        #hidden_2 = tf.keras.layers.Dense(512, activation=tf.nn.tanh, name=\"hidden_2\")(hidden_1)\n",
    "        #q_values = tf.keras.layers.Dense(action_space.n, activation=None, name=\"q_values\")(hidden_2)\n",
    "        #self.supervisor_q_vals = tf.keras.Model(inputs=[obs], outputs=q_values)\n",
    "\n",
    "        ppo_config = ppo.DEFAULT_CONFIG.copy()\n",
    "        ppo_config.update({\"num_gpus\": 0,\"num_workers\": 0,\n",
    "                # Also, use \"framework: tf2\" for tfe eager execution.\n",
    "                \"framework\": \"tf2\",\n",
    "                \"train_batch_size\": batch_size,\n",
    "                \"horizon\": 100,\n",
    "                \"gamma\": 0.95,\n",
    "                \"model\": {\n",
    "                    \"fcnet_hiddens\": [512, 512],\n",
    "                    \"fcnet_activation\": \"relu\",\n",
    "                }})\n",
    "        self.b_line = PPOTrainer(config=ppo_config,env=\"CybORG\")\n",
    "        self.b_line.restore(\"b_line_agent/checkpoint_000109/checkpoint-109\")\n",
    "\n",
    "        self.meander = PPOTrainer(config=ppo_config,env=\"CybORG\")\n",
    "        self.meander.restore(\"supervisor_ppo/checkpoint_000076/checkpoint-76\")\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        action_logits = self.model.forward({'obs_flat': input_dict['obs']}, state, seq_lens)[0]\n",
    "        \n",
    "        #action_logits = self.model.forward(input_dict, state, seq_lens)[0]\n",
    "        return action_logits, state\n",
    "    \n",
    "    @override(ModelV2)\n",
    "    def value_function(self):\n",
    "        return self.model.value_function()\n",
    "\n",
    "def build_model(policy, obs_space, action_space, config):\n",
    "    model = ModelCatalog.get_model_v2(\n",
    "        obs_space,\n",
    "        action_space,\n",
    "        1,\n",
    "        config[\"model\"],\n",
    "        name=\"h_model\",\n",
    "        framework=\"tf2\",\n",
    "        model_interface=HieracrchyModel\n",
    "    )\n",
    "    policy.model_variables = model.variables()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_distribution_fn(policy , model, input_dict, *, explore, is_training, **kwargs):\n",
    "    print(input_dict)\n",
    "    logits = policy.model.forward(input_dict, None, None)[0]\n",
    "    action = tf.random.categorical(logits, 1, dtype=tf.int32)[0]\n",
    "    print(action)\n",
    "    logits = policy.model.b_line.compute_single_action(input_dict[SampleBatch.CUR_OBS][0], full_fetch=True, explore=False, training=False)[2]['action_dist_inputs']\n",
    "    print(logits)\n",
    "    return tf.expand_dims(logits, 1), Categorical, []\n",
    "   # logits = policy.model.meander.compute_single_action(input_dict[SampleBatch.CUR_OBS][0], full_fetch=True, explore=False, training=False)[2]['action_dist_inputs']\n",
    "   # return (tf.expand_dims(logits, 1), Categorical, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_action_sampler(policy, q_model, input_dict, explore, **kwargs):\n",
    "    logits = policy.model.forward({'obs_flat': input_dict}, None, None)[0]\n",
    "    action = tf.random.categorical(logits, 1, dtype=tf.int32)[0]\n",
    "    print(tf.random.categorical(logits, 1, dtype=tf.int32))\n",
    "    return action, logits[action[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.agents.ppo.ppo_tf_policy import PPOTFPolicy\n",
    "\n",
    "HierarchyPolicy = PPOTFPolicy.with_updates(\n",
    "    name=\"HierarchyPPOPolicy\",\n",
    "    make_model=build_model,\n",
    "    action_distribution_fn=action_distribution_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ray.rllib.agents.ppo.ppo_tf_policy import PPOTFPolicy\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "\n",
    "class SupervisorPolicy(PPOTFPolicy):\n",
    "    def __init__(self, observation_space, action_space, config):\n",
    "        PPOTFPolicy.__init__(self, observation_space, action_space, config)\n",
    "\n",
    "    @override(PPOTFPolicy)\n",
    "    def loss(self, model, dist_class, train_batch):\n",
    "        loss = super().loss\n",
    "        \n",
    "        #q_val_s = model.supervisor_q_vals({train_batch[SampleBatch.CUR_OBS]})[0]\n",
    "        #q_val_ns = model.supervisor_q_vals({train_batch[SampleBatch.NEXT_OBS]})[0]\n",
    "        \n",
    "        #action_selection = tf.cast(train_batch[SampleBatch.ACTIONS], dtype=tf.int32)   \n",
    "        #one_hot_selection = tf.one_hot(action_selection, 1)\n",
    "        #selected_q = tf.reduce_sum(q_val_s * one_hot_selection, 1)\n",
    "        #dones = tf.cast(train_batch[SampleBatch.DONES], tf.float32)\n",
    "        \n",
    "        #one_hot_max = tf.one_hot(tf.argmax(q_val_ns, 1), 1)\n",
    "        #ns_max_q = tf.reduce_sum(q_val_ns * one_hot_max, 1)\n",
    "        #ns_max_q = (1.0 - dones) * ns_max_q\n",
    "        #Calculate TD error and convert to huber loss\n",
    "        #target_q = (train_batch[SampleBatch.REWARDS] + policy.config[\"gamma\"] * ns_max_q)\n",
    "        #td_error = selected_q - tf.stop_gradient(target_q)\n",
    "        #DQN_Loss = tf.reduce_sum(huber_loss(td_error)) \n",
    "        \n",
    "        return loss #+ DQN_Loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.ppo.ppo import PPOTrainer\n",
    "class SVTrainer(PPOTrainer):\n",
    "    def get_default_policy_class(self, config):\n",
    "        return HierarchyPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-03 14:39:36,638\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-08-03 14:39:37,173\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-08-03 14:39:37,339\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "2022-08-03 14:39:37,376\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: b_line_agent/checkpoint_000109/checkpoint-109\n",
      "2022-08-03 14:39:37,376\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 109, '_timesteps_total': None, '_time_total': 4080.5770568847656, '_episodes_total': 4360}\n",
      "2022-08-03 14:39:37,775\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-08-03 14:39:37,939\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "2022-08-03 14:39:37,972\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000076/checkpoint-76\n",
      "2022-08-03 14:39:37,972\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 76, '_timesteps_total': None, '_time_total': 2702.382201910019, '_episodes_total': 3040}\n",
      "2022-08-03 14:39:38,012\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SampleBatch(32: ['obs', 'new_obs', 'actions', 'prev_actions', 'rewards', 'prev_rewards', 'dones', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't'])\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "[ 3.1435278e-01 -1.0851140e+00  2.1183152e+00  2.1210349e-01\n",
      "  1.2379090e+00  7.9796135e-01 -4.3306494e-01  1.3179013e+00\n",
      " -1.6268424e+00  1.5975549e+00  1.5686109e+00  2.1595149e+00\n",
      "  3.5432547e-01 -2.6048261e-01  2.0117016e+00 -4.0419511e-02\n",
      "  8.7674385e-01  1.6341534e+00 -2.0381639e+00  1.5786418e+00\n",
      "  1.9773276e+00 -3.8035417e-01 -3.5461459e-01  2.5708525e+00\n",
      "  8.6024022e-01  3.3051972e+00  9.4483644e-01 -1.2722969e-01\n",
      "  8.5048449e-01  3.8812587e+00  1.9359040e+00 -1.7192309e-01\n",
      " -2.4845111e+00  7.1764261e-01  2.3846743e+00  9.5787060e-01\n",
      " -7.6575726e-01  1.5630621e-01  1.7647816e+00  9.4396478e-01\n",
      " -5.6792873e-01  2.5413115e+00  4.3730459e-01  3.4920442e+00\n",
      "  8.4968156e-01  2.3837469e+00  3.3455942e+00  3.8005364e-01\n",
      "  1.9753731e+00  3.2737470e+00  9.1209275e-01  1.6775652e+00\n",
      "  5.7922840e-01 -2.0870343e-01  1.6221014e+00  4.1358433e+00\n",
      "  3.1654432e+00 -6.5308923e-01 -1.2369683e+00  1.7623407e-01\n",
      "  7.5568348e-02  2.0175557e+00  1.7457843e+00 -8.9672273e-01\n",
      "  8.6349326e-01  7.6984447e-01  1.5683703e+00  5.4023755e-01\n",
      "  1.0218338e+00 -5.8968031e-01  1.2471274e+00  1.5829452e+00\n",
      "  1.9871079e+00  2.2486502e-01  3.3822174e+00  3.8019168e+00\n",
      "  1.4475559e+00  1.5495743e+00  1.9588596e+00  1.8368594e-01\n",
      "  7.8730291e-01  2.2213452e+00 -1.3347731e+00  2.2895682e+00\n",
      " -1.9241734e+00 -2.5965002e-01  1.0953054e+00 -3.7961966e-01\n",
      "  4.1694560e+00  1.1322031e+00  2.7034385e+00 -1.3852753e-03\n",
      " -4.1335940e-01  1.3585329e+00 -2.6136920e+00  1.5049688e+00\n",
      "  2.9134593e+00  1.6279548e-01  2.6932166e+00  2.4946761e+00\n",
      "  3.1169937e+00  1.9457511e+00  9.8845744e-01  1.5525689e+00\n",
      "  2.1567934e+00  2.3552351e+00  1.7432425e+00  4.0014682e+00\n",
      "  8.3438933e-01  1.4945018e-01  1.4143951e+00  2.0729268e+00\n",
      "  5.4154521e-01 -9.7730175e-02 -7.8889859e-01  1.8194344e+00\n",
      "  9.5299369e-01  1.4125224e+00  1.3954312e+00  8.0238599e-01\n",
      "  3.5615039e+00  7.7134842e-01 -1.7298623e+00  6.0160375e-01\n",
      "  1.5257963e+00  2.6044261e+00  1.9674530e+00  2.0101330e+00\n",
      "  2.3655614e-01 -1.4373224e+00  1.3438553e+00 -1.5243901e-01\n",
      "  4.0283012e-01 -1.2257280e+00  1.8888400e-01  7.5140762e-01\n",
      "  8.0273449e-01 -2.5625350e+00 -2.2202544e+00 -2.4085501e-01\n",
      "  1.3857586e+00  1.6695471e+00  4.4245270e-01  2.9919682e-02\n",
      "  7.1952975e-01]\n",
      "SampleBatch(1: ['obs'])\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "[ 3.1435278e-01 -1.0851140e+00  2.1183152e+00  2.1210349e-01\n",
      "  1.2379090e+00  7.9796135e-01 -4.3306494e-01  1.3179013e+00\n",
      " -1.6268424e+00  1.5975549e+00  1.5686109e+00  2.1595149e+00\n",
      "  3.5432547e-01 -2.6048261e-01  2.0117016e+00 -4.0419511e-02\n",
      "  8.7674385e-01  1.6341534e+00 -2.0381639e+00  1.5786418e+00\n",
      "  1.9773276e+00 -3.8035417e-01 -3.5461459e-01  2.5708525e+00\n",
      "  8.6024022e-01  3.3051972e+00  9.4483644e-01 -1.2722969e-01\n",
      "  8.5048449e-01  3.8812587e+00  1.9359040e+00 -1.7192309e-01\n",
      " -2.4845111e+00  7.1764261e-01  2.3846743e+00  9.5787060e-01\n",
      " -7.6575726e-01  1.5630621e-01  1.7647816e+00  9.4396478e-01\n",
      " -5.6792873e-01  2.5413115e+00  4.3730459e-01  3.4920442e+00\n",
      "  8.4968156e-01  2.3837469e+00  3.3455942e+00  3.8005364e-01\n",
      "  1.9753731e+00  3.2737470e+00  9.1209275e-01  1.6775652e+00\n",
      "  5.7922840e-01 -2.0870343e-01  1.6221014e+00  4.1358433e+00\n",
      "  3.1654432e+00 -6.5308923e-01 -1.2369683e+00  1.7623407e-01\n",
      "  7.5568348e-02  2.0175557e+00  1.7457843e+00 -8.9672273e-01\n",
      "  8.6349326e-01  7.6984447e-01  1.5683703e+00  5.4023755e-01\n",
      "  1.0218338e+00 -5.8968031e-01  1.2471274e+00  1.5829452e+00\n",
      "  1.9871079e+00  2.2486502e-01  3.3822174e+00  3.8019168e+00\n",
      "  1.4475559e+00  1.5495743e+00  1.9588596e+00  1.8368594e-01\n",
      "  7.8730291e-01  2.2213452e+00 -1.3347731e+00  2.2895682e+00\n",
      " -1.9241734e+00 -2.5965002e-01  1.0953054e+00 -3.7961966e-01\n",
      "  4.1694560e+00  1.1322031e+00  2.7034385e+00 -1.3852753e-03\n",
      " -4.1335940e-01  1.3585329e+00 -2.6136920e+00  1.5049688e+00\n",
      "  2.9134593e+00  1.6279548e-01  2.6932166e+00  2.4946761e+00\n",
      "  3.1169937e+00  1.9457511e+00  9.8845744e-01  1.5525689e+00\n",
      "  2.1567934e+00  2.3552351e+00  1.7432425e+00  4.0014682e+00\n",
      "  8.3438933e-01  1.4945018e-01  1.4143951e+00  2.0729268e+00\n",
      "  5.4154521e-01 -9.7730175e-02 -7.8889859e-01  1.8194344e+00\n",
      "  9.5299369e-01  1.4125224e+00  1.3954312e+00  8.0238599e-01\n",
      "  3.5615039e+00  7.7134842e-01 -1.7298623e+00  6.0160375e-01\n",
      "  1.5257963e+00  2.6044261e+00  1.9674530e+00  2.0101330e+00\n",
      "  2.3655614e-01 -1.4373224e+00  1.3438553e+00 -1.5243901e-01\n",
      "  4.0283012e-01 -1.2257280e+00  1.8888400e-01  7.5140762e-01\n",
      "  8.0273449e-01 -2.5625350e+00 -2.2202544e+00 -2.4085501e-01\n",
      "  1.3857586e+00  1.6695471e+00  4.4245270e-01  2.9919682e-02\n",
      "  7.1952975e-01]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [168]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m novel_actions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m):\n\u001b[0;32m---> 64\u001b[0m     results_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     print_results(results_dict)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/tune/trainable.py:360\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warmup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[1;32m    359\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 360\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep() needs to return a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/agents/trainer.py:1136\u001b[0m, in \u001b[0;36mTrainer.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1134\u001b[0m             \u001b[38;5;66;03m# Allow logs messages to propagate.\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m             time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m-> 1136\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1138\u001b[0m result \u001b[38;5;241m=\u001b[39m step_attempt_results\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworkers\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers, WorkerSet):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;66;03m# Sync filters on workers.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/agents/trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m step_ctx\u001b[38;5;241m.\u001b[39mshould_stop(step_attempt_results):\n\u001b[1;32m   1110\u001b[0m     \u001b[38;5;66;03m# Try to train one step.\u001b[39;00m\n\u001b[1;32m   1111\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1112\u001b[0m         step_attempt_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_attempt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;66;03m# @ray.remote RolloutWorker failure.\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m RayError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1115\u001b[0m         \u001b[38;5;66;03m# Try to recover w/o the failed worker.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/agents/trainer.py:1214\u001b[0m, in \u001b[0;36mTrainer.step_attempt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;66;03m# No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evaluate_this_iter:\n\u001b[0;32m-> 1214\u001b[0m     step_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_exec_plan_or_training_iteration_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# We have to evaluate in this training iteration.\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1217\u001b[0m     \u001b[38;5;66;03m# No parallelism.\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_parallel_to_training\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/agents/trainer.py:2209\u001b[0m, in \u001b[0;36mTrainer._exec_plan_or_training_iteration_fn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2207\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timers[TRAINING_ITERATION_TIMER]:\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_disable_execution_plan_api\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m-> 2209\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2210\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2211\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_exec_impl)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/agents/ppo/ppo.py:437\u001b[0m, in \u001b[0;36mPPOTrainer.training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m     train_batch \u001b[38;5;241m=\u001b[39m synchronous_parallel_sample(\n\u001b[1;32m    434\u001b[0m         worker_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers, max_agent_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    435\u001b[0m     )\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 437\u001b[0m     train_batch \u001b[38;5;241m=\u001b[39m \u001b[43msynchronous_parallel_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_env_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_batch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m train_batch \u001b[38;5;241m=\u001b[39m train_batch\u001b[38;5;241m.\u001b[39mas_multi_agent()\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_counters[NUM_AGENT_STEPS_SAMPLED] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_batch\u001b[38;5;241m.\u001b[39magent_steps()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/execution/rollout_ops.py:96\u001b[0m, in \u001b[0;36msynchronous_parallel_sample\u001b[0;34m(worker_set, max_agent_steps, max_env_steps, concat)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (max_agent_or_env_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m agent_or_env_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m     90\u001b[0m     max_agent_or_env_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m agent_or_env_steps \u001b[38;5;241m<\u001b[39m max_agent_or_env_steps\n\u001b[1;32m     92\u001b[0m ):\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# No remote workers in the set -> Use local worker for collecting\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# samples.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m worker_set\u001b[38;5;241m.\u001b[39mremote_workers():\n\u001b[0;32m---> 96\u001b[0m         sample_batches \u001b[38;5;241m=\u001b[39m [\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# Loop over remote workers' `sample()` method in parallel.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m         sample_batches \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    100\u001b[0m             [worker\u001b[38;5;241m.\u001b[39msample\u001b[38;5;241m.\u001b[39mremote() \u001b[38;5;28;01mfor\u001b[39;00m worker \u001b[38;5;129;01min\u001b[39;00m worker_set\u001b[38;5;241m.\u001b[39mremote_workers()]\n\u001b[1;32m    101\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py:825\u001b[0m, in \u001b[0;36mRolloutWorker.sample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_start\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating sample batch of size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    821\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_fragment_length\n\u001b[1;32m    822\u001b[0m         )\n\u001b[1;32m    823\u001b[0m     )\n\u001b[0;32m--> 825\u001b[0m batches \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    826\u001b[0m steps_so_far \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    827\u001b[0m     batches[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcount\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount_steps_by \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m batches[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39magent_steps()\n\u001b[1;32m    830\u001b[0m )\n\u001b[1;32m    832\u001b[0m \u001b[38;5;66;03m# In truncate_episodes mode, never pull more than 1 batch per env.\u001b[39;00m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;66;03m# This avoids over-running the target batch size.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/sampler.py:115\u001b[0m, in \u001b[0;36mSamplerInput.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@override\u001b[39m(InputReader)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SampleBatchType:\n\u001b[0;32m--> 115\u001b[0m     batches \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    116\u001b[0m     batches\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_extra_batches())\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batches) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/sampler.py:288\u001b[0m, in \u001b[0;36mSyncSampler.get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@override\u001b[39m(SamplerInput)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_data\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SampleBatchType:\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m         item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env_runner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, RolloutMetrics):\n\u001b[1;32m    290\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics_queue\u001b[38;5;241m.\u001b[39mput(item)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/sampler.py:706\u001b[0m, in \u001b[0;36m_env_runner\u001b[0;34m(worker, base_env, extra_batch_callback, horizon, normalize_actions, clip_actions, multiple_episodes_in_batch, callbacks, perf_stats, soft_horizon, no_done_at_end, observation_fn, sample_collector, render)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Process results and update episode state.\u001b[39;00m\n\u001b[1;32m    703\u001b[0m t3 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    704\u001b[0m actions_to_send: Dict[\n\u001b[1;32m    705\u001b[0m     EnvID, Dict[AgentID, EnvActionType]\n\u001b[0;32m--> 706\u001b[0m ] \u001b[38;5;241m=\u001b[39m \u001b[43m_process_policy_eval_results\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mto_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactive_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactive_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactive_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactive_envs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m    \u001b[49m\u001b[43moff_policy_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moff_policy_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalize_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    716\u001b[0m perf_stats\u001b[38;5;241m.\u001b[39maction_processing_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t3\n\u001b[1;32m    718\u001b[0m \u001b[38;5;66;03m# Return computed actions to ready envs. We also send to envs that have\u001b[39;00m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;66;03m# taken off-policy actions; those envs are free to ignore the action.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/sampler.py:1240\u001b[0m, in \u001b[0;36m_process_policy_eval_results\u001b[0;34m(to_eval, eval_results, active_episodes, active_envs, off_policy_actions, policies, normalize_actions, clip_actions)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1238\u001b[0m     action_to_send \u001b[38;5;241m=\u001b[39m action\n\u001b[0;32m-> 1240\u001b[0m env_id: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43meval_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39menv_id\n\u001b[1;32m   1241\u001b[0m agent_id: AgentID \u001b[38;5;241m=\u001b[39m eval_data[i]\u001b[38;5;241m.\u001b[39magent_id\n\u001b[1;32m   1242\u001b[0m episode: Episode \u001b[38;5;241m=\u001b[39m active_episodes[env_id]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from CybORG import CybORG\n",
    "from CybORG.Agents import B_lineAgent, SleepAgent, GreenAgent\n",
    "from CybORG.Agents.SimpleAgents.BaseAgent import BaseAgent\n",
    "from CybORG.Agents.SimpleAgents.BlueReactAgent import BlueReactRemoveAgent\n",
    "from CybORG.Agents.SimpleAgents.Meander import RedMeanderAgent\n",
    "from CybORG.Agents.Wrappers.EnumActionWrapper import EnumActionWrapper\n",
    "from CybORG.Agents.Wrappers.FixedFlatWrapper import FixedFlatWrapper\n",
    "from CybORG.Agents.Wrappers.OpenAIGymWrapper import OpenAIGymWrapper\n",
    "from CybORG.Agents.Wrappers.ReduceActionSpaceWrapper import ReduceActionSpaceWrapper\n",
    "from CybORG.Agents.Wrappers import ChallengeWrapper\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.agents import ppo\n",
    "import inspect\n",
    "from CybORG.Agents.Wrappers.rllib_wrapper import RLlibWrapper\n",
    "\n",
    "def env_creator(env_config: dict):\n",
    "    path = str(inspect.getfile(CybORG))\n",
    "    path = path[:-10] + '/Shared/Scenarios/Scenario2.yaml'\n",
    "    agents = {\"Red\": B_lineAgent, \"Green\": GreenAgent}\n",
    "    cyborg = CybORG(scenario_file=path, environment='sim', agents=agents)\n",
    "    env = RLlibWrapper(env=cyborg, agent_name=\"Blue\", max_steps=100)\n",
    "    return env\n",
    "\n",
    "def print_results(results_dict):\n",
    "    train_iter = results_dict[\"training_iteration\"]\n",
    "    r_mean = results_dict[\"episode_reward_mean\"]\n",
    "    r_max = results_dict[\"episode_reward_max\"]\n",
    "    r_min = results_dict[\"episode_reward_min\"]\n",
    "    print(f\"{train_iter:4d} \\tr_mean: {r_mean:.1f} \\tr_max: {r_max:.1f} \\tr_min: {r_min: .1f}\")\n",
    "\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "\n",
    "batch_size = 2000\n",
    "# Set up CybORG\n",
    "register_env(name=\"CybORG\", env_creator=env_creator)\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "\n",
    "#ModelCatalog.register_custom_model(\"h_model\", HieracrchyModel)\n",
    "\n",
    "from shutil import make_archive\n",
    "allrewards = []\n",
    "\n",
    "config.update({\"num_gpus\": 1,\"num_workers\": 0,\n",
    "                # Also, use \"framework: tf2\" for tfe eager execution.\n",
    "                \"framework\": \"tf2\",\n",
    "                \"train_batch_size\": batch_size,\n",
    "                \"horizon\": 100,\n",
    "                \"gamma\": 0.9,\n",
    "                \"model\": {\n",
    "                    \"fcnet_hiddens\": [512],\n",
    "                    \"fcnet_activation\": \"relu\",\n",
    "                },\n",
    "\n",
    "                }) \n",
    "trainer = SVTrainer(config=config, env=\"CybORG\")\n",
    "\n",
    "reward = []\n",
    "novel_obs = []\n",
    "novel_actions = []\n",
    "for i in range(200):\n",
    "    results_dict = trainer.train()\n",
    "    print_results(results_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
