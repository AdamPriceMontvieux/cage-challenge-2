{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Box\n",
    "import numpy as np\n",
    "\n",
    "from ray.rllib.models.modelv2 import ModelV2\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models.tf.fcnet import FullyConnectedNetwork\n",
    "from ray.rllib.models.torch.misc import SlimFC\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "from ray.rllib.agents.dqn.dqn import DQNTrainer\n",
    "from ray.rllib.agents.dqn import dqn\n",
    "\n",
    "\n",
    "tf1, tf, tfv = try_import_tf()\n",
    "torch, nn = try_import_torch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisorModel(TFModelV2):\n",
    "    \n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        super(SupervisorModel, self).__init__(\n",
    "            obs_space, action_space, num_outputs, model_config, name\n",
    "        )\n",
    "        # Base of the model for PPO agents\n",
    "        self.model = FullyConnectedNetwork(\n",
    "            obs_space, action_space, num_outputs, model_config, name\n",
    "        )\n",
    "  \n",
    "        # Upper DQN for action masking\n",
    "        #obs = tf.keras.layers.Input(shape=(obs_space.shape[0],), name=\"obs\")\n",
    "        #hidden_1 = tf.keras.layers.Dense(512, activation=tf.nn.tanh, name=\"hidden_1\")(obs)\n",
    "        #hidden_2 = tf.keras.layers.Dense(512, activation=tf.nn.tanh, name=\"hidden_2\")(hidden_1)\n",
    "        #q_values = tf.keras.layers.Dense(action_space.n, activation=None, name=\"q_values\")(hidden_2)\n",
    "        #self.supervisor_q_vals = tf.keras.Model(inputs=[obs], outputs=q_values)\n",
    "    \n",
    "        dqn_config = dqn.DEFAULT_CONFIG.copy()\n",
    "        dqn_config.update({\"num_gpus\": 0,\"num_workers\":0,\n",
    "                            \"framework\": \"tf2\",\n",
    "                            \"horizon\": 100,\n",
    "                            'train_batch_size': 32,\n",
    "                            'double_q': True,\n",
    "                            'dueling': True,\n",
    "                           'hiddens': [512],\n",
    "                            \"model\": {\n",
    "                                \"fcnet_hiddens\": [512],\n",
    "                                \"fcnet_activation\": \"relu\",\n",
    "                           }}) \n",
    "        self.supervisor = DQNTrainer(config=dqn_config,env=\"CybORG\")\n",
    "        self.supervisor.restore(\"/Supervisor/checkpoint_000639/checkpoint-639\")\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "    \n",
    "        action_logits = self.model.forward(input_dict, state, seq_lens)[0]\n",
    "       # if input_dict[SampleBatch.CUR_OBS].shape[0] > 1:\n",
    "\n",
    "       #     return action_logits, state\n",
    "        #q_vals = self.supervisor_q_vals(input_dict[SampleBatch.CUR_OBS])\n",
    "        q_vals = self.supervisor.compute_single_action(input_dict[SampleBatch.CUR_OBS][0], full_fetch=True, explore=False, training=False)[2]['q_values']\n",
    "        max_q_vals = tf.math.top_k(q_vals, k=20, sorted=False, name=None)\n",
    "        indices = tf.cast(max_q_vals.indices, dtype=tf.int32).numpy()\n",
    "        action_mask = np.zeros(tf.shape(action_logits).numpy(), dtype=int)\n",
    "        action_mask[:,indices] = 1\n",
    "        \n",
    "        # Expand the model output to [BATCH, 1, EMBED_SIZE]. Note that the\n",
    "        # avail actions tensor is of shape [BATCH, MAX_ACTIONS, EMBED_SIZE].\n",
    "       # intent_vector = tf.expand_dims(action_embed, 1)\n",
    "        action_logits = tf.expand_dims(action_logits, 1)\n",
    "        # Batch dot product => shape of logits is [BATCH, MAX_ACTIONS].\n",
    "        action_logits = tf.cast(tf.reduce_sum(action_mask * action_logits, axis=1), dtype=tf.float32)\n",
    "        # Mask out invalid actions (use tf.float32.min for stability)\n",
    "        inf_mask = tf.cast(tf.maximum(np.log(action_mask), tf.float32.min), dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        return action_logits + inf_mask, state\n",
    "        \n",
    "        #action_mask = np.zeros(tf.shape(q_vals).numpy(), dtype=bool)\n",
    "       # action_mask[indices] = True\n",
    "       # masked_logits = action_logits[0] * action_mask\n",
    "       # return tf.expand_dims(masked_logits, axis=0), state\n",
    "    \n",
    "    @override(ModelV2)\n",
    "    def value_function(self):\n",
    "        return self.model.value_function()\n",
    "\n",
    "    def q_value_function(self, obs, opponent_obs, opponent_actions):\n",
    "        return tf.reshape(self.supervisor_q_vals(obs),[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 425 \tr_mean: -27.8 \tr_max: -11.8 \tr_min: -147.7\n",
    "/Supervisor/checkpoint_000425/checkpoint-425"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ray.rllib.agents.ppo.ppo_tf_policy import PPOTFPolicy\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "\n",
    "class SupervisorPolicy(PPOTFPolicy):\n",
    "    def __init__(self, observation_space, action_space, config):\n",
    "        PPOTFPolicy.__init__(self, observation_space, action_space, config)\n",
    "\n",
    "    @override(PPOTFPolicy)\n",
    "    def loss(self, model, dist_class, train_batch):\n",
    "        loss = super().loss\n",
    "        \n",
    "        #q_val_s = model.supervisor_q_vals({train_batch[SampleBatch.CUR_OBS]})[0]\n",
    "        #q_val_ns = model.supervisor_q_vals({train_batch[SampleBatch.NEXT_OBS]})[0]\n",
    "        \n",
    "        #action_selection = tf.cast(train_batch[SampleBatch.ACTIONS], dtype=tf.int32)   \n",
    "        #one_hot_selection = tf.one_hot(action_selection, 1)\n",
    "        #selected_q = tf.reduce_sum(q_val_s * one_hot_selection, 1)\n",
    "        #dones = tf.cast(train_batch[SampleBatch.DONES], tf.float32)\n",
    "        \n",
    "        #one_hot_max = tf.one_hot(tf.argmax(q_val_ns, 1), 1)\n",
    "        #ns_max_q = tf.reduce_sum(q_val_ns * one_hot_max, 1)\n",
    "        #ns_max_q = (1.0 - dones) * ns_max_q\n",
    "        #Calculate TD error and convert to huber loss\n",
    "        #target_q = (train_batch[SampleBatch.REWARDS] + policy.config[\"gamma\"] * ns_max_q)\n",
    "        #td_error = selected_q - tf.stop_gradient(target_q)\n",
    "        #DQN_Loss = tf.reduce_sum(huber_loss(td_error)) \n",
    "        \n",
    "        return loss #+ DQN_Loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.ppo.ppo import PPOTrainer\n",
    "class SVTrainer(PPOTrainer):\n",
    "    def get_default_policy_class(self, config):\n",
    "        return SupervisorPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=25966)\u001b[0m 2022-08-02 12:43:00,202\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25967)\u001b[0m 2022-08-02 12:43:00,490\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25969)\u001b[0m 2022-08-02 12:43:00,643\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25948)\u001b[0m 2022-08-02 12:43:00,715\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25936)\u001b[0m 2022-08-02 12:43:01,114\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25961)\u001b[0m 2022-08-02 12:43:01,244\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25937)\u001b[0m 2022-08-02 12:43:01,379\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25966)\u001b[0m 2022-08-02 12:43:01,576\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25938)\u001b[0m 2022-08-02 12:43:01,715\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25970)\u001b[0m 2022-08-02 12:43:01,694\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25944)\u001b[0m 2022-08-02 12:43:01,852\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25952)\u001b[0m 2022-08-02 12:43:01,870\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25967)\u001b[0m 2022-08-02 12:43:01,981\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25942)\u001b[0m 2022-08-02 12:43:01,951\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25946)\u001b[0m 2022-08-02 12:43:01,978\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25966)\u001b[0m 2022-08-02 12:43:02,038\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25966)\u001b[0m 2022-08-02 12:43:02,151\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: /Supervisor/checkpoint_000639/checkpoint-639\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25966)\u001b[0m 2022-08-02 12:43:02,151\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 639, '_timesteps_total': None, '_time_total': 2267.168159008026, '_episodes_total': 6640}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25966)\u001b[0m <ipython-input-31-b68e640f6b83>:55: RuntimeWarning: divide by zero encountered in log\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25969)\u001b[0m 2022-08-02 12:43:02,236\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25967)\u001b[0m 2022-08-02 12:43:02,358\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25948)\u001b[0m 2022-08-02 12:43:02,329\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25959)\u001b[0m 2022-08-02 12:43:02,334\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25967)\u001b[0m 2022-08-02 12:43:02,412\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: /Supervisor/checkpoint_000639/checkpoint-639\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25967)\u001b[0m 2022-08-02 12:43:02,413\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 639, '_timesteps_total': None, '_time_total': 2267.168159008026, '_episodes_total': 6640}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25967)\u001b[0m <ipython-input-31-b68e640f6b83>:55: RuntimeWarning: divide by zero encountered in log\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25939)\u001b[0m 2022-08-02 12:43:02,466\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25957)\u001b[0m 2022-08-02 12:43:02,403\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25950)\u001b[0m 2022-08-02 12:43:02,581\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25969)\u001b[0m 2022-08-02 12:43:02,629\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25969)\u001b[0m 2022-08-02 12:43:02,673\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: /Supervisor/checkpoint_000639/checkpoint-639\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25969)\u001b[0m 2022-08-02 12:43:02,673\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 639, '_timesteps_total': None, '_time_total': 2267.168159008026, '_episodes_total': 6640}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25969)\u001b[0m <ipython-input-31-b68e640f6b83>:55: RuntimeWarning: divide by zero encountered in log\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25955)\u001b[0m 2022-08-02 12:43:02,631\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=25948)\u001b[0m 2022-08-02 12:43:02,823\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25968)\u001b[0m 2022-08-02 12:43:02,730\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25948)\u001b[0m 2022-08-02 12:43:02,872\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: /Supervisor/checkpoint_000639/checkpoint-639\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25948)\u001b[0m 2022-08-02 12:43:02,872\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 639, '_timesteps_total': None, '_time_total': 2267.168159008026, '_episodes_total': 6640}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25948)\u001b[0m <ipython-input-31-b68e640f6b83>:55: RuntimeWarning: divide by zero encountered in log\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25961)\u001b[0m 2022-08-02 12:43:02,855\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25936)\u001b[0m 2022-08-02 12:43:03,027\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25961)\u001b[0m 2022-08-02 12:43:03,173\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25954)\u001b[0m 2022-08-02 12:43:03,079\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25937)\u001b[0m 2022-08-02 12:43:03,253\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25961)\u001b[0m 2022-08-02 12:43:03,208\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: /Supervisor/checkpoint_000639/checkpoint-639\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25961)\u001b[0m 2022-08-02 12:43:03,209\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 639, '_timesteps_total': None, '_time_total': 2267.168159008026, '_episodes_total': 6640}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25961)\u001b[0m <ipython-input-31-b68e640f6b83>:55: RuntimeWarning: divide by zero encountered in log\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25938)\u001b[0m 2022-08-02 12:43:03,333\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25944)\u001b[0m 2022-08-02 12:43:03,351\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25952)\u001b[0m 2022-08-02 12:43:03,382\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25970)\u001b[0m 2022-08-02 12:43:03,346\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25942)\u001b[0m 2022-08-02 12:43:03,302\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25946)\u001b[0m 2022-08-02 12:43:03,346\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25936)\u001b[0m 2022-08-02 12:43:03,444\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25936)\u001b[0m 2022-08-02 12:43:03,490\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: /Supervisor/checkpoint_000639/checkpoint-639\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25936)\u001b[0m 2022-08-02 12:43:03,490\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 639, '_timesteps_total': None, '_time_total': 2267.168159008026, '_episodes_total': 6640}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25936)\u001b[0m <ipython-input-31-b68e640f6b83>:55: RuntimeWarning: divide by zero encountered in log\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25942)\u001b[0m 2022-08-02 12:43:03,621\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25937)\u001b[0m 2022-08-02 12:43:03,681\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25937)\u001b[0m 2022-08-02 12:43:03,721\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: /Supervisor/checkpoint_000639/checkpoint-639\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25937)\u001b[0m 2022-08-02 12:43:03,721\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 639, '_timesteps_total': None, '_time_total': 2267.168159008026, '_episodes_total': 6640}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25937)\u001b[0m <ipython-input-31-b68e640f6b83>:55: RuntimeWarning: divide by zero encountered in log\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25944)\u001b[0m 2022-08-02 12:43:03,676\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25944)\u001b[0m 2022-08-02 12:43:03,713\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: /Supervisor/checkpoint_000639/checkpoint-639\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25944)\u001b[0m 2022-08-02 12:43:03,713\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 639, '_timesteps_total': None, '_time_total': 2267.168159008026, '_episodes_total': 6640}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25944)\u001b[0m <ipython-input-31-b68e640f6b83>:55: RuntimeWarning: divide by zero encountered in log\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25952)\u001b[0m 2022-08-02 12:43:03,753\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25970)\u001b[0m 2022-08-02 12:43:03,729\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25942)\u001b[0m 2022-08-02 12:43:03,657\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: /Supervisor/checkpoint_000639/checkpoint-639\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25942)\u001b[0m 2022-08-02 12:43:03,658\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 639, '_timesteps_total': None, '_time_total': 2267.168159008026, '_episodes_total': 6640}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25942)\u001b[0m <ipython-input-31-b68e640f6b83>:55: RuntimeWarning: divide by zero encountered in log\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25946)\u001b[0m 2022-08-02 12:43:03,661\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25946)\u001b[0m 2022-08-02 12:43:03,699\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: /Supervisor/checkpoint_000639/checkpoint-639\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25946)\u001b[0m 2022-08-02 12:43:03,699\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 639, '_timesteps_total': None, '_time_total': 2267.168159008026, '_episodes_total': 6640}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25946)\u001b[0m <ipython-input-31-b68e640f6b83>:55: RuntimeWarning: divide by zero encountered in log\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25959)\u001b[0m 2022-08-02 12:43:03,663\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25938)\u001b[0m 2022-08-02 12:43:03,800\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25938)\u001b[0m 2022-08-02 12:43:03,843\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: /Supervisor/checkpoint_000639/checkpoint-639\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25938)\u001b[0m 2022-08-02 12:43:03,843\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 639, '_timesteps_total': None, '_time_total': 2267.168159008026, '_episodes_total': 6640}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25952)\u001b[0m 2022-08-02 12:43:03,790\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: /Supervisor/checkpoint_000639/checkpoint-639\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25952)\u001b[0m 2022-08-02 12:43:03,790\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 639, '_timesteps_total': None, '_time_total': 2267.168159008026, '_episodes_total': 6640}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25952)\u001b[0m <ipython-input-31-b68e640f6b83>:55: RuntimeWarning: divide by zero encountered in log\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25970)\u001b[0m 2022-08-02 12:43:03,765\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: /Supervisor/checkpoint_000639/checkpoint-639\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25970)\u001b[0m 2022-08-02 12:43:03,765\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 639, '_timesteps_total': None, '_time_total': 2267.168159008026, '_episodes_total': 6640}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25970)\u001b[0m <ipython-input-31-b68e640f6b83>:55: RuntimeWarning: divide by zero encountered in log\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25939)\u001b[0m 2022-08-02 12:43:03,848\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25957)\u001b[0m 2022-08-02 12:43:03,773\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25938)\u001b[0m <ipython-input-31-b68e640f6b83>:55: RuntimeWarning: divide by zero encountered in log\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25950)\u001b[0m 2022-08-02 12:43:03,900\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25959)\u001b[0m 2022-08-02 12:43:03,991\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25959)\u001b[0m 2022-08-02 12:43:04,026\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: /Supervisor/checkpoint_000639/checkpoint-639\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25959)\u001b[0m 2022-08-02 12:43:04,026\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 639, '_timesteps_total': None, '_time_total': 2267.168159008026, '_episodes_total': 6640}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25959)\u001b[0m <ipython-input-31-b68e640f6b83>:55: RuntimeWarning: divide by zero encountered in log\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25955)\u001b[0m 2022-08-02 12:43:03,986\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25968)\u001b[0m 2022-08-02 12:43:03,997\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25957)\u001b[0m 2022-08-02 12:43:04,091\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=25957)\u001b[0m 2022-08-02 12:43:04,127\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: /Supervisor/checkpoint_000639/checkpoint-639\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25957)\u001b[0m 2022-08-02 12:43:04,127\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 639, '_timesteps_total': None, '_time_total': 2267.168159008026, '_episodes_total': 6640}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25957)\u001b[0m <ipython-input-31-b68e640f6b83>:55: RuntimeWarning: divide by zero encountered in log\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25939)\u001b[0m 2022-08-02 12:43:04,214\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25939)\u001b[0m 2022-08-02 12:43:04,247\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: /Supervisor/checkpoint_000639/checkpoint-639\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25939)\u001b[0m 2022-08-02 12:43:04,247\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 639, '_timesteps_total': None, '_time_total': 2267.168159008026, '_episodes_total': 6640}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25939)\u001b[0m <ipython-input-31-b68e640f6b83>:55: RuntimeWarning: divide by zero encountered in log\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25955)\u001b[0m 2022-08-02 12:43:04,320\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25950)\u001b[0m 2022-08-02 12:43:04,257\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25950)\u001b[0m 2022-08-02 12:43:04,291\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: /Supervisor/checkpoint_000639/checkpoint-639\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25950)\u001b[0m 2022-08-02 12:43:04,291\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 639, '_timesteps_total': None, '_time_total': 2267.168159008026, '_episodes_total': 6640}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25950)\u001b[0m <ipython-input-31-b68e640f6b83>:55: RuntimeWarning: divide by zero encountered in log\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25968)\u001b[0m 2022-08-02 12:43:04,333\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25955)\u001b[0m 2022-08-02 12:43:04,357\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: /Supervisor/checkpoint_000639/checkpoint-6392022-08-02 12:43:04,458\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25955)\u001b[0m 2022-08-02 12:43:04,357\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 639, '_timesteps_total': None, '_time_total': 2267.168159008026, '_episodes_total': 6640}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25955)\u001b[0m <ipython-input-31-b68e640f6b83>:55: RuntimeWarning: divide by zero encountered in log\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25968)\u001b[0m 2022-08-02 12:43:04,369\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: /Supervisor/checkpoint_000639/checkpoint-639\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25968)\u001b[0m 2022-08-02 12:43:04,369\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 639, '_timesteps_total': None, '_time_total': 2267.168159008026, '_episodes_total': 6640}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25968)\u001b[0m <ipython-input-31-b68e640f6b83>:55: RuntimeWarning: divide by zero encountered in log\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25954)\u001b[0m 2022-08-02 12:43:04,361\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-08-02 12:43:04,744\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25954)\u001b[0m 2022-08-02 12:43:04,661\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25954)\u001b[0m 2022-08-02 12:43:04,695\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: /Supervisor/checkpoint_000639/checkpoint-639\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25954)\u001b[0m 2022-08-02 12:43:04,696\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 639, '_timesteps_total': None, '_time_total': 2267.168159008026, '_episodes_total': 6640}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25954)\u001b[0m <ipython-input-31-b68e640f6b83>:55: RuntimeWarning: divide by zero encountered in log\n",
      "2022-08-02 12:43:04,786\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: /Supervisor/checkpoint_000639/checkpoint-639\n",
      "2022-08-02 12:43:04,787\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 639, '_timesteps_total': None, '_time_total': 2267.168159008026, '_episodes_total': 6640}\n",
      "<ipython-input-31-b68e640f6b83>:55: RuntimeWarning: divide by zero encountered in log\n",
      "  inf_mask = tf.cast(tf.maximum(np.log(action_mask), tf.float32.min), dtype=tf.float32)\n",
      "2022-08-02 12:43:04,929\tINFO trainable.py:159 -- Trainable.setup took 12.074 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-08-02 12:43:04,930\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25936)\u001b[0m /usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:179: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25936)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "2022-08-02 12:43:14,151\tWARNING deprecation.py:46 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n",
      "2022-08-02 12:43:14,152\tWARNING deprecation.py:46 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1 \tr_mean: -44.3 \tr_max: -11.8 \tr_min: -234.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2 \tr_mean: -38.1 \tr_max: -11.8 \tr_min: -234.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3 \tr_mean: -41.3 \tr_max: -12.8 \tr_min: -234.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   4 \tr_mean: -43.4 \tr_max: -12.8 \tr_min: -154.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5 \tr_mean: -42.2 \tr_max: -13.8 \tr_min: -175.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from CybORG import CybORG\n",
    "from CybORG.Agents import B_lineAgent, SleepAgent, GreenAgent\n",
    "from CybORG.Agents.SimpleAgents.BaseAgent import BaseAgent\n",
    "from CybORG.Agents.SimpleAgents.BlueReactAgent import BlueReactRemoveAgent\n",
    "from CybORG.Agents.SimpleAgents.Meander import RedMeanderAgent\n",
    "from CybORG.Agents.Wrappers.EnumActionWrapper import EnumActionWrapper\n",
    "from CybORG.Agents.Wrappers.FixedFlatWrapper import FixedFlatWrapper\n",
    "from CybORG.Agents.Wrappers.OpenAIGymWrapper import OpenAIGymWrapper\n",
    "from CybORG.Agents.Wrappers.ReduceActionSpaceWrapper import ReduceActionSpaceWrapper\n",
    "from CybORG.Agents.Wrappers import ChallengeWrapper\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.agents import ppo\n",
    "import inspect\n",
    "from CybORG.Agents.Wrappers.rllib_wrapper import RLlibWrapper\n",
    "\n",
    "def env_creator(env_config: dict):\n",
    "    path = str(inspect.getfile(CybORG))\n",
    "    path = path[:-10] + '/Shared/Scenarios/Scenario2.yaml'\n",
    "    agents = {\"Red\": B_lineAgent, \"Green\": GreenAgent}\n",
    "    cyborg = CybORG(scenario_file=path, environment='sim', agents=agents)\n",
    "    env = RLlibWrapper(env=cyborg, agent_name=\"Blue\", max_steps=100)\n",
    "    return env\n",
    "\n",
    "def print_results(results_dict):\n",
    "    train_iter = results_dict[\"training_iteration\"]\n",
    "    r_mean = results_dict[\"episode_reward_mean\"]\n",
    "    r_max = results_dict[\"episode_reward_max\"]\n",
    "    r_min = results_dict[\"episode_reward_min\"]\n",
    "    print(f\"{train_iter:4d} \\tr_mean: {r_mean:.1f} \\tr_max: {r_max:.1f} \\tr_min: {r_min: .1f}\")\n",
    "\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "\n",
    "batch_size = 4000\n",
    "# Set up CybORG\n",
    "register_env(name=\"CybORG\", env_creator=env_creator)\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "\n",
    "ModelCatalog.register_custom_model(\"sv_model\", SupervisorModel)\n",
    "\n",
    "from shutil import make_archive\n",
    "allrewards = []\n",
    "\n",
    "config.update({\"num_gpus\": 1,\"num_workers\": 20,\n",
    "                # Also, use \"framework: tf2\" for tfe eager execution.\n",
    "                \"framework\": \"tf2\",\n",
    "                \"train_batch_size\": batch_size,\n",
    "                \"horizon\": 100,\n",
    "                \"gamma\": 0.9,\n",
    "                \"model\": {\n",
    "                    \"custom_model\": \"sv_model\",\n",
    "                    \"fcnet_hiddens\": [512, 512],\n",
    "                    \"fcnet_activation\": \"relu\",\n",
    "                },\n",
    "\n",
    "                }) \n",
    "trainer = SVTrainer(config=config, env=\"CybORG\")\n",
    "\n",
    "reward = []\n",
    "novel_obs = []\n",
    "novel_actions = []\n",
    "for i in range(200):\n",
    "    results_dict = trainer.train()\n",
    "    print_results(results_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
