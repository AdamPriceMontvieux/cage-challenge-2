{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Box\n",
    "import numpy as np\n",
    "\n",
    "from ray.rllib.models.modelv2 import ModelV2\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models.tf.fcnet import FullyConnectedNetwork\n",
    "from ray.rllib.models.torch.misc import SlimFC\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "from ray.rllib.agents.ppo.ppo import PPOTrainer\n",
    "from ray.rllib.agents.ppo import ppo\n",
    "from ray.rllib.models.tf.tf_action_dist import Categorical\n",
    "from ray.rllib.models.tf.misc import normc_initializer\n",
    "from ray.rllib.models.tf.recurrent_net import RecurrentNetwork\n",
    "import gym\n",
    "\n",
    "tf1, tf, tfv = try_import_tf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option Critic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class HieracrchyModel(TFModelV2):\n",
    "    \n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        super(HieracrchyModel, self).__init__(\n",
    "            obs_space, action_space, num_outputs, model_config, name\n",
    "        )\n",
    "        self.inputs = tf.keras.layers.Input(shape=obs_space.shape, name=\"observations\")\n",
    "        layer_1 = tf.keras.layers.Dense(\n",
    "            256,\n",
    "            name=\"layer1\",\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=normc_initializer(1.0),\n",
    "        )(self.inputs)\n",
    "        layer_2 = tf.keras.layers.Dense(\n",
    "            128,\n",
    "            name=\"layer2\",\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=normc_initializer(1.0),\n",
    "        )(layer_1)\n",
    "        layer_out = tf.keras.layers.Dense(\n",
    "            num_outputs,\n",
    "            name=\"out\",\n",
    "            activation=None,\n",
    "            kernel_initializer=normc_initializer(0.01),\n",
    "        )(layer_1)\n",
    "        value_out = tf.keras.layers.Dense(\n",
    "            1,\n",
    "            name=\"value_out\",\n",
    "            activation=None,\n",
    "            kernel_initializer=normc_initializer(0.01),\n",
    "        )(layer_1)\n",
    "        self.model = tf.keras.Model(self.inputs, [layer_out, value_out])\n",
    "        \n",
    "        ppo_config = ppo.DEFAULT_CONFIG.copy()\n",
    "        ppo_config.update({\"num_gpus\": 0,\"num_workers\": 0,\n",
    "                # Also, use \"framework: tf2\" for tfe eager execution.\n",
    "                \"framework\": \"tf2\",\n",
    "                \"train_batch_size\": 200,\n",
    "                \"horizon\": 100,\n",
    "                \"gamma\": 0.95,\n",
    "                \"model\": {\n",
    "                    \"fcnet_hiddens\": [512, 512],\n",
    "                    \"fcnet_activation\": \"relu\",\n",
    "                }})\n",
    "        \n",
    "        b_line = PPOTrainer(config=ppo_config,env=\"CybORG\")\n",
    "        b_line.restore(\"b_line_agent/checkpoint_000109/checkpoint-109\")\n",
    "        meander = PPOTrainer(config=ppo_config,env=\"CybORG\")\n",
    "        meander.restore(\"supervisor_ppo/checkpoint_000183/checkpoint-183\")\n",
    "        self.sub_agents = [b_line.get_policy().model, meander.get_policy().model]\n",
    "        \n",
    "        self.action = np.array([0])\n",
    "        self.action_logp = np.array([0.])\n",
    "        self.logits = np.array([[0.,0.]])\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        self.logits, self.value = self.model({'observations': input_dict[SampleBatch.CUR_OBS]})\n",
    "        self.action = tf.random.categorical(self.logits, 1, dtype=tf.int32)\n",
    "        self.action = tf.reshape(self.action, self.action.shape[0])\n",
    "        self.action_logp = -tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=self.action, logits=self.logits)\n",
    "        return self.logits, state\n",
    "    \n",
    "    @override(ModelV2)\n",
    "    def value_function(self):\n",
    "        return tf.reshape(self.value, [-1])\n",
    "    \n",
    "def build_model(policy, obs_space, action_space, config) -> ModelV2:\n",
    "    return ModelCatalog.get_model_v2(obs_space,\n",
    "            2,\n",
    "            2,\n",
    "            config[\"model\"],\n",
    "            name=\"h_model\",\n",
    "            framework=\"tf2\",\n",
    "            model_interface=HieracrchyModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from https://github.com/ray-project/ray/blob/master/rllib/examples/models/rnn_model.py\n",
    "class HieracrchyRNNModel(RecurrentNetwork):\n",
    "    \"\"\"Example of using the Keras functional API to define a RNN model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_space,\n",
    "        action_space,\n",
    "        num_outputs,\n",
    "        model_config,\n",
    "        name,\n",
    "        hiddens_size=256,\n",
    "        cell_size=64,\n",
    "    ):\n",
    "        super(HieracrchyRNNModel, self).__init__(\n",
    "            obs_space, action_space, num_outputs, model_config, name\n",
    "        )\n",
    "        self.cell_size = cell_size\n",
    "\n",
    "        # Define input layers\n",
    "        input_layer = tf.keras.layers.Input(\n",
    "            shape=(None, obs_space.shape[0]), name=\"inputs\"\n",
    "        )\n",
    "        state_in_h = tf.keras.layers.Input(shape=(cell_size,), name=\"h\")\n",
    "        state_in_c = tf.keras.layers.Input(shape=(cell_size,), name=\"c\")\n",
    "        seq_in = tf.keras.layers.Input(shape=(), name=\"seq_in\", dtype=tf.int32)\n",
    "\n",
    "        # Preprocess observation with a hidden layer and send to LSTM cell\n",
    "        dense1 = tf.keras.layers.Dense(\n",
    "            hiddens_size, activation=tf.nn.relu, name=\"dense1\"\n",
    "        )(input_layer)\n",
    "        lstm_out, state_h, state_c = tf.keras.layers.LSTM(\n",
    "            cell_size, return_sequences=True, return_state=True, name=\"lstm\"\n",
    "        )(\n",
    "            inputs=dense1,\n",
    "            mask=tf.sequence_mask(seq_in),\n",
    "            initial_state=[state_in_h, state_in_c],\n",
    "        )\n",
    "\n",
    "        # Postprocess LSTM output with another hidden layer and compute values\n",
    "        logits = tf.keras.layers.Dense(\n",
    "            self.num_outputs, activation=tf.keras.activations.linear, name=\"logits\"\n",
    "        )(lstm_out)\n",
    "        values = tf.keras.layers.Dense(1, activation=None, name=\"values\")(lstm_out)\n",
    "\n",
    "        # Create the RNN model\n",
    "        self.rnn_model = tf.keras.Model(\n",
    "            inputs=[input_layer, seq_in, state_in_h, state_in_c],\n",
    "            outputs=[logits, values, state_h, state_c],\n",
    "        )        \n",
    "        \n",
    "        ppo_config = ppo.DEFAULT_CONFIG.copy()\n",
    "        ppo_config.update({\"num_gpus\": 0,\"num_workers\": 0,\n",
    "            \"framework\": \"tf2\",\n",
    "            \"model\": {\n",
    "                \"fcnet_hiddens\": [512, 512],\n",
    "                \"fcnet_activation\": \"relu\",\n",
    "            }})\n",
    "        \n",
    "        b_line = PPOTrainer(config=ppo_config,env=\"CybORG\")\n",
    "        b_line.restore(\"b_line_agent/checkpoint_000109/checkpoint-109\")\n",
    "        meander = PPOTrainer(config=ppo_config,env=\"CybORG\")\n",
    "        meander.restore(\"supervisor_ppo/checkpoint_000183/checkpoint-183\")\n",
    "        self.sub_agents = [b_line.get_policy().model, meander.get_policy().model]\n",
    "        \n",
    "        self.action = np.array([0])\n",
    "        self.action_logp = np.array([0.])\n",
    "        self.logits = np.array([[0.,0.]])\n",
    "        \n",
    "        \n",
    "    @override(RecurrentNetwork)\n",
    "    def forward_rnn(self, inputs, state, seq_lens):\n",
    "        self.logits, self._value_out, h, c = self.rnn_model([inputs, seq_lens] + state)\n",
    "        #self.logits = tf.squeeze(self.logits)\n",
    "        #print(self.logits)\n",
    "        #self.action = tf.squeeze(tf.random.categorical(self.logits, 1, dtype=tf.int32))\n",
    "        ##self.action = tf.reshape(self.action, self.action.shape[0])\n",
    "\n",
    "        #self.action_logp = -tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        #    labels=self.action, logits=self.logits)\n",
    "        \n",
    "        #dist = Categorical(tf.squeeze(self.logits))\n",
    "        #self.action = dist.sample()\n",
    "       # print(self.action)\n",
    "       # self.action_logp = dist.sampled_action_logp()\n",
    "      #  print(self.action_logp)\n",
    "        return self.logits, [h, c]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def get_initial_state(self):\n",
    "        return [\n",
    "            np.zeros(self.cell_size, np.float32),\n",
    "            np.zeros(self.cell_size, np.float32),\n",
    "        ]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def value_function(self):\n",
    "        return tf.reshape(self._value_out, [-1])\n",
    "    \n",
    "def build_model(policy, obs_space, action_space, config) -> ModelV2:\n",
    "    return ModelCatalog.get_model_v2(obs_space,\n",
    "            2,\n",
    "            2,\n",
    "            config[\"model\"],\n",
    "            name=\"h_rnn_model\",\n",
    "            framework=\"tf2\",\n",
    "            model_interface=HieracrchyRNNModel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.ppo.ppo_tf_policy import PPOTFPolicy\n",
    "from ray.rllib.agents.ppo.ppo_tf_policy import ppo_surrogate_loss\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.agents.ppo.ppo import PPOTrainer\n",
    "from ray.rllib.utils.exploration.stochastic_sampling import StochasticSampling\n",
    "from gym.spaces.discrete import Discrete\n",
    "from ray.rllib.models.tf.tf_action_dist import Categorical\n",
    "\n",
    "def after_action(policy):\n",
    "    return {\n",
    "        SampleBatch.ACTIONS: policy.model.action,\n",
    "        SampleBatch.ACTION_LOGP: policy.model.action_logp,\n",
    "        SampleBatch.ACTION_DIST_INPUTS: policy.model.logits,\n",
    "        SampleBatch.VF_PREDS: policy.model.value_function(),\n",
    "    } \n",
    "\n",
    "def loss(self, model, dist_class, train_batch):\n",
    "    return ppo_surrogate_loss(self, model, Categorical, train_batch)\n",
    "\n",
    "def action_sampler_fn(self, model, obs_batch, explore, seq_lens, state_batches, \n",
    "                      prev_action_batch, prev_reward_batch, is_training):\n",
    "    self.model.forward({'obs_flat': obs_batch}, state_batches, seq_lens)\n",
    "    #For tf2\n",
    "    logits = model.sub_agents[model.action[0]].forward({'obs_flat': obs}, None, None)[0]\n",
    "    #Annoying bodge for tf1, will need to get smart, if more than than 2 options\n",
    "    #obs_copy = tf.identity(obs_batch)\n",
    "    #logits = tf.cond(tf.equal(model.action[0], tf.constant(0)),\n",
    "    #                 lambda: model.sub_agents[0].forward({'obs_flat': obs_copy}, None, None)[0],\n",
    "    #                 lambda: model.sub_agents[1].forward({'obs_flat': obs}, None, None)[0])\n",
    "    \n",
    "    #return tf.random.categorical(logits, 1, dtype=tf.int32)[0], None\n",
    "    return tf.math.argmax(logits, axis=1), None\n",
    "\n",
    "def action_distribution_fn(self, model, input_dict, state_batches, seq_lens, explore, timestep, is_training=False):\n",
    "    self.model.forward({'obs_flat': input_dict['obs']}, state_batches, seq_lens)\n",
    "    logits = model.sub_agents[model.action[0]].forward({'obs_flat': input_dict['obs']}, None, None)[0]\n",
    "    return logits, Categorical, state_batches\n",
    "    \n",
    "HierarchyPolicy = PPOTFPolicy.with_updates(\n",
    "    name=\"HierarchyPPOPolicy\",\n",
    "    loss_fn=loss,\n",
    "    #action_sampler_fn=action_sampler_fn,\n",
    "    #action_distribution_fn=action_distribution_fn,\n",
    "    make_model=build_model,\n",
    "    extra_action_out_fn=after_action)\n",
    "\n",
    "class HieracrchyTrainer(PPOTrainer):\n",
    "    def get_default_policy_class(self, config):\n",
    "        return HierarchyPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Option Critic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import time\n",
    "from statistics import mean, stdev\n",
    "from CybORG import CybORG\n",
    "from CybORG.Agents import B_lineAgent, SleepAgent, GreenAgent\n",
    "from CybORG.Agents.SimpleAgents.BaseAgent import BaseAgent\n",
    "from CybORG.Agents.SimpleAgents.BlueReactAgent import BlueReactRemoveAgent\n",
    "from CybORG.Agents.SimpleAgents.Meander import RedMeanderAgent\n",
    "from CybORG.Agents.Wrappers.EnumActionWrapper import EnumActionWrapper\n",
    "from CybORG.Agents.Wrappers.FixedFlatWrapper import FixedFlatWrapper\n",
    "from CybORG.Agents.Wrappers.OpenAIGymWrapper import OpenAIGymWrapper\n",
    "from CybORG.Agents.Wrappers.ReduceActionSpaceWrapper import ReduceActionSpaceWrapper\n",
    "from CybORG.Agents.Wrappers import ChallengeWrapper\n",
    "import os\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.tune.registry import register_env\n",
    "from CybORG.Agents.Wrappers.rllib_wrapper import RLlibWrapper\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPS = 50\n",
    "agent_name = 'Blue'\n",
    "\n",
    "def wrap(env):\n",
    "    return RLlibWrapper(agent_name=\"Blue\", env=env)\n",
    "\n",
    "\n",
    "def evaluate(steps):\n",
    "    path = str(inspect.getfile(CybORG))\n",
    "    path = path[:-10] + '/Shared/Scenarios/Scenario2.yaml'\n",
    "\n",
    "    #print(f'using CybORG v{cyborg_version}, {scenario}\\n')\n",
    "    for num_steps in steps:\n",
    "        #for red_agent in [B_lineAgent, RedMeanderAgent, SleepAgent]:\n",
    "        rs = []\n",
    "        for red_agent in [RedMeanderAgent, B_lineAgent]:\n",
    "\n",
    "            cyborg = CybORG(path, 'sim', agents={'Red': red_agent})\n",
    "            wrapped_cyborg = wrap(cyborg)\n",
    "\n",
    "            observation = wrapped_cyborg.reset()\n",
    "            # observation = cyborg.reset().observation\n",
    "\n",
    "            action_space = wrapped_cyborg.get_action_space(agent_name)\n",
    "            # action_space = cyborg.get_action_space(agent_name)\n",
    "            total_reward = []\n",
    "            actions = []\n",
    "            for i in range(MAX_EPS):\n",
    "                r = []\n",
    "                a = []\n",
    "                # cyborg.env.env.tracker.render()\n",
    "                for j in range(num_steps):\n",
    "                    action = agent.compute_single_action(observation)\n",
    "                    #action = agent.get_action(observation, action_space)\n",
    "                    observation, rew, done, info = wrapped_cyborg.step(action)\n",
    "                    # result = cyborg.step(agent_name, action)\n",
    "                    r.append(rew)\n",
    "                    # r.append(result.reward)\n",
    "                    a.append((str(cyborg.get_last_action('Blue')), str(cyborg.get_last_action('Red'))))\n",
    "                total_reward.append(sum(r))\n",
    "                actions.append(a)\n",
    "                # observation = cyborg.reset().observation\n",
    "                observation = wrapped_cyborg.reset()\n",
    "            rs.append(mean(total_reward))\n",
    "            print(f'Average reward for red agent {red_agent.__name__} at steps {num_steps} is: {mean(total_reward):.1f} with a standard deviation of {stdev(total_reward):.1f}')\n",
    "    return rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-08 14:15:24,514\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-08-08 14:15:26,095\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-08-08 14:15:26,846\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "2022-08-08 14:15:26,885\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: b_line_agent/checkpoint_000109/checkpoint-109\n",
      "2022-08-08 14:15:26,886\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 109, '_timesteps_total': None, '_time_total': 4080.5770568847656, '_episodes_total': 4360}\n",
      "2022-08-08 14:15:27,275\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-08-08 14:15:27,475\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "2022-08-08 14:15:27,511\tINFO trainable.py:588 -- Restored on 172.28.0.2 from checkpoint: supervisor_ppo/checkpoint_000183/checkpoint-183\n",
      "2022-08-08 14:15:27,512\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 183, '_timesteps_total': None, '_time_total': 6626.44996380806, '_episodes_total': 7320}\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "logits should be a matrix, got shape [32,1,2] [Op:Multinomial]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [189]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m ModelCatalog\u001b[38;5;241m.\u001b[39mregister_custom_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh_rnn_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, HieracrchyRNNModel)\n\u001b[1;32m     58\u001b[0m config\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_gpus\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_workers\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     59\u001b[0m                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultienv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     60\u001b[0m                 \u001b[38;5;66;03m# Also, use \"framework: tf2\" for tfe eager execution.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m \n\u001b[1;32m     73\u001b[0m                 }) \n\u001b[0;32m---> 74\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mHieracrchyTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m):\n\u001b[1;32m     77\u001b[0m     results_dict \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/agents/trainer.py:870\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, config, env, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;66;03m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;66;03m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;66;03m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;66;03m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward_max\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    867\u001b[0m     }\n\u001b[1;32m    868\u001b[0m }\n\u001b[0;32m--> 870\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremote_checkpoint_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msync_function_tpl\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/tune/trainable.py:156\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[1;32m    154\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_ip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_current_ip()\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/agents/trainer.py:950\u001b[0m, in \u001b[0;36mTrainer.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;66;03m# New design: Override `Trainable.setup()` (as indented by Trainable)\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;66;03m# and do or don't call super().setup() from within your override.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# By default, `super().setup()` will create both worker sets:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;66;03m# parallel to training.\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;66;03m# TODO: Deprecate `_init()` and remove this try/except block.\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;66;03m# Only if user did not override `_init()`:\u001b[39;00m\n\u001b[1;32m    945\u001b[0m     \u001b[38;5;66;03m# - Create rollout workers here automatically.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;66;03m# This matches the behavior of using `build_trainer()`, which\u001b[39;00m\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;66;03m# has been deprecated.\u001b[39;00m\n\u001b[0;32m--> 950\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m=\u001b[39m \u001b[43mWorkerSet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_workers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;66;03m# By default, collect metrics for all remote workers.\u001b[39;00m\n\u001b[1;32m    960\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remote_workers_for_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\u001b[38;5;241m.\u001b[39mremote_workers()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/worker_set.py:170\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, policy_class, trainer_config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m    167\u001b[0m     spaces \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_worker:\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_worker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRolloutWorker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_policy_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworker_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_local_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/worker_set.py:630\u001b[0m, in \u001b[0;36mWorkerSet._make_worker\u001b[0;34m(self, cls, env_creator, validate_env, policy_cls, worker_index, num_workers, recreated_worker, config, spaces)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     extra_python_environs \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_python_environs_for_worker\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 630\u001b[0m worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy_mapping_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmultiagent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpolicy_mapping_fn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicies_to_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmultiagent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpolicies_to_train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtf_session_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msession_creator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtf_session_args\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrollout_fragment_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrollout_fragment_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcount_steps_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmultiagent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcount_steps_by\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_mode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisode_horizon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhorizon\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocessor_pref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreprocessor_pref\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_async\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msample_async\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompress_observations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompress_observations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_envs_per_worker\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobservation_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmultiagent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobservation_fn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobservation_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobservation_filter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_rewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclip_rewards\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalize_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnormalize_actions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclip_actions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menv_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworker_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrecreated_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecreated_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrecord_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecord_env\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_logdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlog_level\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_evaluation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_evaluation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_worker_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mremote_worker_envs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_env_batch_wait_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mremote_env_batch_wait_ms\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43msoft_horizon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msoft_horizon\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_done_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mno_done_at_end\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mworker_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfake_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfake_sampler\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_python_environs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_python_environs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_env_checking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdisable_env_checking\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m worker\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py:630\u001b[0m, in \u001b[0;36mRolloutWorker.__init__\u001b[0;34m(self, env_creator, validate_env, policy_spec, policy_mapping_fn, policies_to_train, tf_session_creator, rollout_fragment_length, count_steps_by, batch_mode, episode_horizon, preprocessor_pref, sample_async, compress_observations, num_envs, observation_fn, observation_filter, clip_rewards, normalize_actions, clip_actions, env_config, model_config, policy_config, worker_index, num_workers, recreated_worker, record_env, log_dir, log_level, callbacks, input_creator, input_evaluation, output_creator, remote_worker_envs, remote_env_batch_wait_ms, soft_horizon, no_done_at_end, seed, extra_python_environs, fake_sampler, spaces, policy, monitor_path, disable_env_checking)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    618\u001b[0m     ray\u001b[38;5;241m.\u001b[39mis_initialized()\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ray\u001b[38;5;241m.\u001b[39mworker\u001b[38;5;241m.\u001b[39m_mode() \u001b[38;5;241m==\u001b[39m ray\u001b[38;5;241m.\u001b[39mworker\u001b[38;5;241m.\u001b[39mLOCAL_MODE\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m num_gpus \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m policy_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_fake_gpus\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    622\u001b[0m ):\n\u001b[1;32m    623\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    624\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are running ray with `local_mode=True`, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    625\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigured \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_gpus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GPUs to be used! In local mode, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    626\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPolicies are placed on the CPU and the `num_gpus` setting \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    627\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    628\u001b[0m     )\n\u001b[0;32m--> 630\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_policy_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf_session_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;66;03m# Update Policy's view requirements from Model, only if Policy directly\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;66;03m# inherited from base `Policy` class. At this point here, the Policy\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;66;03m# must have it's Model (if any) defined and ready to output an initial\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;66;03m# state.\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pol \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_map\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py:1788\u001b[0m, in \u001b[0;36mRolloutWorker._build_policy_map\u001b[0;34m(self, policy_dict, policy_config, session_creator, seed)\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessors[name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m     \u001b[38;5;66;03m# Create the actual policy object.\u001b[39;00m\n\u001b[0;32m-> 1788\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_map\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_policy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1789\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_cls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerged_conf\u001b[49m\n\u001b[1;32m   1790\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworker_index \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1793\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilt policy map: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_map\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/policy_map.py:146\u001b[0m, in \u001b[0;36mPolicyMap.create_policy\u001b[0;34m(self, policy_id, policy_cls, observation_space, action_space, config_override, merged_config)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# For tf-eager: no graph, no session.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m tf1\u001b[38;5;241m.\u001b[39mvariable_scope(var_scope):\n\u001b[0;32m--> 146\u001b[0m             \u001b[38;5;28mself\u001b[39m[policy_id] \u001b[38;5;241m=\u001b[39m \u001b[43mclass_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m                \u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerged_config\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Non-tf: No graph, no session.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     class_ \u001b[38;5;241m=\u001b[39m policy_cls\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/eager_tf_policy.py:448\u001b[0m, in \u001b[0;36mbuild_eager_tf_policy.<locals>.eager_policy_cls.__init__\u001b[0;34m(self, observation_space, action_space, config)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# Backward compatibility: A user's policy may only support a single\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# loss term and optimizer (no lists).\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer: LocalOptimizer \u001b[38;5;241m=\u001b[39m optimizers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m optimizers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 448\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_loss_from_dummy_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_remove_unneeded_view_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstats_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss_initialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m after_init:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/policy/policy.py:956\u001b[0m, in \u001b[0;36mPolicy._initialize_loss_from_dummy_batch\u001b[0;34m(self, auto_remove_unneeded_view_reqs, stats_fn)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;66;03m# Call the loss function, if it exists.\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 956\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdist_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;66;03m# Call the stats fn, if given.\u001b[39;00m\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stats_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Input \u001b[0;32mIn [186]\u001b[0m, in \u001b[0;36mloss\u001b[0;34m(self, model, dist_class, train_batch)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, dist_class, train_batch):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mppo_surrogate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCategorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/agents/ppo/ppo_tf_policy.py:89\u001b[0m, in \u001b[0;36mppo_surrogate_loss\u001b[0;34m(policy, model, dist_class, train_batch)\u001b[0m\n\u001b[1;32m     86\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     reduce_mean_valid \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean\n\u001b[0;32m---> 89\u001b[0m prev_action_dist \u001b[38;5;241m=\u001b[39m \u001b[43mdist_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mSampleBatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mACTION_DIST_INPUTS\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m logp_ratio \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexp(\n\u001b[1;32m     92\u001b[0m     curr_action_dist\u001b[38;5;241m.\u001b[39mlogp(train_batch[SampleBatch\u001b[38;5;241m.\u001b[39mACTIONS])\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;241m-\u001b[39m train_batch[SampleBatch\u001b[38;5;241m.\u001b[39mACTION_LOGP]\n\u001b[1;32m     94\u001b[0m )\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Only calculate kl loss if necessary (kl-coeff > 0.0).\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/models/tf/tf_action_dist.py:58\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, inputs, model, temperature)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m temperature \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCategorical `temperature` must be > 0.0!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Allow softmax formula w/ temperature != 1.0:\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Divide inputs by temperature.\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/models/tf/tf_action_dist.py:27\u001b[0m, in \u001b[0;36mTFActionDistribution.__init__\u001b[0;34m(self, inputs, model)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@override\u001b[39m(ActionDistribution)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: List[TensorType], model: ModelV2):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(inputs, model)\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_op \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_sample_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampled_action_logp_op \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_op)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/models/tf/tf_action_dist.py:91\u001b[0m, in \u001b[0;36mCategorical._build_sample_op\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;129m@override\u001b[39m(TFActionDistribution)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_sample_op\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TensorType:\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39msqueeze(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:7186\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7185\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7186\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: logits should be a matrix, got shape [32,1,2] [Op:Multinomial]"
     ]
    }
   ],
   "source": [
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "def print_results(results_dict):\n",
    "    train_iter = results_dict[\"training_iteration\"]\n",
    "    r_mean = results_dict[\"episode_reward_mean\"]\n",
    "    r_max = results_dict[\"episode_reward_max\"]\n",
    "    r_min = results_dict[\"episode_reward_min\"]\n",
    "    print(f\"{train_iter:4d} \\tr_mean: {r_mean:.1f} \\tr_max: {r_max:.1f} \\tr_min: {r_min: .1f}\")\n",
    "    \n",
    "\n",
    "class MultiEnv(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        # pick actual env based on worker and env indexes\n",
    "        self.env = self.choose_env_for(env_config.worker_index)\n",
    "        self.action_space = self.env.action_space\n",
    "        self.observation_space = self.env.observation_space\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)   \n",
    "    def choose_env_for(self, index):\n",
    "        if index > 20:\n",
    "            path = str(inspect.getfile(CybORG))\n",
    "            path = path[:-10] + '/Shared/Scenarios/Scenario2.yaml'\n",
    "            agents = {\"Red\": SleepAgent, \"Green\": GreenAgent}\n",
    "            cyborg = CybORG(scenario_file=path, environment='sim', agents=agents)\n",
    "            return RLlibWrapper(env=cyborg, agent_name=\"Blue\", max_steps=100)\n",
    "        elif index % 2 == 0:\n",
    "            path = str(inspect.getfile(CybORG))\n",
    "            path = path[:-10] + '/Shared/Scenarios/Scenario2.yaml'\n",
    "            agents = {\"Red\": B_lineAgent, \"Green\": GreenAgent}\n",
    "            cyborg = CybORG(scenario_file=path, environment='sim', agents=agents)\n",
    "            return RLlibWrapper(env=cyborg, agent_name=\"Blue\", max_steps=100)\n",
    "        else:\n",
    "            path = str(inspect.getfile(CybORG))\n",
    "            path = path[:-10] + '/Shared/Scenarios/Scenario2.yaml'\n",
    "            agents = {\"Red\": RedMeanderAgent, \"Green\": GreenAgent}\n",
    "            cyborg = CybORG(scenario_file=path, environment='sim', agents=agents)\n",
    "            return RLlibWrapper(env=cyborg, agent_name=\"Blue\", max_steps=100)\n",
    "        \n",
    "def env_creator(env_config: dict):\n",
    "    path = str(inspect.getfile(CybORG))\n",
    "    path = path[:-10] + '/Shared/Scenarios/Scenario2.yaml'\n",
    "    agents = {\"Red\": B_lineAgent, \"Green\": GreenAgent}\n",
    "    cyborg = CybORG(scenario_file=path, environment='sim', agents=agents)\n",
    "    env = RLlibWrapper(env=cyborg, agent_name=\"Blue\", max_steps=100)\n",
    "    return env\n",
    "\n",
    "register_env(\"CybORG\", env_creator=env_creator)\n",
    "register_env(\"multienv\", lambda config: MultiEnv(config))\n",
    "\n",
    "batch_size = 4000\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "\n",
    "ModelCatalog.register_custom_model(\"h_rnn_model\", HieracrchyRNNModel)\n",
    "\n",
    "config.update({\"num_gpus\": 1, \"num_workers\": 0,\n",
    "               \"env\": \"multienv\",\n",
    "                # Also, use \"framework: tf2\" for tfe eager execution.\n",
    "                \"framework\": \"tf2\",\n",
    "                \"train_batch_size\": batch_size,\n",
    "                \"horizon\": 100,\n",
    "                \"sgd_minibatch_size\": 100,\n",
    "                \"use_critic\": False,\n",
    "                \"use_gae\": False,\n",
    "                \"gamma\": 0.8,\n",
    "                \"batch_mode\":\"complete_episodes\",\n",
    "                \"model\": {\n",
    "                    \"custom_model\": \"h_rnn_model\",\n",
    "                },\n",
    "\n",
    "                }) \n",
    "agent = HieracrchyTrainer(config=config)\n",
    "\n",
    "for i in range(200):\n",
    "    results_dict = agent.train()\n",
    "    print_results(results_dict)\n",
    "    #evaluate([100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.DEFAULT_CONFIG.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
