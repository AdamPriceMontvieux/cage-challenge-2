{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import time\n",
    "from statistics import mean, stdev\n",
    "from CybORG import CybORG\n",
    "from CybORG.Agents import B_lineAgent, SleepAgent, GreenAgent\n",
    "from CybORG.Agents.SimpleAgents.BaseAgent import BaseAgent\n",
    "from CybORG.Agents.SimpleAgents.BlueReactAgent import BlueReactRemoveAgent\n",
    "from CybORG.Agents.SimpleAgents.Meander import RedMeanderAgent\n",
    "from CybORG.Agents.Wrappers.FixedFlatWrapper import FixedFlatWrapper\n",
    "from CybORG.Agents.Wrappers.OpenAIGymWrapper import OpenAIGymWrapper\n",
    "from CybORG.Agents.Wrappers import ChallengeWrapper\n",
    "import os\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.tune.registry import register_env\n",
    "from CybORG.Agents.Wrappers.rllib_wrapper import RLlibWrapper\n",
    "import warnings\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy\n",
    "from collections import deque\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPS = 20\n",
    "agent_name = 'Blue'\n",
    "\n",
    "def wrap(env):\n",
    "    return RLlibWrapper(agent_name=\"Blue\", env=env)\n",
    "\n",
    "\n",
    "def evaluate(steps):\n",
    "    path = str(inspect.getfile(CybORG))\n",
    "    path = path[:-10] + '/Shared/Scenarios/Scenario2.yaml'\n",
    "\n",
    "    #print(f'using CybORG v{cyborg_version}, {scenario}\\n')\n",
    "    for num_steps in steps:\n",
    "        for red_agent in [B_lineAgent]:\n",
    "\n",
    "            cyborg = CybORG(path, 'sim', agents={'Red': red_agent})\n",
    "            wrapped_cyborg = wrap(cyborg)\n",
    "\n",
    "            observation = wrapped_cyborg.reset()\n",
    "            # observation = cyborg.reset().observation\n",
    "\n",
    "            action_space = wrapped_cyborg.get_action_space(agent_name)\n",
    "            # action_space = cyborg.get_action_space(agent_name)\n",
    "            total_reward = []\n",
    "            actions = []\n",
    "            for i in range(MAX_EPS):\n",
    "                r = []\n",
    "                a = []\n",
    "                # cyborg.env.env.tracker.render()\n",
    "                for j in range(num_steps):\n",
    "                    action, _, _ = trainer.compute_actions(observation)\n",
    "                    observation, rew, done, info = wrapped_cyborg.step(action)\n",
    "                    # result = cyborg.step(agent_name, action)\n",
    "                    r.append(rew)\n",
    "                    # r.append(result.reward)\n",
    "                    a.append((str(cyborg.get_last_action('Blue')), str(cyborg.get_last_action('Red'))))\n",
    "                total_reward.append(sum(r))\n",
    "                actions.append(a)\n",
    "                # observation = cyborg.reset().observation\n",
    "                observation = wrapped_cyborg.reset()\n",
    "            print(f'Average reward for red agent {red_agent.__name__} and steps {num_steps} is: {mean(total_reward):.1f} with a standard deviation of {stdev(total_reward):.1f}')\n",
    "    return mean(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tree  # pip install dm_tree\n",
    "from typing import Optional\n",
    "\n",
    "import ray\n",
    "import ray.experimental.tf_utils\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.filter import get_filter\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "from ray.rllib.utils.spaces.space_utils import get_base_struct_from_space, unbatch\n",
    "from ray.rllib.execution.replay_ops import Replay, StoreToReplayBuffer\n",
    "from decimal import Decimal\n",
    "tf1, tf, tfv = try_import_tf()\n",
    "\n",
    "def rollout(\n",
    "    policy: Policy,\n",
    "    env: gym.Env,\n",
    "    novelty_archive,\n",
    "    timestep_limit: Optional[int] = None,\n",
    "    add_noise: bool = False,\n",
    "    offset: float = 0.0,\n",
    "    novelty_type: str = \"action\"\n",
    "):\n",
    "    \"\"\"Do a rollout.\n",
    "    If add_noise is True, the rollout will take noisy actions with\n",
    "    noise drawn from that stream. Otherwise, no action noise will be added.\n",
    "    Args:\n",
    "        policy: RLlib Policy from which to draw actions.\n",
    "        env: Environment from which to draw rewards, done, and\n",
    "            next state.\n",
    "        timestep_limit: Steps after which to end the rollout.\n",
    "            If None, use `env.spec.max_episode_steps` or 999999.\n",
    "        add_noise: Indicates whether exploratory action noise should be\n",
    "            added.\n",
    "        offset: Value to subtract from the reward (e.g. survival bonus\n",
    "            from humanoid).\n",
    "    \"\"\"\n",
    "    max_timestep_limit = 999999\n",
    "    env_timestep_limit = (\n",
    "        env.spec.max_episode_steps\n",
    "        if (hasattr(env, \"spec\") and hasattr(env.spec, \"max_episode_steps\"))\n",
    "        else max_timestep_limit\n",
    "    )\n",
    "    timestep_limit = (\n",
    "        env_timestep_limit\n",
    "        if timestep_limit is None\n",
    "        else min(timestep_limit, env_timestep_limit)\n",
    "    )\n",
    "    t = 0\n",
    "    cur_obs = env.reset()\n",
    "    novel = []; returns = []\n",
    "    batch = SampleBatchBuilder() \n",
    "    for _ in range(timestep_limit or max_timestep_limit):\n",
    "        action, dist, _ = policy.compute_actions([cur_obs], add_noise=add_noise, update=True)\n",
    "        new_obs, r, done, _ = env.step(action[0])\n",
    "        if novelty_type == 'action':\n",
    "            action_vector = np.zeros(145)\n",
    "            action_vector[action] = 1\n",
    "            novel.append(action_vector); \n",
    "        else:\n",
    "            novel.append(cur_obs); \n",
    "        returns.append(r)          \n",
    "        batch.add_values(\n",
    "                obs=cur_obs,\n",
    "                actions=action[0],\n",
    "                rewards=r,\n",
    "                dones=done,\n",
    "                new_obs=new_obs)      \n",
    "        cur_obs = new_obs\n",
    "       # print(new_obs)\n",
    "        if offset != 0.0: r -= np.abs(offset)\n",
    "        t += 1\n",
    "        if done:\n",
    "            sample = batch.build_and_reset()\n",
    "            returns = np.array(returns)\n",
    "            sample[Postprocessing.ADVANTAGES] = scipy.signal.lfilter([1], [1, float(-0.9)], returns[::-1], axis=0)[::-1]\n",
    "            break\n",
    "        \n",
    "    \n",
    "    returns = np.array(returns, dtype=np.float64)\n",
    "    novel = np.mean(np.array(novel), axis=0)\n",
    "    return returns, t, novel, sample\n",
    "\n",
    "def evaluation_rollout(\n",
    "    policy: Policy,\n",
    "    env: gym.Env,\n",
    "):\n",
    "    max_timestep_limit = 999999\n",
    "    env_timestep_limit = (\n",
    "        env.spec.max_episode_steps\n",
    "        if (hasattr(env, \"spec\") and hasattr(env.spec, \"max_episode_steps\"))\n",
    "        else max_timestep_limit\n",
    "    )\n",
    "    timestep_limit = (\n",
    "        env_timestep_limit\n",
    "        if timestep_limit is None\n",
    "        else min(timestep_limit, env_timestep_limit)\n",
    "    )\n",
    "    t = 0\n",
    "    returns = []\n",
    "    for _ in range(timestep_limit or max_timestep_limit):\n",
    "        action, _, _ = policy.compute_actions([cur_obs], add_noise=add_noise, update=True)\n",
    "        new_obs, r, done, _ = env.step(action[0])\n",
    "        returns.append(r)          \n",
    "        cur_obs = new_obs\n",
    "        if done:\n",
    "            returns = np.array(returns)\n",
    "            break\n",
    "        \n",
    "    returns = np.array(returns, dtype=np.float64)\n",
    "    return returns\n",
    "\n",
    "def make_session(single_threaded):\n",
    "    if not single_threaded:\n",
    "        return tf1.Session()\n",
    "    return tf1.Session(\n",
    "        config=tf1.ConfigProto(\n",
    "            inter_op_parallelism_threads=1, intra_op_parallelism_threads=1\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "class GATFPolicy(Policy):\n",
    "    def __init__(self, obs_space, action_space, config):\n",
    "        super().__init__(obs_space, action_space, config)\n",
    "        self.action_space_struct = get_base_struct_from_space(action_space)\n",
    "        self.action_noise_std = self.config[\"action_noise_std\"]\n",
    "        self.preprocessor = ModelCatalog.get_preprocessor_for_space(obs_space)\n",
    "        self.observation_filter = get_filter(\n",
    "            self.config[\"observation_filter\"], self.preprocessor.shape\n",
    "        )\n",
    "        self.single_threaded = self.config.get(\"single_threaded\", False)\n",
    "        self.config[\"framework\"] = \"tfe\"\n",
    "        if self.config[\"framework\"] == \"tf2\":\n",
    "            self.sess = make_session(single_threaded=self.single_threaded)\n",
    "\n",
    "            # Set graph-level seed.\n",
    "            if config.get(\"seed\") is not None:\n",
    "                with self.sess.as_default():\n",
    "                    tf1.set_random_seed(config[\"seed\"])\n",
    "\n",
    "            self.inputs = tf1.placeholder(\n",
    "                tf.float32, [None] + list(self.preprocessor.shape)\n",
    "            )\n",
    "        else:\n",
    "            if not tf1.executing_eagerly():\n",
    "                tf1.enable_eager_execution()\n",
    "            self.sess = self.inputs = None\n",
    "            if config.get(\"seed\") is not None:\n",
    "                # Tf2.x.\n",
    "                if config.get(\"framework\") == \"tf2\":\n",
    "                    tf.random.set_seed(config[\"seed\"])\n",
    "                # Tf-eager.\n",
    "                elif tf1 and config.get(\"framework\") == \"tfe\":\n",
    "                    tf1.set_random_seed(config[\"seed\"])\n",
    "\n",
    "        # Policy network.\n",
    "        self.dist_class, dist_dim = ModelCatalog.get_action_dist(\n",
    "            self.action_space, self.config[\"model\"], dist_type=\"deterministic\"\n",
    "        )\n",
    "\n",
    "        self.model = ModelCatalog.get_model_v2(\n",
    "            obs_space=self.preprocessor.observation_space,\n",
    "            action_space=action_space,\n",
    "            num_outputs=dist_dim,\n",
    "            model_config=self.config[\"model\"],\n",
    "        )\n",
    "\n",
    "        self.sampler = None\n",
    "        if self.sess:\n",
    "            dist_inputs, _ = self.model({SampleBatch.CUR_OBS: self.inputs})\n",
    "            dist = self.dist_class(dist_inputs, self.model)\n",
    "            self.sampler = dist.sample()\n",
    "            self.variables = ray.experimental.tf_utils.TensorFlowVariables(\n",
    "                dist_inputs, self.sess\n",
    "            )\n",
    "            self.sess.run(tf1.global_variables_initializer())\n",
    "        else:\n",
    "            self.variables = ray.experimental.tf_utils.TensorFlowVariables(\n",
    "                [], None, self.model.variables()\n",
    "            )\n",
    "\n",
    "        self.num_params = sum(\n",
    "            np.prod(variable.shape.as_list())\n",
    "            for _, variable in self.variables.variables.items()\n",
    "        )\n",
    "\n",
    "    @override(Policy)\n",
    "    def compute_actions(self, observation, add_noise=False, update=True, **kwargs):\n",
    "        # Squeeze batch dimension (we always calculate actions for only a\n",
    "        # single obs).\n",
    "        observation = observation[0]\n",
    "        observation = self.preprocessor.transform(observation)\n",
    "        observation = self.observation_filter(observation[None], update=update)\n",
    "        # `actions` is a list of (component) batches.\n",
    "        # Eager mode.\n",
    "        dist_inputs, _ = self.model({SampleBatch.CUR_OBS: observation})\n",
    "        dist = self.dist_class(dist_inputs, self.model)\n",
    "        actions = dist.sample()\n",
    "        actions = tree.map_structure(lambda a: a.numpy(), actions)\n",
    "        return actions, dist_inputs.numpy()[0], {}\n",
    "\n",
    "    def compute_single_action(\n",
    "        self, observation, add_noise=False, update=True, **kwargs\n",
    "    ):\n",
    "        action, state_outs, extra_fetches = self.compute_actions(\n",
    "            [observation], add_noise=add_noise, update=update, **kwargs\n",
    "        )\n",
    "        return action[0], state_outs, extra_fetches\n",
    "\n",
    "    def _add_noise(self, single_action, single_action_space):\n",
    "        if isinstance(\n",
    "            single_action_space, gym.spaces.Box\n",
    "        ) and single_action_space.dtype.name.startswith(\"float\"):\n",
    "            single_action += (\n",
    "                np.random.randn(*single_action.shape) * self.action_noise_std\n",
    "            )\n",
    "        return single_action\n",
    "\n",
    "    def get_state(self):\n",
    "        return {\"state\": self.get_flat_weights()}\n",
    "\n",
    "    def set_state(self, state):\n",
    "        return self.set_flat_weights(state[\"state\"])\n",
    "\n",
    "    def set_flat_weights(self, x):\n",
    "        self.variables.set_flat(x)\n",
    "\n",
    "    def get_flat_weights(self):\n",
    "        return self.variables.get_flat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import namedtuple\n",
    "import logging\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import Trainer\n",
    "from CybORG.Agents.ES.RLLibFiles.trainer_config import TrainerConfig\n",
    "from CybORG.Agents.ES import optimizers, utils\n",
    "from ray.rllib.agents.dqn.dqn_tf_policy import DQNTFPolicy\n",
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "from ray.rllib.env.env_context import EnvContext\n",
    "from ray.rllib.policy.sample_batch import DEFAULT_POLICY_ID\n",
    "from ray.rllib.utils import FilterManager\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.deprecation import Deprecated\n",
    "from ray.rllib.utils.metrics import (\n",
    "    NUM_AGENT_STEPS_SAMPLED,\n",
    "    NUM_AGENT_STEPS_TRAINED,\n",
    "    NUM_ENV_STEPS_SAMPLED,\n",
    "    NUM_ENV_STEPS_TRAINED,\n",
    ")\n",
    "from ray.rllib.utils.torch_utils import set_torch_seed\n",
    "from ray.rllib.utils.typing import TrainerConfigDict\n",
    "from ray.rllib.evaluation.sample_batch_builder import SampleBatchBuilder\n",
    "from ray.rllib.evaluation.postprocessing import Postprocessing\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "Result = namedtuple(\n",
    "    \"Result\",\n",
    "    [\n",
    "        \"noise_indices\",\n",
    "        \"eval_returns\",\n",
    "        \"observations\",\n",
    "        \"novelties\",\n",
    "        \"experience\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "class GAConfig(TrainerConfig):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(trainer_class=GATrainer)\n",
    "\n",
    "        # ES specific settings:\n",
    "        self.action_noise_std = 0.0\n",
    "        self.l2_coeff = 0.005\n",
    "        self.noise_stdev = 0.02\n",
    "        self.noise_decay = 0.995\n",
    "        self.episodes_per_batch = 100\n",
    "        self.reward_coefficient = 0.5\n",
    "        self.store_novelty_probs = 0.03\n",
    "        self.stepsize = 0.01\n",
    "        self.noise_size = 250000000\n",
    "        self.num_workers = 30\n",
    "        self.individuals_per_worker = 1\n",
    "        self.observation_filter = \"MeanStdFilter\"\n",
    "        self.framework = \"tf\"\n",
    "        self.evaluation_config[\"num_envs_per_worker\"] = 1\n",
    "        self.evaluation_config[\"observation_filter\"] = \"NoFilter\"\n",
    "        self.experience_sample_rate = 0.1\n",
    "        self.experience_store_dir = \"tmp\"\n",
    "        self.tourney_size = 12\n",
    "        self.novelty_max_size = 150\n",
    "        self.novelty_k = 25\n",
    "        self.novelty_type = 'action'\n",
    "\n",
    "\n",
    "    @override(TrainerConfig)\n",
    "    def training(\n",
    "        self,\n",
    "        *,\n",
    "        action_noise_std: Optional[float] = None,\n",
    "        noise_stdev: Optional[int] = None,\n",
    "        noise_decay: Optional[float] = None,\n",
    "        episodes_per_batch: Optional[int] = None,\n",
    "        reward_coefficient: Optional[float] = None,\n",
    "        stepsize: Optional[float] = None,\n",
    "        noise_size: Optional[int] = None,\n",
    "        tourney_size: Optional[int] = None,\n",
    "        individuals_per_worker: Optional[int] = None,\n",
    "        store_novelty_probs: Optional[float] = None,\n",
    "        experience_sample_rate: Optional[float] = None,\n",
    "        experience_store_dir: Optional[str] = None,\n",
    "        novelty_max_size: Optional[int] = None,\n",
    "        novelty_k: Optional[int] = None,\n",
    "        novelty_type: Optional[str] = None,\n",
    "        **kwargs,\n",
    "    ) -> \"GAConfig\":\n",
    "        \"\"\"Sets the training related configuration.\n",
    "        Args:\n",
    "            action_noise_std: Std. deviation to be used when adding (standard normal)\n",
    "                noise to computed actions. Action noise is only added, if\n",
    "                `compute_actions` is called with the `add_noise` arg set to True.\n",
    "            l2_coeff: Coefficient to multiply current weights with inside the globalg\n",
    "                optimizer update term.\n",
    "            noise_stdev: Std. deviation of parameter noise.\n",
    "            episodes_per_batch: Minimum number of episodes to pack into the train batch.\n",
    "            store_novelty_probs: Probability of \n",
    "            stepsize: SGD step-size used for the Adam optimizer.\n",
    "            noise_size: Number of rows in the noise table (shared across workers).\n",
    "                Each row contains a gaussian noise value for each model parameter.\n",
    "        Returns:\n",
    "            This updated TrainerConfig object.\n",
    "        \"\"\"\n",
    "        # Pass kwargs onto super's `training()` method.\n",
    "        super().training(**kwargs)\n",
    "\n",
    "        if action_noise_std is not None:\n",
    "            self.action_noise_std = action_noise_std\n",
    "        if noise_stdev is not None:\n",
    "            self.noise_stdev = noise_stdev\n",
    "        if noise_decay is not None:\n",
    "            self.noise_decay = noise_decay\n",
    "        if episodes_per_batch is not None:\n",
    "            self.episodes_per_batch = episodes_per_batch\n",
    "        if reward_coefficient is not None:\n",
    "            self.reward_coefficient = reward_coefficient\n",
    "        if individuals_per_worker is not None:\n",
    "            self.individuals_per_worker = individuals_per_worker\n",
    "        if stepsize is not None:\n",
    "            self.stepsize = stepsize\n",
    "        if noise_size is not None:\n",
    "            self.noise_size = noise_size\n",
    "        if store_novelty_probs is not None:\n",
    "            self.store_novelty_probs = store_novelty_probs\n",
    "        if experience_sample_rate is not None:\n",
    "            self.experience_sample_rate = experience_sample_rate\n",
    "        if experience_store_dir is not None:\n",
    "            self.experience_store_dir = experience_store_dir\n",
    "        if tourney_size is not None:\n",
    "            self.tourney_size = tourney_size\n",
    "        if novelty_max_size is not None:\n",
    "            self.novelty_max_size = novelty_max_size\n",
    "        if novelty_k is not None:\n",
    "            self.novelty_k = novelty_k\n",
    "        if novelty_type is not None:\n",
    "            self.novelty_type = novelty_type\n",
    "        return self\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def create_shared_noise(count):\n",
    "    \"\"\"Create a large array of noise to be shared by all workers.\"\"\"\n",
    "    seed = 123\n",
    "    noise = np.random.RandomState(seed).randn(count).astype(np.float32)\n",
    "    return noise\n",
    "\n",
    "\n",
    "class SharedNoiseTable:\n",
    "    def __init__(self, noise):\n",
    "        self.noise = noise\n",
    "        assert self.noise.dtype == np.float32\n",
    "\n",
    "    def get(self, i, dim):\n",
    "        return self.noise[i : i + dim]\n",
    "\n",
    "    def sample_index(self, dim):\n",
    "        return np.random.randint(0, len(self.noise) - dim + 1)\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class Worker:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        policy_params,\n",
    "        env_creator,\n",
    "        noise,\n",
    "        worker_index,\n",
    "        min_task_runtime=0.2,\n",
    "    ):\n",
    "\n",
    "        # Set Python random, numpy, env, and torch/tf seeds.\n",
    "        seed = config.get(\"seed\")\n",
    "        if seed is not None:\n",
    "            # Python random module.\n",
    "            random.seed(seed)\n",
    "            # Numpy.\n",
    "            np.random.seed(seed)\n",
    "            # Torch.\n",
    "            if config.get(\"framework\") == \"torch\":\n",
    "                set_torch_seed(seed)\n",
    "\n",
    "        self.min_task_runtime = min_task_runtime\n",
    "        self.config = config\n",
    "        self.config.update(policy_params)\n",
    "        self.config[\"single_threaded\"] = False\n",
    "        self.noise = SharedNoiseTable(noise)\n",
    "\n",
    "        env_context = EnvContext(config[\"env_config\"] or {}, worker_index)\n",
    "        self.env = env_creator(env_context)\n",
    "        # Seed the env, if gym.Env.\n",
    "        if not hasattr(self.env, \"seed\"):\n",
    "            logger.info(\"Env doesn't support env.seed(): {}\".format(self.env))\n",
    "        else:\n",
    "            self.env.seed(seed)\n",
    "\n",
    "        from ray.rllib import models\n",
    "\n",
    "        _policy_class = get_policy_class(config)\n",
    "        self.policy = _policy_class(\n",
    "            self.env.observation_space, self.env.action_space, config\n",
    "        )\n",
    "\n",
    "    def rollout(self, timestep_limit, novelty_archive, add_noise=True):\n",
    "        rollout_reward, rollout_fragment_length, obs, batch = rollout(\n",
    "            self.policy, self.env, novelty_archive, timestep_limit=timestep_limit, add_noise=add_noise, novelty_type=self.config[\"novelty_type\"]\n",
    "        )\n",
    "        return rollout_reward, obs, batch\n",
    "    \n",
    "    def evaluation_rollout(self):\n",
    "        rollout_reward = evaluation_rollout(\n",
    "            self.policy, self.env\n",
    "        )\n",
    "        return rollout_reward\n",
    "    \n",
    "    #optimise this? \n",
    "    def calculate_model_weights(self, noise_indexes):\n",
    "        weights = self.config[\"noise_stdev\"] * self.noise.get(noise_indexes[0], self.policy.num_params)\n",
    "        for i in range(1, len(noise_indexes)):\n",
    "            weights += (self.config[\"noise_stdev\"] * self.config[\"noise_decay\"]**i) * self.noise.get(noise_indexes[i], self.policy.num_params)    \n",
    "        return weights \n",
    "    \n",
    "    def euclidean_distance(self, x, y):\n",
    "        n, m = len(x), len(y)\n",
    "        if n > m:\n",
    "            a = np.linalg.norm(y - x[:m])\n",
    "            b = np.linalg.norm(y[-1] - x[m:])\n",
    "        else:\n",
    "            a = np.linalg.norm(x - y[:n])\n",
    "            b = np.linalg.norm(x[-1] - y[n:])\n",
    "        return np.sqrt(a**2 + b**2)\n",
    "\n",
    "    def compute_novelty_vs_archive(self, archive, novelty_vector, k):\n",
    "        if len(archive) < k:\n",
    "            return 0\n",
    "        distances = []\n",
    "        nov = novelty_vector.astype(np.float64)\n",
    "        for point in archive:\n",
    "            distances.append(self.euclidean_distance(point.astype(np.float64), nov))\n",
    "\n",
    "        # Pick k nearest neighbors\n",
    "        distances = np.array(distances)\n",
    "        top_k = np.sort(distances)[:k]\n",
    "        return top_k.mean()\n",
    "    \n",
    "    def do_evaluate(self, noise_indices, novelty_archive, timestep_limit=None):\n",
    "        weights = self.calculate_model_weights(noise_indices)\n",
    "        self.policy.set_flat_weights(weights)\n",
    "        reward = []\n",
    "        for i in range(3):\n",
    "            r = self.evaluation_rollout(timestep_limit, novelty_archive)\n",
    "            reward.append(np.sum(r))\n",
    "        return mean(reward)\n",
    "    \n",
    "    def do_rollouts(self, noise_indices, novelty_archive, timestep_limit=None):\n",
    "        # Set the network weights.\n",
    "        pop = []\n",
    "        rewards = []\n",
    "        observations = []\n",
    "        novelties = []\n",
    "        batch_builder = SampleBatchBuilder()\n",
    "        writer = JsonWriterEdit(os.path.join(ray._private.utils.get_user_temp_dir(), self.config[\"experience_store_dir\"])) \n",
    "        for ni in noise_indices:\n",
    "            weights = self.calculate_model_weights(ni)\n",
    "\n",
    "            # Do a regular run with parameter perturbations.\n",
    "            noise_index = self.noise.sample_index(self.policy.num_params)\n",
    "\n",
    "            perturbation = self.config[\"noise_stdev\"] * self.noise.get(\n",
    "                noise_index, self.policy.num_params\n",
    "            )\n",
    "\n",
    "            self.policy.set_flat_weights(weights + perturbation)\n",
    "            reward = 0; obs = []\n",
    "            for i in range(self.config[\"episodes_per_batch\"]):\n",
    "                r, o, batch = self.rollout(timestep_limit, novelty_archive)\n",
    "                if np.random.rand() <= self.config[\"experience_sample_rate\"]:  \n",
    "                    writer.write(batch)\n",
    "                reward += np.sum(r)\n",
    "                obs.append(o)\n",
    "            rewards.append(reward/self.config[\"episodes_per_batch\"])\n",
    "            obvs = np.mean(obs, axis=0)\n",
    "            observations.append(obvs)\n",
    "            novelties.append(self.compute_novelty_vs_archive(novelty_archive, obvs, self.config[\"novelty_k\"]))\n",
    "            ni.append(noise_index)\n",
    "            pop.append(ni)\n",
    "            \n",
    "        return Result(\n",
    "            noise_indices=pop,\n",
    "            eval_returns=rewards,\n",
    "            observations=observations,  \n",
    "            novelties=novelties,\n",
    "            experience=batch_builder.build_and_reset(),\n",
    "        )\n",
    "\n",
    "\n",
    "def get_policy_class(config):\n",
    "    return GATFPolicy\n",
    "\n",
    "class GATrainer(Trainer):\n",
    "    \"\"\"Large-scale implementation of Evolution Strategies in Ray.\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    @override(Trainer)\n",
    "    def get_default_config(cls) -> TrainerConfigDict:\n",
    "        return GAConfig().to_dict()\n",
    "\n",
    "    @override(Trainer)\n",
    "    def validate_config(self, config: TrainerConfigDict) -> None:\n",
    "        # Call super's validation method.\n",
    "        super().validate_config(config)\n",
    "\n",
    "        if config[\"num_gpus\"] > 1:\n",
    "            raise ValueError(\"`num_gpus` > 1 not yet supported for ES!\")\n",
    "        if config[\"num_workers\"] <= 0:\n",
    "            raise ValueError(\"`num_workers` must be > 0 for ES!\")\n",
    "        if config[\"evaluation_config\"][\"num_envs_per_worker\"] != 1:\n",
    "            raise ValueError(\n",
    "                \"`evaluation_config.num_envs_per_worker` must always be 1 for \"\n",
    "                \"ES! To parallelize evaluation, increase \"\n",
    "                \"`evaluation_num_workers` to > 1.\"\n",
    "            )\n",
    "\n",
    "    @override(Trainer)\n",
    "    def setup(self, config):\n",
    "        # Setup our config: Merge the user-supplied config (which could\n",
    "        # be a partial config dict with the class' default).\n",
    "        if isinstance(config, dict):\n",
    "            self.config = self.merge_trainer_configs(\n",
    "                self.get_default_config(), config, self._allow_unknown_configs\n",
    "            )\n",
    "        else:\n",
    "            self.config = config.to_dict()\n",
    "\n",
    "        # Call super's validation method.\n",
    "        self.validate_config(self.config)\n",
    "\n",
    "        # Generate `self.env_creator` callable to create an env instance.\n",
    "        self.env_creator = self._get_env_creator_from_env_id(self._env_id)\n",
    "        # Generate the local env.\n",
    "        env_context = EnvContext(self.config[\"env_config\"] or {}, worker_index=0)\n",
    "        env = self.env_creator(env_context)\n",
    "\n",
    "        self.callbacks = self.config[\"callbacks\"]()\n",
    "\n",
    "        self._policy_class = get_policy_class(self.config)\n",
    "        self.policy = self._policy_class(\n",
    "            obs_space=env.observation_space,\n",
    "            action_space=env.action_space,\n",
    "            config=self.config,\n",
    "        )\n",
    "\n",
    "        # Create the shared noise table.\n",
    "        logger.info(\"Creating shared noise table.\")\n",
    "        noise_id = create_shared_noise.remote(self.config[\"noise_size\"])\n",
    "        self.noise = SharedNoiseTable(ray.get(noise_id))\n",
    "\n",
    "        # Create the actors.\n",
    "        logger.info(\"Creating actors.\")\n",
    "        self.workers = [\n",
    "            Worker.remote(self.config, {}, self.env_creator, noise_id, idx + 1)\n",
    "            for idx in range(self.config[\"num_workers\"])\n",
    "        ]\n",
    "        \n",
    "        self.population = [[i] for i in range(int(self.config[\"num_workers\"] * self.config[\"individuals_per_worker\"]))]\n",
    "        self.novelty_archive = deque([], maxlen=self.config[\"novelty_max_size\"])\n",
    "        self.novelty_history = []\n",
    "        self.episodes_so_far = 0\n",
    "        self.reward_list = []\n",
    "        self.tstart = time.time()\n",
    "        self.elite = [0]\n",
    "\n",
    "    @override(Trainer)\n",
    "    def get_policy(self, policy=DEFAULT_POLICY_ID):\n",
    "        if policy != DEFAULT_POLICY_ID:\n",
    "            raise ValueError(\n",
    "                \"ES has no policy '{}'! Use {} \"\n",
    "                \"instead.\".format(policy, DEFAULT_POLICY_ID)\n",
    "            )\n",
    "        return self.policy\n",
    "\n",
    "    @override(Trainer)\n",
    "    def step_attempt(self):\n",
    "        config = self.config\n",
    "\n",
    "        #theta = self.policy.get_flat_weights()\n",
    "        #assert theta.dtype == np.float32\n",
    "        #assert len(theta.shape) == 1\n",
    "\n",
    "        # Put the current policy weights in the object store.\n",
    "       # theta_id = ray.put(theta)\n",
    "        # Use the actors to do rollouts. Note that we pass in the ID of the\n",
    "        # policy weights as these are shared.\n",
    "        results = self._collect_results(self.population, self.novelty_archive)\n",
    "        \n",
    "        # Update our sample steps counters.\n",
    "\n",
    "        # Loop over the results.\n",
    "        self.episodes_so_far += 1\n",
    "        # Assemble the results.\n",
    "        returns = []\n",
    "        novelty = []\n",
    "        individuals = []\n",
    "        for i, result in enumerate(results):\n",
    "            returns.extend(result.eval_returns)\n",
    "            novelty.extend(result.novelties)\n",
    "            individuals.extend(result.noise_indices)\n",
    "            if np.random.rand() <= self.config[\"store_novelty_probs\"]:\n",
    "                #print(result.observations)\n",
    "                self.novelty_archive.extend(result.observations)\n",
    "                self.novelty_history.extend(result.observations)\n",
    "            \n",
    "        #Learn GA\n",
    "        novelty = np.array(novelty); returns = np.array(returns)\n",
    "        values = []\n",
    "        for i in range(len(individuals)): \n",
    "            n = (novelty[i] / np.max(novelty)) * (1-self.config[\"reward_coefficient\"]) if np.max(novelty) > 0 else 0\n",
    "            r = ((returns[i]+np.abs(np.min(returns))) / (np.max(returns)+np.abs(np.min(returns)))) * self.config[\"reward_coefficient\"]\n",
    "            values.append(n + r)\n",
    "        values = np.array(values)\n",
    "        \n",
    "        \n",
    "        population = [self.elite]\n",
    "        self.elite = individuals[np.argmax(returns)]\n",
    "        \n",
    "        rollout_ids = [\n",
    "            worker.do_evaluate.remote(individuals[np.argmax(returns)], self.novelty_archive) for i, worker in enumerate(self.workers)\n",
    "        ]\n",
    "        evals = []\n",
    "        for result in ray.get(rollout_ids):\n",
    "            evals.append(result)\n",
    "        print('eval:' + str(mean(evals)))\n",
    "        \n",
    "        indexes = np.arange(len(individuals))\n",
    "        for i in range(len(individuals)-1):\n",
    "            np.random.shuffle(indexes)\n",
    "            winner = np.max(values[indexes[0:self.config[\"tourney_size\"]]])\n",
    "            index = np.where(values == winner)[0][0]\n",
    "            population.append(individuals[index])\n",
    "        \n",
    "        self.population = population\n",
    "        \n",
    "\n",
    "        info = {\n",
    "            \"episodes_so_far\": self.episodes_so_far,\n",
    "        }\n",
    "\n",
    "        result = dict(\n",
    "            episode_reward_mean=mean(returns),\n",
    "            episode_reward_max=max(returns),\n",
    "            episode_novelty_mean=mean(novelty),\n",
    "            episode_elite_eval=mean(evals),\n",
    "            #episode_len_mean=eval_lengths.mean(),\n",
    "            #timesteps_this_iter=noisy_lengths.sum(),\n",
    "            info=info,\n",
    "        )\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def calculate_model_weights(self, noise_indexes):\n",
    "        weights = self.config[\"noise_stdev\"] * self.noise.get(noise_indexes[0], self.policy.num_params)\n",
    "        for i in range(1, len(noise_indexes)):\n",
    "            weights += (self.config[\"noise_stdev\"] * self.config[\"noise_decay\"]**i) * self.noise.get(noise_indexes[i], self.policy.num_params)    \n",
    "        return weights\n",
    "\n",
    "    @override(Trainer)\n",
    "    def compute_single_action(self, observation, *args, **kwargs):\n",
    "        action, _, _ = self.policy.compute_actions([observation], update=False)\n",
    "        if kwargs.get(\"full_fetch\"):\n",
    "            return action[0], [], {}\n",
    "        return action[0]\n",
    "\n",
    "    @override(Trainer)\n",
    "    def _sync_weights_to_workers(self, *, worker_set=None, workers=None):\n",
    "        # Broadcast the new policy weights to all evaluation workers.\n",
    "        assert worker_set is not None\n",
    "        logger.info(\"Synchronizing weights to evaluation workers.\")\n",
    "        weights = ray.put(self.policy.get_flat_weights())\n",
    "        worker_set.foreach_policy(lambda p, pid: p.set_flat_weights(ray.get(weights)))\n",
    "\n",
    "    @override(Trainer)\n",
    "    def cleanup(self):\n",
    "        # workaround for https://github.com/ray-project/ray/issues/1516\n",
    "        for w in self.workers:\n",
    "            w.__ray_terminate__.remote()\n",
    "\n",
    "    def _collect_results(self, population, novelty_archive):\n",
    "        num_timesteps = 0\n",
    "        results = []\n",
    "\n",
    "        ind = self.config['individuals_per_worker']\n",
    "        rollout_ids = [\n",
    "            worker.do_rollouts.remote(population[int(i*ind):int((i+1)*ind)], novelty_archive) for i, worker in enumerate(self.workers)\n",
    "        ]\n",
    "        # Get the results of the rollouts.\n",
    "        for result in ray.get(rollout_ids):\n",
    "            results.append(result)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return {\n",
    "            \"weights\": self.policy.get_flat_weights(),\n",
    "            \"filter\": self.policy.observation_filter,\n",
    "            \"episodes_so_far\": self.episodes_so_far,\n",
    "        }\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.episodes_so_far = state[\"episodes_so_far\"]\n",
    "        self.policy.set_flat_weights(state[\"weights\"])\n",
    "        self.policy.observation_filter = state[\"filter\"]\n",
    "        FilterManager.synchronize(\n",
    "            {DEFAULT_POLICY_ID: self.policy.observation_filter}, self.workers\n",
    "        )\n",
    "\n",
    "\n",
    "# Deprecated: Use ray.rllib.algorithms.es.ESConfig instead!\n",
    "class _deprecated_default_config(dict):\n",
    "    def __init__(self):\n",
    "        super().__init__(GAConfig().to_dict())\n",
    "\n",
    "    @Deprecated(\n",
    "        old=\"ray.rllib.algorithms.es.es.DEFAULT_CONFIG\",\n",
    "        new=\"ray.rllib.algorithms.es.es.ESConfig(...)\",\n",
    "        error=False,\n",
    "    )\n",
    "    def __getitem__(self, item):\n",
    "        return super().__getitem__(item)\n",
    "\n",
    "\n",
    "DEFAULT_CONFIG = _deprecated_default_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.offline.json_writer import JsonWriter\n",
    "from ray.rllib.offline.dataset_writer import DatasetWriter\n",
    "from ray.rllib.offline.io_context import IOContext\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "def env_creator(env_config: dict):\n",
    "    path = str(inspect.getfile(CybORG))\n",
    "    path = path[:-10] + '/Shared/Scenarios/Scenario2.yaml'\n",
    "    agents = {\"Red\": B_lineAgent, \"Green\": GreenAgent}\n",
    "    cyborg = CybORG(scenario_file=path, environment='sim', agents=agents)\n",
    "    env = RLlibWrapper(env=cyborg, agent_name=\"Blue\", max_steps=100)\n",
    "    return env\n",
    "\n",
    "def print_results(results_dict):\n",
    "    train_iter = results_dict[\"training_iteration\"]\n",
    "    r_mean = results_dict[\"episode_reward_mean\"]\n",
    "    r_max = results_dict[\"episode_reward_max\"]\n",
    "    r_min = results_dict[\"episode_reward_min\"]\n",
    "    print(f\"{train_iter:4d} \\tr_mean: {r_mean:.1f} \\tr_max: {r_max:.1f} \\tr_min: {r_min: .1f}\")\n",
    "    \n",
    "register_env(name=\"CybORG\", env_creator=env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-24 21:12:45,867\tINFO trainable.py:159 -- Trainable.setup took 11.044 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-06-24 21:12:45,871\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Worker pid=36639)\u001b[0m 2022-06-24 21:12:54,269\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=36650)\u001b[0m 2022-06-24 21:12:54,457\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=36644)\u001b[0m 2022-06-24 21:12:54,593\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=36648)\u001b[0m 2022-06-24 21:12:54,695\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=36642)\u001b[0m 2022-06-24 21:12:54,772\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=36638)\u001b[0m 2022-06-24 21:12:54,871\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=36635)\u001b[0m 2022-06-24 21:12:55,007\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=36643)\u001b[0m 2022-06-24 21:12:55,061\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=36664)\u001b[0m 2022-06-24 21:12:55,181\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=36641)\u001b[0m 2022-06-24 21:12:55,208\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=36646)\u001b[0m 2022-06-24 21:12:55,220\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=36652)\u001b[0m 2022-06-24 21:12:55,237\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=36661)\u001b[0m 2022-06-24 21:12:55,241\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=36651)\u001b[0m 2022-06-24 21:12:55,306\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=36658)\u001b[0m 2022-06-24 21:12:55,315\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=36666)\u001b[0m 2022-06-24 21:12:55,262\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=36657)\u001b[0m 2022-06-24 21:12:55,295\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=36645)\u001b[0m 2022-06-24 21:12:55,361\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=36675)\u001b[0m 2022-06-24 21:12:55,364\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=36682)\u001b[0m 2022-06-24 21:12:55,327\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=36640)\u001b[0m 2022-06-24 21:12:55,382\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=36636)\u001b[0m 2022-06-24 21:12:55,382\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=36649)\u001b[0m 2022-06-24 21:12:55,393\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=36654)\u001b[0m 2022-06-24 21:12:55,418\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=36656)\u001b[0m 2022-06-24 21:12:55,515\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=36669)\u001b[0m 2022-06-24 21:12:55,460\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=36653)\u001b[0m 2022-06-24 21:12:55,505\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=36684)\u001b[0m 2022-06-24 21:12:55,499\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=36637)\u001b[0m 2022-06-24 21:12:55,656\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=36672)\u001b[0m 2022-06-24 21:12:55,654\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval:-741.0822222222221\n",
      "  0 reward mean: -752.88, reward max: -592.54, novelty mean:   0.00\n",
      "eval:-650.3511111111111\n",
      "  1 reward mean: -719.61, reward max: -441.93, novelty mean:   0.00\n",
      "eval:-442.00111111111113\n",
      "  2 reward mean: -672.85, reward max: -386.56, novelty mean:   0.00\n",
      "eval:-299.08777777777783\n",
      "  3 reward mean: -564.92, reward max: -291.20, novelty mean:   0.00\n",
      "eval:-258.77444444444444\n",
      "  4 reward mean: -429.65, reward max: -216.54, novelty mean:   0.07\n",
      "eval:-220.21555555555557\n",
      "  5 reward mean: -302.69, reward max: -199.10, novelty mean:   0.09\n",
      "eval:-215.88777777777779\n",
      "  6 reward mean: -247.33, reward max: -190.11, novelty mean:   0.13\n",
      "eval:-195.0088888888889\n",
      "  7 reward mean: -222.05, reward max: -192.18, novelty mean:   0.19\n",
      "eval:-198.19111111111113\n",
      "  8 reward mean: -215.71, reward max: -184.33, novelty mean:   0.16\n",
      "eval:-196.84777777777776\n",
      "  9 reward mean: -210.74, reward max: -175.90, novelty mean:   0.18\n",
      "eval:-178.4288888888889\n",
      " 10 reward mean: -211.66, reward max: -173.06, novelty mean:   0.19\n",
      "eval:-174.85555555555555\n",
      " 11 reward mean: -200.03, reward max: -173.36, novelty mean:   0.22\n",
      "eval:-179.23777777777778\n",
      " 12 reward mean: -196.06, reward max: -172.25, novelty mean:   0.19\n",
      "eval:-184.26333333333332\n",
      " 13 reward mean: -204.39, reward max: -160.78, novelty mean:   0.18\n",
      "eval:-173.41444444444443\n",
      " 14 reward mean: -212.15, reward max: -158.05, novelty mean:   0.20\n",
      "eval:-166.8022222222222\n",
      " 15 reward mean: -208.66, reward max: -153.67, novelty mean:   0.22\n",
      "eval:-154.98555555555555\n",
      " 16 reward mean: -200.40, reward max: -156.93, novelty mean:   0.21\n",
      "eval:-163.03\n",
      " 17 reward mean: -197.50, reward max: -155.36, novelty mean:   0.23\n",
      "eval:-190.66\n",
      " 18 reward mean: -194.76, reward max: -142.89, novelty mean:   0.24\n",
      "eval:-160.04666666666665\n",
      " 19 reward mean: -198.64, reward max: -152.32, novelty mean:   0.23\n",
      "eval:-154.27888888888887\n",
      " 20 reward mean: -194.61, reward max: -146.79, novelty mean:   0.22\n",
      "eval:-148.51444444444442\n",
      " 21 reward mean: -196.41, reward max: -139.74, novelty mean:   0.22\n",
      "eval:-143.68666666666664\n",
      " 22 reward mean: -190.17, reward max: -130.14, novelty mean:   0.24\n",
      "eval:-129.44444444444443\n",
      " 23 reward mean: -178.73, reward max: -129.78, novelty mean:   0.26\n",
      "eval:-120.46444444444441\n",
      " 24 reward mean: -173.40, reward max: -121.69, novelty mean:   0.27\n",
      "eval:-122.92888888888886\n",
      " 25 reward mean: -169.95, reward max: -112.98, novelty mean:   0.26\n",
      "eval:-122.22555555555553\n",
      " 26 reward mean: -167.09, reward max: -109.22, novelty mean:   0.27\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import subprocess\n",
    "from shutil import make_archive\n",
    "model = {\"fcnet_hiddens\": [512,512],\n",
    "         \"fcnet_activation\": \"relu\",}\n",
    "\n",
    "names = [\"85_data_a\", \"9_data_a\"]\n",
    "#names = [\"50_store\",\"150_store\",\"250_store\",\"350_store\"]\n",
    "means_all = []; max_all = []; novel_all = []; archives = []; evals_all = []\n",
    "for i, co in enumerate([.85,.9]):\n",
    "    if os.path.isdir(os.path.join(ray._private.utils.get_user_temp_dir(), names[i])):\n",
    "        shutil.rmtree(os.path.join(ray._private.utils.get_user_temp_dir(), names[i]))\n",
    "\n",
    "    config = GAConfig().training(\n",
    "                             episodes_per_batch=25, \n",
    "                             reward_coefficient=co,\n",
    "                             model=model, \n",
    "                             noise_stdev=0.035,\n",
    "                             noise_decay=0.997,\n",
    "                             store_novelty_probs=0.1,\n",
    "                             individuals_per_worker=4,\n",
    "                             novelty_max_size=10000, #Go as high as you dare\n",
    "                             experience_sample_rate=0,\n",
    "                             tourney_size=12,\n",
    "                             novelty_k=30,\n",
    "                             novelty_type='action', #'action' or 'state'\n",
    "                             experience_store_dir=names[i])\\\n",
    "                             .resources(num_gpus=1).rollouts(num_rollout_workers=30).debugging(log_level='ERROR')\n",
    "    trainer = config.build(env=\"CybORG\")\n",
    "    print(co)\n",
    "    s = \"{:3d} reward mean: {:6.2f}, reward max: {:6.2f}, novelty mean: {:6.2f}\"\n",
    "    means = []; maxs = []; nov = []; evall = []\n",
    "    for j in range(int(200)):\n",
    "        result = trainer.train()\n",
    "        means.append(result[\"episode_reward_mean\"])\n",
    "        maxs.append(result[\"episode_reward_max\"])\n",
    "        nov.append(result[\"episode_novelty_mean\"])\n",
    "        evall.append(result[\"episode_elite_eval\"])\n",
    "        print(s.format(j,result[\"episode_reward_mean\"], result[\"episode_reward_max\"], result[\"episode_novelty_mean\"]))\n",
    "\n",
    "    #Collected data needs to be cleaned \n",
    "    result = subprocess.run(['ls', os.path.join(ray._private.utils.get_user_temp_dir(), names[i])], stdout=subprocess.PIPE)\n",
    "    removed = 0\n",
    "    for j, name in enumerate(str(result.stdout)[2:].split('\\\\n')[0:-1]):\n",
    "        f = open(os.path.join(ray._private.utils.get_user_temp_dir(), names[i], name))\n",
    "        try:\n",
    "            json.load(f)\n",
    "        except ValueError as err:\n",
    "            os.remove(os.path.join(ray._private.utils.get_user_temp_dir(), names[i], name)) \n",
    "            removed += 1\n",
    "    print('Removed ' + str(removed) + ' files, of ' + str(j) + 'files')\n",
    "    make_archive(names[i], 'zip', os.path.join(ray._private.utils.get_user_temp_dir(), names[i]))\n",
    "    \n",
    "    evals_all.append(evall); np.save(names[i]+'_evals.npy', np.array(evall))\n",
    "    means_all.append(means); np.save(names[i]+'_means.npy', np.array(means))\n",
    "    max_all.append(maxs); np.save(names[i]+'_maxs.npy', np.array(maxs))\n",
    "    novel_all.append(nov); np.save(names[i]+'_nov.npy', np.array(nov))\n",
    "    archives.append(np.stack(trainer.novelty_history)); np.save(names[i]+'_archive.npy', np.stack(trainer.novelty_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'umap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [93]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\n\u001b[1;32m      4\u001b[0m archives \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m archives\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m95_data_a_archive.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'umap'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "\n",
    "archives = []\n",
    "archives.append(np.load('95_data_a_archive.npy'))\n",
    "archives.append(np.load('9_data_a_archive.npy'))\n",
    "archives.append(np.load('85_data_a_archive.npy'))\n",
    "archives.append(np.load('ppo_novel_actions.npy'))\n",
    "\n",
    "names = ['Reward CoEf: 0.95', 'Reward CoEf: 0.9', 'Reward CoEf: 0.85', 'PPO']\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "runner = 0\n",
    "#reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1)\n",
    "#umap_embedding = reducer.fit_transform(np.concatenate(archives))\n",
    "ax1 = fig.add_subplot(111)\n",
    "for i, a in enumerate(archives[:4]):\n",
    "    ax1.scatter(umap_embedding[runner:runner+len(a),0],umap_embedding[runner:runner+len(a),1], label=names[i], alpha=0.1)\n",
    "    runner += len(a)\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('uMAP Plot of Action Based Novelty Experiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fml.txt') as f:\n",
    "    lines = f.readlines()\n",
    "reward = []\n",
    "for l in lines:\n",
    "    if \"eps 100 is: \" in l: \n",
    "        reward.append(float((l[59:64])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,12))\n",
    "plt.plot(np.load('9_data_s_maxs.npy'), label=\"Reward CoEf: 0.9\")\n",
    "plt.plot(np.load('8_data_s_maxs.npy'), label=\"Reward CoEf: 0.8\")\n",
    "plt.plot(np.load('7_data_s_maxs.npy'), label=\"Reward CoEf: 0.7\")\n",
    "plt.plot(reward[0:200], label=\"PPO\")\n",
    "plt.title('Reward Accumilation Using Action Based Novelty')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim([0,200])\n",
    "plt.ylim([-60,-15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(archives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from matplotlib import animation, rc\n",
    "\n",
    "rewards = np.load('ppo_reward.npy')\n",
    "novel = np.load('ppo_novel.npy')\n",
    "\n",
    "whole = [archives[1]]\n",
    "whole.append(novel)\n",
    "\n",
    "#reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1)\n",
    "#umap_embedding = reducer.fit_transform(np.concatenate(whole))\n",
    "animation_frames = 150\n",
    "runner = 0\n",
    "samples = 0\n",
    "start = 0\n",
    "for i, a in enumerate(archives):\n",
    "    if i == 0:\n",
    "        start = runner\n",
    "        samples = len(a)\n",
    "    runner += len(a)\n",
    "filenames = []\n",
    "for i in range(150):\n",
    "    fig = plt.figure(figsize=(10, 8), dpi=100)\n",
    "    i_line = int(i*(150/animation_frames))\n",
    "    i_scatter = int(i*(samples/animation_frames))\n",
    "    ax = plt.subplot(1, 2, 1)\n",
    "    ax.set_xlim([-4,15])\n",
    "    ax.set_ylim([-4,11])\n",
    "    ax.set_title('uMAP of State Embedding')\n",
    "    ax1 = plt.subplot(1, 2, 2)\n",
    "    ax1.set_xlim([0,150])\n",
    "    ax1.set_ylim([-100,-20])\n",
    "    ax1.set_title('Reward Accumilation')\n",
    "    ax1.set_xlabel('Iteration')\n",
    "    ax1.set_ylabel('Reward')\n",
    "    ax1.plot(max_all[1][:i_line+1], label=\"GA\")\n",
    "    ax1.plot(rewards[:i+1], label=\"PPO\")\n",
    "    ax1.legend(loc='lower right')\n",
    "    ax.scatter(umap_embedding[0:i_scatter+1,0], umap_embedding[0:i_scatter+1,1])\n",
    "    ax.scatter(umap_embedding[samples:samples+i+1,0], umap_embedding[samples:samples+i+1,1])\n",
    "    plt.savefig('gagif/'+str(i)+'.png')\n",
    "    filenames.append('gagif/'+str(i)+'.png')\n",
    "\n",
    "import imageio\n",
    "images = []\n",
    "for filename in filenames:\n",
    "    images.append(imageio.imread(filename))\n",
    "imageio.mimsave('movie.gif', images, duration=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.mimsave('movie.gif', images, duration=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(16,12))\n",
    "ax1 = fig.add_subplot(111)\n",
    "#ax1.plot(max_all[0] ,label=\"Reward CoEf = 0.6, Max\")\n",
    "#ax1.plot(max_all[0] ,label=\"Reward CoEf = 0.7, Max\")\n",
    "ax1.plot(max_all[0] ,label=\"Reward CoEf = 0.7\")\n",
    "ax1.plot(max_all[1],label=\"Reward CoEf = 0.8\")\n",
    "ax1.plot(max_all[2], label=\"Reward CoEf = 0.9\")\n",
    "#ax1.plot(means_all[0], label=\"Reward CoEf = 0.7, Mean\", color='b')\n",
    "#ax1.plot(means_all[1], label=\"Reward CoEf = 0.8, Mean\", color='g')\n",
    "#ax1.plot(means_all[2], label=\"Reward CoEf = 0.9, Mean\", color='r')\n",
    "#ax1.plot(means_all[3], label=\"Reward CoEf = 1, Mean\", color='c')\n",
    "#ax1.plot(means_all[5], label=\"Reward CoEf = 1, Mean\", color='m')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Novelty Experiment (B_lineAgent)')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Max Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "from six.moves.urllib.parse import urlparse\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from smart_open import smart_open\n",
    "except ImportError:\n",
    "    smart_open = None\n",
    "\n",
    "from ray.rllib.policy.sample_batch import MultiAgentBatch\n",
    "from ray.rllib.offline.io_context import IOContext\n",
    "from ray.rllib.offline.output_writer import OutputWriter\n",
    "from ray.rllib.utils.annotations import override, PublicAPI\n",
    "from ray.rllib.utils.compression import pack, compression_supported\n",
    "from ray.rllib.utils.typing import FileType, SampleBatchType\n",
    "from ray.util.ml_utils.json import SafeFallbackEncoder\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "WINDOWS_DRIVES = [chr(i) for i in range(ord(\"c\"), ord(\"z\") + 1)]\n",
    "\n",
    "\n",
    "# TODO(jungong) : use DatasetWriter to back JsonWriter, so we reduce\n",
    "#     codebase complexity without losing existing functionality.\n",
    "@PublicAPI\n",
    "class JsonWriterEdit(OutputWriter):\n",
    "    \"\"\"Writer object that saves experiences in JSON file chunks.\"\"\"\n",
    "\n",
    "    @PublicAPI\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: str,\n",
    "        ioctx: IOContext = None,\n",
    "        max_file_size: int = 64 * 1024 * 1024,\n",
    "        compress_columns: List[str] = frozenset([]), #[\"obs\", \"new_obs\"]\n",
    "    ):\n",
    "        \"\"\"Initializes a JsonWriter instance.\n",
    "\n",
    "        Args:\n",
    "            path: a path/URI of the output directory to save files in.\n",
    "            ioctx: current IO context object.\n",
    "            max_file_size: max size of single files before rolling over.\n",
    "            compress_columns: list of sample batch columns to compress.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        self.ioctx = ioctx or IOContext()\n",
    "        self.max_file_size = max_file_size\n",
    "        self.compress_columns = compress_columns\n",
    "        if urlparse(path).scheme not in [\"\"] + WINDOWS_DRIVES:\n",
    "            self.path_is_uri = True\n",
    "        else:\n",
    "            path = os.path.abspath(os.path.expanduser(path))\n",
    "            # Try to create local dirs if they don't exist\n",
    "            try:\n",
    "                os.makedirs(path)\n",
    "            except OSError:\n",
    "                pass  # already exists\n",
    "            assert os.path.exists(path), \"Failed to create {}\".format(path)\n",
    "            self.path_is_uri = False\n",
    "        self.path = path\n",
    "        self.file_index = 0\n",
    "        self.bytes_written = 0\n",
    "        self.cur_file = None\n",
    "\n",
    "    @override(OutputWriter)\n",
    "    def write(self, sample_batch: SampleBatchType):\n",
    "        start = time.time()\n",
    "        data = _to_json(sample_batch, self.compress_columns)\n",
    "        f = self._get_file()\n",
    "        f.write(data)\n",
    "        if hasattr(f, \"flush\"):  # legacy smart_open impls\n",
    "            f.flush()\n",
    "        self.bytes_written += len(data)\n",
    "        f.close()\n",
    "        logger.debug(\n",
    "            \"Wrote {} bytes to {} in {}s\".format(len(data), f, time.time() - start)\n",
    "        )\n",
    "\n",
    "    def _get_file(self) -> FileType:\n",
    "        timestr = datetime.utcnow().strftime('%H:%M:%S:%f')[:-3]\n",
    "        path = os.path.join(\n",
    "            self.path,\n",
    "            \"output-{}_worker-{}_{}.json\".format(\n",
    "                timestr, self.ioctx.worker_index, self.file_index\n",
    "            ),\n",
    "        )\n",
    "        return open(path, \"w\")\n",
    "\n",
    "\n",
    "def _to_jsonable(v, compress: bool) -> Any:\n",
    "    if compress and compression_supported():\n",
    "        return str(pack(v))\n",
    "    elif isinstance(v, np.ndarray):\n",
    "        return v.tolist()\n",
    "    return v\n",
    "\n",
    "\n",
    "def _to_json_dict(batch: SampleBatchType, compress_columns: List[str]) -> Dict:\n",
    "    out = {}\n",
    "    if isinstance(batch, MultiAgentBatch):\n",
    "        out[\"type\"] = \"MultiAgentBatch\"\n",
    "        out[\"count\"] = batch.count\n",
    "        policy_batches = {}\n",
    "        for policy_id, sub_batch in batch.policy_batches.items():\n",
    "            policy_batches[policy_id] = {}\n",
    "            for k, v in sub_batch.items():\n",
    "                policy_batches[policy_id][k] = _to_jsonable(\n",
    "                    v, compress=k in compress_columns\n",
    "                )\n",
    "        out[\"policy_batches\"] = policy_batches\n",
    "    else:\n",
    "        out[\"type\"] = \"SampleBatch\"\n",
    "        for k, v in batch.items():\n",
    "            out[k] = _to_jsonable(v, compress=k in compress_columns)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _to_json(batch: SampleBatchType, compress_columns: List[str]) -> str:\n",
    "    out = _to_json_dict(batch, compress_columns)\n",
    "    return json.dumps(out, cls=SafeFallbackEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "os.mkdir(os.path.join(ray._private.utils.get_user_temp_dir(), \"8_data_s_large.\"))\n",
    "result = subprocess.run(['ls', os.path.join(ray._private.utils.get_user_temp_dir(), \"8_data_s\")], stdout=subprocess.PIPE)\n",
    "for i, s in enumerate(str(result.stdout)[2:].split('\\\\n')[0:-1]):\n",
    "    if i % 5 == 0:\n",
    "        shutil.copyfile(os.path.join(ray._private.utils.get_user_temp_dir(), \"8_data_s\",s), os.path.join(ray._private.utils.get_user_temp_dir(), \"8_data_s_smallish\",s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "result = subprocess.run(['ls', os.path.join(ray._private.utils.get_user_temp_dir(), \"8_data_s_smallish\")], stdout=subprocess.PIPE)\n",
    "len(str(result.stdout)[2:].split('\\\\n')[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Worker pid=8196)\u001b[0m 2022-06-24 14:55:03,071\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8184)\u001b[0m 2022-06-24 14:55:03,139\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8182)\u001b[0m 2022-06-24 14:55:03,318\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8197)\u001b[0m 2022-06-24 14:55:03,456\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8199)\u001b[0m 2022-06-24 14:55:03,669\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8213)\u001b[0m 2022-06-24 14:55:03,658\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8201)\u001b[0m 2022-06-24 14:55:03,750\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8204)\u001b[0m 2022-06-24 14:55:03,842\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8176)\u001b[0m 2022-06-24 14:55:03,964\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8202)\u001b[0m 2022-06-24 14:55:03,955\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8174)\u001b[0m 2022-06-24 14:55:03,919\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8185)\u001b[0m 2022-06-24 14:55:04,005\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8181)\u001b[0m 2022-06-24 14:55:04,075\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8183)\u001b[0m 2022-06-24 14:55:04,112\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8187)\u001b[0m 2022-06-24 14:55:04,132\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8195)\u001b[0m 2022-06-24 14:55:04,197\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8179)\u001b[0m 2022-06-24 14:55:04,206\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8198)\u001b[0m 2022-06-24 14:55:04,226\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8188)\u001b[0m 2022-06-24 14:55:04,207\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8216)\u001b[0m 2022-06-24 14:55:04,233\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8212)\u001b[0m 2022-06-24 14:55:04,243\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8180)\u001b[0m 2022-06-24 14:55:04,320\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8175)\u001b[0m 2022-06-24 14:55:04,341\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Worker pid=8203)\u001b[0m 2022-06-24 14:55:04,422\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n",
      "2022-06-24 14:55:08,408\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8184, ip=192.168.16.2, repr=<__main__.Worker object at 0x7fddf218e940>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 92 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:08,410\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8178, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f705f06da00>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 131 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:08,411\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8196, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f7cab16aa30>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 90 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:08,412\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8208, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f3558158970>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 118 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:08,413\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8211, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f7ebcd5da30>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 59 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:08,413\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8205, ip=192.168.16.2, repr=<__main__.Worker object at 0x7fe2493899d0>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 75 is out of bounds for axis 0 with size 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-24 14:55:08,414\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8200, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f6ec43458e0>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 124 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:09,408\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8181, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f1a8648b970>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 81 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:09,409\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8182, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f964581da30>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 94 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:09,410\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8204, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f0856f16a30>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 67 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:09,411\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8176, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f3a074199d0>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 99 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:09,412\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8197, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f31dc361970>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 84 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:09,413\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8185, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f83ff3cc910>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 91 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:09,414\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8201, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f0d58d0e9d0>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 69 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:09,415\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8174, ip=192.168.16.2, repr=<__main__.Worker object at 0x7fa9103db970>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 119 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:09,416\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8183, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f3d5cded9d0>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 121 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:09,418\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8195, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f555a6dea30>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 69 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:09,419\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8199, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f83e6d14a30>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 140 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:09,420\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8202, ip=192.168.16.2, repr=<__main__.Worker object at 0x7fafaa90ea30>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 141 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:09,420\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8213, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f59330b2970>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 116 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:10,409\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8198, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f16d9da5910>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 129 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:10,410\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8179, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f85b2dbea30>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 80 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:10,411\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8212, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f71322f7970>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 66 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:10,412\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8216, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f7f59dbaa00>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 120 is out of bounds for axis 0 with size 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-24 14:55:10,412\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8203, ip=192.168.16.2, repr=<__main__.Worker object at 0x7feb2f493a30>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 135 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:10,413\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8188, ip=192.168.16.2, repr=<__main__.Worker object at 0x7ff0b83ac9d0>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 117 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:10,414\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8180, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f0947c6c9d0>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 88 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:10,415\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8187, ip=192.168.16.2, repr=<__main__.Worker object at 0x7f75eb80aa30>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 81 is out of bounds for axis 0 with size 54\n",
      "2022-06-24 14:55:10,416\tERROR worker.py:94 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Worker.do_rollouts()\u001b[39m (pid=8175, ip=192.168.16.2, repr=<__main__.Worker object at 0x7ef778ba2970>)\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 268, in do_rollouts\n",
      "  File \"<ipython-input-14-ffd2de595bdb>\", line 211, in rollout\n",
      "  File \"<ipython-input-13-8e26cdbe66b8>\", line 62, in rollout\n",
      "IndexError: index 101 is out of bounds for axis 0 with size 54\n"
     ]
    }
   ],
   "source": [
    "from shutil import make_archive\n",
    "make_archive(\"8_data_s_small\", 'zip', os.path.join(ray._private.utils.get_user_temp_dir(), \"8_data_s_small\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
